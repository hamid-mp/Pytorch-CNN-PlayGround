{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamid-mp/Pytorch-CNN-PlayGround/blob/main/gold_price_forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook focuses on **gold price forecasting** using a combination of baseline and deep learning models implemented in PyTorch. The project aims to predict future gold prices based on historical market data, enabling informed decision-making for trading and investment strategies.\n",
        "\n",
        "The workflow includes:\n",
        "\n",
        "* **Data Loading and Preprocessing** — Importing gold price data, applying statistical checks, scaling features, and generating technical indicators to enhance predictive power.\n",
        "* **Feature Engineering** — Leveraging rolling statistical tests and custom transformations to capture temporal dynamics in gold price movements.\n",
        "* **Model Development** — Implementing multiple architectures including:\n",
        "\n",
        "  * **Baseline Model** — A simple average-based predictor for performance benchmarking.\n",
        "  * **LSTM and GRU Networks** — Recurrent neural network models designed to capture sequential dependencies in time series data.\n",
        "* **Training and Evaluation** — Applying a structured training/testing regime, integrating techniques such as early stopping, and comparing model performances against the baseline.\n",
        "\n",
        "By establishing a robust comparison between naive prediction methods and advanced neural architectures, this notebook not only measures forecasting accuracy but also highlights the added value of deep learning for financial time series prediction.\n"
      ],
      "metadata": {
        "id": "8HWgI6bWS55c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMuRZfc26cgT",
        "outputId": "8c387934-60c3-4b82-9b38-88d266d73293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=d673f9afbdb01e48a3bc6a5a81181bf5c67d5aec79d58915660a7d83af70d668\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.5-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from ptflops) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.5-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.5\n"
          ]
        }
      ],
      "source": [
        " ! pip install ta\n",
        " ! pip install ptflops\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tteu6Rhk6HHi"
      },
      "source": [
        "# import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqHG44Kw6Cl2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from ptflops import get_model_complexity_info\n",
        "import math\n",
        "import seaborn as sns\n",
        "import ta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yHH7wOW6L31"
      },
      "source": [
        "# Load Data and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfp_h0Qm6UhS"
      },
      "outputs": [],
      "source": [
        "def rolling_adf(df, col, window_size=30):\n",
        "    \"\"\"\n",
        "    Calculate the Augmented Dickey-Fuller test statistic on a rolling window.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing the column on which to perform the ADF test.\n",
        "    col : str\n",
        "        The name of the column on which to perform the ADF test.\n",
        "    window_size : int\n",
        "        The size of the rolling window.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_copy : pandas.DataFrame\n",
        "        A new DataFrame with an additional column containing the rolling ADF test statistic.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Create an empty series to store rolling ADF test statistic\n",
        "    rolling_adf_stat = pd.Series(dtype='float64', index=df_copy.index)\n",
        "\n",
        "    # Loop through the DataFrame by `window_size` and apply `adfuller`.\n",
        "    for i in range(window_size, len(df)):\n",
        "        window = df_copy[col].iloc[i-window_size:i]\n",
        "        adf_result = adfuller(window)\n",
        "        adf_stat = adf_result[0]\n",
        "        rolling_adf_stat.at[df_copy.index[i]] = adf_stat\n",
        "\n",
        "    # Add the rolling ADF test statistic series to the original DataFrame\n",
        "    # df_copy['rolling_adf_stat'] = rolling_adf_stat\n",
        "\n",
        "    return rolling_adf_stat\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Kaufman’s Adaptive Moving Average (KAMA)\n",
        "\n",
        "def kama(df, col, n):\n",
        "  df_copy = df.copy()\n",
        "  # df_copy[f'kama_{n}'] = ta.momentum.KAMAIndicator(df_copy[col], n).kama()\n",
        "  return ta.momentum.KAMAIndicator(df_copy[col], n).kama()\n",
        "\n",
        "def moving_parkinson_estimator(df, window_size=10):\n",
        "\n",
        "  def parkinson_estimator(df):\n",
        "    N = len(df)\n",
        "    sum_squared = np.sum(np.log(df['High'] / df['Low'])**2)\n",
        "    volatility = math.sqrt((1/(4 * N * math.log(2))) * sum_squared)\n",
        "    return volatility\n",
        "\n",
        "  df_copy = df.copy()\n",
        "\n",
        "  rolling_volatility = pd.Series(dtype='float64')\n",
        "\n",
        "  for i in range(window_size, len(df)):\n",
        "    window = df_copy.loc[df_copy.index[i-window_size] : df_copy.index[i]]\n",
        "    volatility = parkinson_estimator(window)\n",
        "    rolling_volatility.at[df_copy.index[i]] = volatility\n",
        "\n",
        "  # df_copy['rolling_volatility_parkinson'] = rolling_volatility\n",
        "\n",
        "  return rolling_volatility\n",
        "\n",
        "def moving_yang_zhang_estimator(df, window_size=30):\n",
        "    \"\"\"\n",
        "    Calculate Parkinson's volatility estimator based on high and low prices.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing 'high' and 'low' columns for each trading period.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    volatility : float\n",
        "        Estimated volatility based on Parkinson's method.\n",
        "    \"\"\"\n",
        "    def yang_zhang_estimator(df):\n",
        "        N = len(window)\n",
        "\n",
        "        term1 = np.log(window['High'] / window['Close']) * np.log(window['High'] / window['Open'])\n",
        "        term2 = np.log(window['Low'] / window['Close']) * np.log(window['Low'] / window['Open'])\n",
        "\n",
        "        sum_squared = np.sum(term1 + term2)\n",
        "        volatility = np.sqrt(sum_squared / N)\n",
        "\n",
        "        return volatility\n",
        "\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    rolling_volatility = pd.Series(dtype='float64')\n",
        "\n",
        "    for i in range(window_size, len(df)):\n",
        "        window = df_copy.loc[df_copy.index[i-window_size]: df_copy.index[i]]\n",
        "        volatility = yang_zhang_estimator(window)\n",
        "        rolling_volatility.at[df_copy.index[i]] = volatility\n",
        "\n",
        "\n",
        "    return rolling_volatility\n",
        "\n",
        "def RSI(df, col, window_size):\n",
        "  delta = df[col].diff(1)\n",
        "  positive = delta.copy()\n",
        "  negative = delta.copy()\n",
        "  positive[positive < 0] = 0\n",
        "  negative[negative > 0] = 0\n",
        "  average_gain = positive.rolling(window=window_size).mean()\n",
        "  average_loss = abs(negative.rolling(window=window_size).mean())\n",
        "  relative_strength = average_gain / average_loss\n",
        "  RSI = 100.0 - (100.0 - (1.0 + relative_strength))\n",
        "\n",
        "  return RSI\n",
        "\n",
        "\n",
        "def create_features(df, window_size=30):\n",
        "  df['moving_parkinson'] = moving_parkinson_estimator(df, window_size=window_size)\n",
        "  df['rolling_adf'] = rolling_adf(df, 'Price', window_size=window_size)\n",
        "  df['moving_yang_zhang'] = moving_yang_zhang_estimator(df, window_size=window_size)\n",
        "  df['kama'] =kama(df, 'Price', window_size)\n",
        "  df['rsi'] = RSI(df, 'Price', window_size)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_MonFxF6tIb"
      },
      "outputs": [],
      "source": [
        "pth = '/content/drive/MyDrive/data.xlsx'\n",
        "\n",
        "df = pd.read_excel(pth, header=1, sheet_name='Data')\n",
        "\n",
        "df['Timestamp'] = pd.to_timedelta(df['Day'], unit='d') + \\\n",
        "                pd.to_timedelta(df['Hour'], unit='h') + \\\n",
        "                pd.to_timedelta(df['Minute'], unit='m') + \\\n",
        "                pd.to_timedelta(df['Second'], unit='s')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers(df):\n",
        "  df1 = deepcopy(df)\n",
        "  z_scores = (df1['Price.'] - df1['Price.'].mean()) / df1['Price.'].std()\n",
        "\n",
        "  # Filter out rows where |z| > 2\n",
        "  df_no_outliers = df1[np.abs(z_scores) <= 2.0]\n",
        "\n",
        "  # Optional: reset index\n",
        "  df_no_outliers = df_no_outliers.reset_index()\n",
        "\n",
        "  return df_no_outliers\n",
        "\n",
        "\n",
        "\n",
        "df_no_outliers = remove_outliers(df)"
      ],
      "metadata": {
        "id": "Dn1IqKe37fHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 1)\n",
        "axs[0].plot(df['Price.'])\n",
        "axs[1].plot(df_no_outliers['Price.'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "bYJITXDTyXHB",
        "outputId": "b48e1df2-c8d4-4c7a-fd36-b7d48a8ca035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7a84fd3ff890>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8wfPXVX6J_z"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsjTYESI04kF"
      },
      "outputs": [],
      "source": [
        "class AddGaussianNoise:\n",
        "    def __init__(self):\n",
        "        self.noise_std = random.uniform(0, 0.005)\n",
        "    def __call__(self, x):\n",
        "        noise = torch.randn_like(x) * self.noise_std * x\n",
        "        return x + noise\n",
        "\n",
        "class RandomScaling:\n",
        "    def __init__(self, scale_range=(0.995, 1.005)):\n",
        "        self.scale_range = scale_range\n",
        "    def __call__(self, x):\n",
        "        scale = np.random.uniform(*self.scale_range)\n",
        "        return x * scale\n",
        "\n",
        "class Compose:\n",
        "    \"\"\"Chain multiple augmentations.\"\"\"\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "    def __call__(self, x):\n",
        "        for t in self.transforms:\n",
        "            x = t(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9AWmTzW6Lae"
      },
      "outputs": [],
      "source": [
        "class GoldDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataset, lookback, transform=None):\n",
        "\n",
        "    self.transform = transform\n",
        "    self.dataset = dataset\n",
        "    self.lookback = lookback\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    data = self.transform(self.dataset[idx]) if self.transform else self.dataset[idx]\n",
        "\n",
        "    x = data[:self.lookback].unsqueeze(-1)\n",
        "    y = data[self.lookback:]\n",
        "\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSar_zTr8X0y"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "def get_loaders(dataframe, time_bin, scaler, lookback, lookforward,batch_size: int, train_size:float,  transform=None):\n",
        "\n",
        "    dataframe['TimeBin'] = df['Timestamp'].dt.floor(f'{time_bin}min')\n",
        "    grouped_df = dataframe.groupby('TimeBin').agg(\n",
        "                Price = ('Price.', 'mean'))\n",
        "    lookforward -= 1\n",
        "\n",
        "    grouped_df['ScaledPrice'] = scaler.fit_transform(grouped_df[['Price']])\n",
        "\n",
        "    for i in range((lookback+lookforward), 0, -1):\n",
        "\n",
        "      grouped_df[f'price_lag_{i}'] = grouped_df['ScaledPrice'].shift(i)\n",
        "\n",
        "    grouped_df.dropna(inplace=True)\n",
        "\n",
        "    lag_cols = [f'price_lag_{i}' for i in range(lookback+lookforward, 0, -1)]\n",
        "    lag_cols.append('ScaledPrice')\n",
        "\n",
        "    data = torch.tensor(grouped_df[lag_cols].values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    index = int(len(grouped_df) * train_size)\n",
        "\n",
        "    train_raw = data[:index, :]\n",
        "    test_raw = data[index:, :]\n",
        "\n",
        "    trainset = GoldDataset(train_raw, lookback, transform)\n",
        "    testset = GoldDataset(test_raw, lookback, None)\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return trainset, train_loader, testset, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKjGBmkY7-m7"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "  def __init__(self, sample_mean=3):\n",
        "\n",
        "    super().__init__()\n",
        "    self.sample_mean = sample_mean\n",
        "\n",
        "  def forward(self, x):\n",
        "    last_samples = x[:, -self.sample_mean:, :]   # shape: [batch, 2, 1]\n",
        "    return last_samples.mean(dim=1)"
      ],
      "metadata": {
        "id": "OO4lvKmeFkDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vp1OQNz6iIw"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_stacked_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_stacked_layers = num_stacked_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvlZWAeQ0d3e"
      },
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_stacked_layers, output_size, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_stacked_layers\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_stacked_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_stacked_layers > 1 else 0.0\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, C]\n",
        "        B = x.size(0)\n",
        "        h0 = torch.zeros(self.num_layers, B, self.hidden_size, device=x.device)\n",
        "        out, _ = self.gru(x, h0)     # out: [B, T, H]\n",
        "        out = out[:, -1, :]          # last timestep\n",
        "        out = self.fc(out)           # [B, output_size]\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lqj166D06lE"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)                  # [T, D]\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)    # [T, 1]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)  # not a parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, D]\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:T].unsqueeze(0)   # broadcast to [B, T, D]\n",
        "\n",
        "class TransformerTS(nn.Module):\n",
        "    \"\"\"\n",
        "    Projects input_size -> d_model, adds positional encoding,\n",
        "    passes through TransformerEncoder, takes last timestep, then a head.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 num_encoder_layers,\n",
        "                 dim_feedforward,\n",
        "                 output_size,\n",
        "                 dropout=0.1,\n",
        "                 layer_norm_eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_encoder_layers,\n",
        "            norm=nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        # x: [B, T, C]\n",
        "        x = self.input_proj(x)            # [B, T, D]\n",
        "        x = self.pos_enc(x)               # [B, T, D]\n",
        "        # src_key_padding_mask: [B, T] with True for PAD positions (optional)\n",
        "        h = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # [B, T, D]\n",
        "        h_last = h[:, -1, :]              # last timestep representation\n",
        "        out = self.head(h_last)           # [B, output_size]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51tGBmNs8CG7"
      },
      "source": [
        "# training & test regime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzKs_3miAgc_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0, mode='min', verbose=False, save_path='checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        patience : int         # how many epochs to wait without improvement\n",
        "        min_delta : float      # minimum change to count as an improvement\n",
        "        mode : 'max' | 'min'   # 'max' for accuracy, 'min' for loss\n",
        "        verbose : bool         # print messages when improvement happens\n",
        "        save_path : str        # path to save the best model\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.verbose = verbose\n",
        "        self.save_path = save_path\n",
        "\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_best = np.inf if mode == 'min' else -np.inf\n",
        "\n",
        "    def __call__(self, val_metric, model):\n",
        "        score = val_metric\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            if self.best_score is None:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "            elif score > self.best_score - self.min_delta:\n",
        "                self.counter += 1\n",
        "                if self.verbose:\n",
        "                    print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "                if self.counter >= self.patience:\n",
        "                    self.early_stop = True\n",
        "            else:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "                self.counter = 0\n",
        "        else:  # mode == 'max'\n",
        "            if self.best_score is None:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "            elif score < self.best_score + self.min_delta:\n",
        "                self.counter += 1\n",
        "                if self.verbose:\n",
        "                    print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "                if self.counter >= self.patience:\n",
        "                    self.early_stop = True\n",
        "            else:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "                self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        \"\"\"Saves model when validation metric improves.\"\"\"\n",
        "        torch.save(model.state_dict(), self.save_path)\n",
        "        if self.verbose:\n",
        "            print(f\"Model improved. Saving model to {self.save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PjYtPD18BgK"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, show=True):\n",
        "\n",
        "    model.train(True)\n",
        "\n",
        "    if show:\n",
        "      print(f'Epoch: {epoch + 1}')\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_index, batch in enumerate(train_loader):\n",
        "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "        output = model(x_batch)\n",
        "        loss = loss_function(output, y_batch)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_index % 100 == 99:  # print every 100 batches\n",
        "            avg_loss_across_batches = running_loss / 100\n",
        "            if show:\n",
        "              print('Batch {0}, Loss: {1:.3f}'.format(batch_index+1,\n",
        "                                                    avg_loss_across_batches))\n",
        "            running_loss = 0.0\n",
        "    if show:\n",
        "      print()\n",
        "\n",
        "def validate_one_epoch(model,early_stopping, show=True):\n",
        "    model.train(False)\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_index, batch in enumerate(test_loader):\n",
        "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(x_batch)\n",
        "            loss = loss_function(output, y_batch)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    avg_loss_across_batches = running_loss / len(test_loader)\n",
        "    early_stopping(avg_loss_across_batches, model)\n",
        "\n",
        "\n",
        "    if show:\n",
        "      print('Val Loss: {0:.5f}'.format(avg_loss_across_batches))\n",
        "      print('***************************************************')\n",
        "      print()\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "      return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWLksV-tKKRC"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7OXYXinKPqH"
      },
      "outputs": [],
      "source": [
        "def _mae(y, yhat):\n",
        "    return np.mean(np.abs(y - yhat))\n",
        "\n",
        "def _rmse(y, yhat):\n",
        "    return np.sqrt(np.mean((y - yhat) ** 2))\n",
        "\n",
        "def _mape(y, yhat, eps=1e-8):\n",
        "    denom = np.maximum(np.abs(y), eps)\n",
        "    return 100.0 * np.mean(np.abs((y - yhat) / denom))\n",
        "\n",
        "def _smape(y, yhat, eps=1e-8):\n",
        "    denom = np.maximum((np.abs(y) + np.abs(yhat)) / 2.0, eps)\n",
        "    return 100.0 * np.mean(np.abs(y - yhat) / denom)\n",
        "\n",
        "def _r2(y, yhat):\n",
        "    ss_res = np.sum((y - yhat) ** 2)\n",
        "    ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
        "    return 1.0 - ss_res / ss_tot if ss_tot > 0 else np.nan\n",
        "\n",
        "def _mase(y_insample, y_insample_naive, y, yhat, eps=1e-12):\n",
        "    \"\"\"\n",
        "    y_insample: series used to compute naive scale (e.g., train_true[1:])\n",
        "    y_insample_naive: naive preds for insample (e.g., train_true[:-1])\n",
        "    MASE = MAE(model)/MAE(naive) using in-sample naive MAE as the scale.\n",
        "    \"\"\"\n",
        "    scale = _mae(y_insample, y_insample_naive)\n",
        "    return _mae(y, yhat) / (scale + eps)\n",
        "\n",
        "def evaluate_forecast(out_dict, horizon_step=0):\n",
        "\n",
        "    def select_step(arr):\n",
        "      if arr.ndim == 2:\n",
        "          return arr[:, horizon_step]\n",
        "      return arr\n",
        "\n",
        "    train_pred = select_step(out_dict[\"train_pred_inv\"])\n",
        "    train_true = select_step(out_dict[\"train_true_inv\"])\n",
        "    test_pred  = select_step(out_dict[\"test_pred_inv\"])\n",
        "    test_true  = select_step(out_dict[\"test_true_inv\"])\n",
        "    # ensure 1D & drop NaNs if any\n",
        "    def _clean(a):\n",
        "        a = np.asarray(a).reshape(-1)\n",
        "        return a[~np.isnan(a)]\n",
        "    train_true = _clean(train_true); train_pred = _clean(train_pred)\n",
        "    test_true  = _clean(test_true);  test_pred  = _clean(test_pred)\n",
        "\n",
        "    # In-sample naive (one-step) for MASE scale: y_{t-1}\n",
        "    # Align lengths: compare train_true[1:] vs naive = train_true[:-1]\n",
        "    if len(train_true) >= 2:\n",
        "        insample_y = train_true[1:]\n",
        "        insample_naive = train_true[:-1]\n",
        "        mase_train = _mase(insample_y, insample_naive, train_true, train_pred)\n",
        "        mase_test  = _mase(insample_y, insample_naive, test_true,  test_pred)\n",
        "    else:\n",
        "        mase_train = np.nan\n",
        "        mase_test  = np.nan\n",
        "\n",
        "    metrics_train = {\n",
        "        \"MAE\":  _mae(train_true, train_pred),\n",
        "        \"RMSE\": _rmse(train_true, train_pred),\n",
        "        \"MAPE\": _mape(train_true, train_pred),\n",
        "        \"sMAPE\": _smape(train_true, train_pred),\n",
        "        \"R2\":   _r2(train_true, train_pred),\n",
        "        \"MASE\": mase_train,\n",
        "    }\n",
        "    metrics_test = {\n",
        "        \"MAE\":  _mae(test_true, test_pred),\n",
        "        \"RMSE\": _rmse(test_true, test_pred),\n",
        "        \"MAPE\": _mape(test_true, test_pred),\n",
        "        \"sMAPE\": _smape(test_true, test_pred),\n",
        "        \"R2\":   _r2(test_true, test_pred),\n",
        "        \"MASE\": mase_test,\n",
        "    }\n",
        "    return {\"train\": metrics_train, \"test\": metrics_test}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jht49w2-Wdf_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def _predict_on_loader(model, loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    preds, trues = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)                 # [B, lookback, 1]\n",
        "        y = y.to(device)                 # [B, horizon]\n",
        "        out = model(x)                   # expect [B, horizon] (or [B, horizon, 1] -> squeeze)\n",
        "        if out.dim() == 3 and out.size(-1) == 1:\n",
        "            out = out.squeeze(-1)\n",
        "        preds.append(out.detach().cpu())\n",
        "        trues.append(y.detach().cpu())\n",
        "\n",
        "    return torch.cat(preds, dim=0), torch.cat(trues, dim=0)  # [N, horizon], [N, horizon]\n",
        "\n",
        "def _inverse_scale_2d(arr_2d, scaler):\n",
        "    \"\"\"\n",
        "    arr_2d: numpy array [N, H] in scaler space.\n",
        "    scaler: fitted MinMaxScaler on price column.\n",
        "    \"\"\"\n",
        "    flat = arr_2d.reshape(-1, 1)\n",
        "    inv = scaler.inverse_transform(flat)\n",
        "    return inv.reshape(arr_2d.shape)\n",
        "\n",
        "def predict_train_test(\n",
        "    model,\n",
        "    trainset,\n",
        "    testset,\n",
        "    batch_size=256,\n",
        "    device=None,\n",
        "    scaler=None,          # pass if you want inverse-scaled outputs\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dict with predictions and targets for train and test.\n",
        "    Shapes are [N, horizon] for both preds and targets.\n",
        "    If `scaler` is provided, adds *_inv with inverse-scaled prices.\n",
        "    \"\"\"\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n",
        "    test_loader  = DataLoader(testset,  batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n",
        "\n",
        "    train_pred, train_true = _predict_on_loader(model, train_loader, device)\n",
        "    test_pred,  test_true  = _predict_on_loader(model, test_loader,  device)\n",
        "\n",
        "    out = {\n",
        "        \"train_pred\": train_pred.numpy(),   # scaled\n",
        "        \"train_true\": train_true.numpy(),\n",
        "        \"test_pred\":  test_pred.numpy(),\n",
        "        \"test_true\":  test_true.numpy(),\n",
        "    }\n",
        "\n",
        "    if scaler is not None:\n",
        "        out[\"train_pred_inv\"] = _inverse_scale_2d(out[\"train_pred\"], scaler)\n",
        "        out[\"train_true_inv\"] = _inverse_scale_2d(out[\"train_true\"], scaler)\n",
        "        out[\"test_pred_inv\"]  = _inverse_scale_2d(out[\"test_pred\"],  scaler)\n",
        "        out[\"test_true_inv\"]  = _inverse_scale_2d(out[\"test_true\"],  scaler)\n",
        "\n",
        "\n",
        "    return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdnp5LgEIC2J"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_predictions_plotly(out_dict, horizon_step=0, title=\"Prediction vs Actual (Train/Test)\"):\n",
        "    \"\"\"\n",
        "    out_dict: dictionary with keys:\n",
        "      train_pred_inv, train_true_inv, test_pred_inv, test_true_inv\n",
        "      Each should be numpy arrays [N, H] or [N] if single step\n",
        "    horizon_step: which forecast step to plot (0 = next step)\n",
        "    \"\"\"\n",
        "\n",
        "    # pick correct horizon step\n",
        "    def select_step(arr):\n",
        "        if arr.ndim == 2:\n",
        "            return arr[:, horizon_step]\n",
        "        return arr\n",
        "\n",
        "    train_pred = select_step(out_dict[\"train_pred_inv\"])\n",
        "    train_true = select_step(out_dict[\"train_true_inv\"])\n",
        "    test_pred  = select_step(out_dict[\"test_pred_inv\"])\n",
        "    test_true  = select_step(out_dict[\"test_true_inv\"])\n",
        "\n",
        "    n_train = len(train_pred)\n",
        "    n_test  = len(test_pred)\n",
        "\n",
        "    # x axes\n",
        "    x_train = list(range(n_train))\n",
        "    x_test  = list(range(n_train, n_train + n_test))\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Train actual\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_train, y=train_true,\n",
        "        mode='lines', name='Train Actual',\n",
        "        line=dict(color='blue')\n",
        "    ))\n",
        "\n",
        "    # Train prediction\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_train, y=train_pred,\n",
        "        mode='lines', name='Train Pred',\n",
        "        line=dict(color='blue', dash='dash')\n",
        "    ))\n",
        "\n",
        "    # Test actual\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_test, y=test_true,\n",
        "        mode='lines', name='Test Actual',\n",
        "        line=dict(color='orange')\n",
        "    ))\n",
        "\n",
        "    # Test prediction\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_test, y=test_pred,\n",
        "        mode='lines', name='Test Pred',\n",
        "        line=dict(color='orange', dash='dash')\n",
        "    ))\n",
        "\n",
        "    # vertical line for train/test split\n",
        "    fig.add_vline(\n",
        "        x=n_train - 0.5,\n",
        "        line_width=2, line_dash=\"dot\", line_color=\"black\"\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"Sample Index (time ordered)\",\n",
        "        yaxis_title=\"Price\",\n",
        "        legend=dict(x=0, y=1),\n",
        "        hovermode=\"x unified\",\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9OWAs-oL0kk"
      },
      "outputs": [],
      "source": [
        "def plot_test_predictions_plotly(\n",
        "    results,\n",
        "    horizon_step: int = 0,\n",
        "    title: str = \"Prediction vs Actual (Test Only)\"\n",
        "):\n",
        "    \"\"\"\n",
        "    results: dict with structure\n",
        "        {\n",
        "            \"test_true_inv\": np.ndarray [N] or [N, H],\n",
        "            \"preds\": {\n",
        "                \"LSTM\": np.ndarray [N] or [N, H],\n",
        "                \"GRU\":  np.ndarray [N] or [N, H],\n",
        "                \"Transformer\": np.ndarray [N] or [N, H],\n",
        "                ...\n",
        "            }\n",
        "        }\n",
        "    horizon_step: which forecast step to plot (0 = next step)\n",
        "    \"\"\"\n",
        "\n",
        "    def select_step(arr):\n",
        "        # Accept shape [N] or [N, H]\n",
        "        if arr.ndim == 2:\n",
        "            if horizon_step < 0 or horizon_step >= arr.shape[1]:\n",
        "                raise ValueError(f\"horizon_step {horizon_step} out of range for array with shape {arr.shape}.\")\n",
        "            return arr[:, horizon_step]\n",
        "        return arr\n",
        "\n",
        "    # Extract and validate y_true\n",
        "    if \"test_true_inv\" not in results or \"preds\" not in results:\n",
        "        raise KeyError('Expected keys: \"test_true_inv\" and \"preds\" in results.')\n",
        "\n",
        "    y_true_full = np.asarray(results[\"test_true_inv\"])\n",
        "    y_true = select_step(y_true_full)\n",
        "    N = len(y_true)\n",
        "\n",
        "    # Prepare x axis (test indices only)\n",
        "    x = np.arange(N)\n",
        "\n",
        "    # Build figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Actual test series (solid line)\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x, y=y_true,\n",
        "        mode=\"lines\",\n",
        "        name=\"Test Actual\",\n",
        "        line=dict(color=\"black\")\n",
        "    ))\n",
        "\n",
        "    # Add one dashed line per model\n",
        "    for model_name, y_pred_full in results[\"preds\"].items():\n",
        "        y_pred = select_step(np.asarray(y_pred_full))\n",
        "\n",
        "        if len(y_pred) != N:\n",
        "            raise ValueError(\n",
        "                f'Length mismatch for \"{model_name}\": got {len(y_pred)} vs y_true length {N}.'\n",
        "            )\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=x, y=y_pred,\n",
        "            mode=\"lines\",\n",
        "            name=f\"{model_name} Pred\",\n",
        "            line=dict(dash=\"dash\")  # color auto-cycles\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"Test Sample Index (time ordered)\",\n",
        "        yaxis_title=\"Price\",\n",
        "        legend=dict(x=0, y=1),\n",
        "        hovermode=\"x unified\",\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLAQbHGyHIQ"
      },
      "source": [
        "# Concat all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvb1VaCsyQw6"
      },
      "outputs": [],
      "source": [
        "augmentations = Compose([AddGaussianNoise()])#, RandomScaling()])\n",
        "\n",
        "trainset, train_loader, testset, test_loader = get_loaders(df_no_outliers, time_bin=60, scaler=scaler, lookback=12,\n",
        "                                        lookforward=1, batch_size=8, train_size=0.9,\n",
        "                                        transform=augmentations)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 200\n",
        "input_size=1\n",
        "d_model=128\n",
        "nhead=8\n",
        "num_encoder_layers=1\n",
        "dim_feedforward=64\n",
        "output_size=1\n",
        "dropout=0.1"
      ],
      "metadata": {
        "id": "QqKKnKl56CJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0yyg3gsySEq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(patience=50, mode='min', verbose=False, save_path='/content/checkpoint_transformer.pt')\n",
        "\n",
        "\n",
        "model = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size=output_size,\n",
        "    dropout=dropout)\n",
        "\n",
        "# model = GRUModel(1, 4, 1, 1)\n",
        "model.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, show=False)\n",
        "    stop_criteria = validate_one_epoch(model, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtHiKv5A-kfM"
      },
      "source": [
        "# Models Comparison"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "output_size = 1\n",
        "num_hidden_layers = 4\n",
        "num_stacked_layers = 1\n",
        "patience = 30\n",
        "num_epochs = 5000\n",
        "learning_rate = 0.0005"
      ],
      "metadata": {
        "id": "f4H-tZb0wdrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB3twgcd-nal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9825efef-fc2a-4615-a6bf-784152132f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.00408\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00356\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00457\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00413\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00511\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00425\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00434\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00435\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00428\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00394\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00377\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00420\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00399\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00361\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00384\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00402\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00446\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00362\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00392\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00370\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00381\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00345\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00348\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00349\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00405\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00460\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00372\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00379\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00351\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00439\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00394\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00423\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00371\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00383\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00309\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00280\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00354\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00406\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00357\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00334\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00344\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00410\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00389\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00322\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00410\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00353\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00364\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00384\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00347\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00453\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00362\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00349\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00371\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00416\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00428\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00409\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00327\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00381\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00358\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00332\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00445\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00369\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00322\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00401\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00395\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00319\n",
            "***************************************************\n",
            "\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "# train LSTM\n",
        "lstm_model = LSTM(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "\n",
        "lstm_model.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
        "early_stopping = EarlyStopping(patience=patience, mode='min', verbose=False, save_path='/content/checkpoint_lstm.pt')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(lstm_model, show=False)\n",
        "\n",
        "    stop_criteria = validate_one_epoch(lstm_model, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnpSfTEE_XHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7c882111-1c0b-466e-c457-c59f3138d223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "\n",
            "Val Loss: 1.00405\n",
            "***************************************************\n",
            "\n",
            "Epoch: 2\n",
            "\n",
            "Val Loss: 0.75313\n",
            "***************************************************\n",
            "\n",
            "Epoch: 3\n",
            "\n",
            "Val Loss: 0.53319\n",
            "***************************************************\n",
            "\n",
            "Epoch: 4\n",
            "\n",
            "Val Loss: 0.36551\n",
            "***************************************************\n",
            "\n",
            "Epoch: 5\n",
            "\n",
            "Val Loss: 0.25250\n",
            "***************************************************\n",
            "\n",
            "Epoch: 6\n",
            "\n",
            "Val Loss: 0.18785\n",
            "***************************************************\n",
            "\n",
            "Epoch: 7\n",
            "\n",
            "Val Loss: 0.15519\n",
            "***************************************************\n",
            "\n",
            "Epoch: 8\n",
            "\n",
            "Val Loss: 0.13744\n",
            "***************************************************\n",
            "\n",
            "Epoch: 9\n",
            "\n",
            "Val Loss: 0.12689\n",
            "***************************************************\n",
            "\n",
            "Epoch: 10\n",
            "\n",
            "Val Loss: 0.11706\n",
            "***************************************************\n",
            "\n",
            "Epoch: 11\n",
            "\n",
            "Val Loss: 0.11144\n",
            "***************************************************\n",
            "\n",
            "Epoch: 12\n",
            "\n",
            "Val Loss: 0.10242\n",
            "***************************************************\n",
            "\n",
            "Epoch: 13\n",
            "\n",
            "Val Loss: 0.09784\n",
            "***************************************************\n",
            "\n",
            "Epoch: 14\n",
            "\n",
            "Val Loss: 0.09155\n",
            "***************************************************\n",
            "\n",
            "Epoch: 15\n",
            "\n",
            "Val Loss: 0.08592\n",
            "***************************************************\n",
            "\n",
            "Epoch: 16\n",
            "\n",
            "Val Loss: 0.08139\n",
            "***************************************************\n",
            "\n",
            "Epoch: 17\n",
            "\n",
            "Val Loss: 0.07861\n",
            "***************************************************\n",
            "\n",
            "Epoch: 18\n",
            "\n",
            "Val Loss: 0.07440\n",
            "***************************************************\n",
            "\n",
            "Epoch: 19\n",
            "\n",
            "Val Loss: 0.07065\n",
            "***************************************************\n",
            "\n",
            "Epoch: 20\n",
            "\n",
            "Val Loss: 0.06725\n",
            "***************************************************\n",
            "\n",
            "Epoch: 21\n",
            "\n",
            "Val Loss: 0.06436\n",
            "***************************************************\n",
            "\n",
            "Epoch: 22\n",
            "\n",
            "Val Loss: 0.06234\n",
            "***************************************************\n",
            "\n",
            "Epoch: 23\n",
            "\n",
            "Val Loss: 0.05935\n",
            "***************************************************\n",
            "\n",
            "Epoch: 24\n",
            "\n",
            "Val Loss: 0.05583\n",
            "***************************************************\n",
            "\n",
            "Epoch: 25\n",
            "\n",
            "Val Loss: 0.05475\n",
            "***************************************************\n",
            "\n",
            "Epoch: 26\n",
            "\n",
            "Val Loss: 0.05185\n",
            "***************************************************\n",
            "\n",
            "Epoch: 27\n",
            "\n",
            "Val Loss: 0.04995\n",
            "***************************************************\n",
            "\n",
            "Epoch: 28\n",
            "\n",
            "Val Loss: 0.04824\n",
            "***************************************************\n",
            "\n",
            "Epoch: 29\n",
            "\n",
            "Val Loss: 0.04749\n",
            "***************************************************\n",
            "\n",
            "Epoch: 30\n",
            "\n",
            "Val Loss: 0.04555\n",
            "***************************************************\n",
            "\n",
            "Epoch: 31\n",
            "\n",
            "Val Loss: 0.04439\n",
            "***************************************************\n",
            "\n",
            "Epoch: 32\n",
            "\n",
            "Val Loss: 0.04177\n",
            "***************************************************\n",
            "\n",
            "Epoch: 33\n",
            "\n",
            "Val Loss: 0.04130\n",
            "***************************************************\n",
            "\n",
            "Epoch: 34\n",
            "\n",
            "Val Loss: 0.03887\n",
            "***************************************************\n",
            "\n",
            "Epoch: 35\n",
            "\n",
            "Val Loss: 0.03850\n",
            "***************************************************\n",
            "\n",
            "Epoch: 36\n",
            "\n",
            "Val Loss: 0.03617\n",
            "***************************************************\n",
            "\n",
            "Epoch: 37\n",
            "\n",
            "Val Loss: 0.03563\n",
            "***************************************************\n",
            "\n",
            "Epoch: 38\n",
            "\n",
            "Val Loss: 0.03456\n",
            "***************************************************\n",
            "\n",
            "Epoch: 39\n",
            "\n",
            "Val Loss: 0.03204\n",
            "***************************************************\n",
            "\n",
            "Epoch: 40\n",
            "\n",
            "Val Loss: 0.03252\n",
            "***************************************************\n",
            "\n",
            "Epoch: 41\n",
            "\n",
            "Val Loss: 0.03209\n",
            "***************************************************\n",
            "\n",
            "Epoch: 42\n",
            "\n",
            "Val Loss: 0.03005\n",
            "***************************************************\n",
            "\n",
            "Epoch: 43\n",
            "\n",
            "Val Loss: 0.02869\n",
            "***************************************************\n",
            "\n",
            "Epoch: 44\n",
            "\n",
            "Val Loss: 0.02758\n",
            "***************************************************\n",
            "\n",
            "Epoch: 45\n",
            "\n",
            "Val Loss: 0.02831\n",
            "***************************************************\n",
            "\n",
            "Epoch: 46\n",
            "\n",
            "Val Loss: 0.02732\n",
            "***************************************************\n",
            "\n",
            "Epoch: 47\n",
            "\n",
            "Val Loss: 0.02539\n",
            "***************************************************\n",
            "\n",
            "Epoch: 48\n",
            "\n",
            "Val Loss: 0.02554\n",
            "***************************************************\n",
            "\n",
            "Epoch: 49\n",
            "\n",
            "Val Loss: 0.02469\n",
            "***************************************************\n",
            "\n",
            "Epoch: 50\n",
            "\n",
            "Val Loss: 0.02377\n",
            "***************************************************\n",
            "\n",
            "Epoch: 51\n",
            "\n",
            "Val Loss: 0.02224\n",
            "***************************************************\n",
            "\n",
            "Epoch: 52\n",
            "\n",
            "Val Loss: 0.02250\n",
            "***************************************************\n",
            "\n",
            "Epoch: 53\n",
            "\n",
            "Val Loss: 0.02124\n",
            "***************************************************\n",
            "\n",
            "Epoch: 54\n",
            "\n",
            "Val Loss: 0.02160\n",
            "***************************************************\n",
            "\n",
            "Epoch: 55\n",
            "\n",
            "Val Loss: 0.02212\n",
            "***************************************************\n",
            "\n",
            "Epoch: 56\n",
            "\n",
            "Val Loss: 0.02148\n",
            "***************************************************\n",
            "\n",
            "Epoch: 57\n",
            "\n",
            "Val Loss: 0.02022\n",
            "***************************************************\n",
            "\n",
            "Epoch: 58\n",
            "\n",
            "Val Loss: 0.01856\n",
            "***************************************************\n",
            "\n",
            "Epoch: 59\n",
            "\n",
            "Val Loss: 0.01864\n",
            "***************************************************\n",
            "\n",
            "Epoch: 60\n",
            "\n",
            "Val Loss: 0.01712\n",
            "***************************************************\n",
            "\n",
            "Epoch: 61\n",
            "\n",
            "Val Loss: 0.01836\n",
            "***************************************************\n",
            "\n",
            "Epoch: 62\n",
            "\n",
            "Val Loss: 0.01885\n",
            "***************************************************\n",
            "\n",
            "Epoch: 63\n",
            "\n",
            "Val Loss: 0.01704\n",
            "***************************************************\n",
            "\n",
            "Epoch: 64\n",
            "\n",
            "Val Loss: 0.01662\n",
            "***************************************************\n",
            "\n",
            "Epoch: 65\n",
            "\n",
            "Val Loss: 0.01530\n",
            "***************************************************\n",
            "\n",
            "Epoch: 66\n",
            "\n",
            "Val Loss: 0.01724\n",
            "***************************************************\n",
            "\n",
            "Epoch: 67\n",
            "\n",
            "Val Loss: 0.01598\n",
            "***************************************************\n",
            "\n",
            "Epoch: 68\n",
            "\n",
            "Val Loss: 0.01614\n",
            "***************************************************\n",
            "\n",
            "Epoch: 69\n",
            "\n",
            "Val Loss: 0.01422\n",
            "***************************************************\n",
            "\n",
            "Epoch: 70\n",
            "\n",
            "Val Loss: 0.01464\n",
            "***************************************************\n",
            "\n",
            "Epoch: 71\n",
            "\n",
            "Val Loss: 0.01478\n",
            "***************************************************\n",
            "\n",
            "Epoch: 72\n",
            "\n",
            "Val Loss: 0.01297\n",
            "***************************************************\n",
            "\n",
            "Epoch: 73\n",
            "\n",
            "Val Loss: 0.01425\n",
            "***************************************************\n",
            "\n",
            "Epoch: 74\n",
            "\n",
            "Val Loss: 0.01291\n",
            "***************************************************\n",
            "\n",
            "Epoch: 75\n",
            "\n",
            "Val Loss: 0.01251\n",
            "***************************************************\n",
            "\n",
            "Epoch: 76\n",
            "\n",
            "Val Loss: 0.01274\n",
            "***************************************************\n",
            "\n",
            "Epoch: 77\n",
            "\n",
            "Val Loss: 0.01306\n",
            "***************************************************\n",
            "\n",
            "Epoch: 78\n",
            "\n",
            "Val Loss: 0.01201\n",
            "***************************************************\n",
            "\n",
            "Epoch: 79\n",
            "\n",
            "Val Loss: 0.01211\n",
            "***************************************************\n",
            "\n",
            "Epoch: 80\n",
            "\n",
            "Val Loss: 0.01110\n",
            "***************************************************\n",
            "\n",
            "Epoch: 81\n",
            "\n",
            "Val Loss: 0.01057\n",
            "***************************************************\n",
            "\n",
            "Epoch: 82\n",
            "\n",
            "Val Loss: 0.01164\n",
            "***************************************************\n",
            "\n",
            "Epoch: 83\n",
            "\n",
            "Val Loss: 0.00967\n",
            "***************************************************\n",
            "\n",
            "Epoch: 84\n",
            "\n",
            "Val Loss: 0.01056\n",
            "***************************************************\n",
            "\n",
            "Epoch: 85\n",
            "\n",
            "Val Loss: 0.01081\n",
            "***************************************************\n",
            "\n",
            "Epoch: 86\n",
            "\n",
            "Val Loss: 0.01080\n",
            "***************************************************\n",
            "\n",
            "Epoch: 87\n",
            "\n",
            "Val Loss: 0.01029\n",
            "***************************************************\n",
            "\n",
            "Epoch: 88\n",
            "\n",
            "Val Loss: 0.01114\n",
            "***************************************************\n",
            "\n",
            "Epoch: 89\n",
            "\n",
            "Val Loss: 0.00937\n",
            "***************************************************\n",
            "\n",
            "Epoch: 90\n",
            "\n",
            "Val Loss: 0.01008\n",
            "***************************************************\n",
            "\n",
            "Epoch: 91\n",
            "\n",
            "Val Loss: 0.00876\n",
            "***************************************************\n",
            "\n",
            "Epoch: 92\n",
            "\n",
            "Val Loss: 0.00827\n",
            "***************************************************\n",
            "\n",
            "Epoch: 93\n",
            "\n",
            "Val Loss: 0.00888\n",
            "***************************************************\n",
            "\n",
            "Epoch: 94\n",
            "\n",
            "Val Loss: 0.01093\n",
            "***************************************************\n",
            "\n",
            "Epoch: 95\n",
            "\n",
            "Val Loss: 0.00815\n",
            "***************************************************\n",
            "\n",
            "Epoch: 96\n",
            "\n",
            "Val Loss: 0.00887\n",
            "***************************************************\n",
            "\n",
            "Epoch: 97\n",
            "\n",
            "Val Loss: 0.00744\n",
            "***************************************************\n",
            "\n",
            "Epoch: 98\n",
            "\n",
            "Val Loss: 0.00843\n",
            "***************************************************\n",
            "\n",
            "Epoch: 99\n",
            "\n",
            "Val Loss: 0.00787\n",
            "***************************************************\n",
            "\n",
            "Epoch: 100\n",
            "\n",
            "Val Loss: 0.00868\n",
            "***************************************************\n",
            "\n",
            "Epoch: 101\n",
            "\n",
            "Val Loss: 0.00884\n",
            "***************************************************\n",
            "\n",
            "Epoch: 102\n",
            "\n",
            "Val Loss: 0.00743\n",
            "***************************************************\n",
            "\n",
            "Epoch: 103\n",
            "\n",
            "Val Loss: 0.00799\n",
            "***************************************************\n",
            "\n",
            "Epoch: 104\n",
            "\n",
            "Val Loss: 0.00768\n",
            "***************************************************\n",
            "\n",
            "Epoch: 105\n",
            "\n",
            "Val Loss: 0.00820\n",
            "***************************************************\n",
            "\n",
            "Epoch: 106\n",
            "\n",
            "Val Loss: 0.00820\n",
            "***************************************************\n",
            "\n",
            "Epoch: 107\n",
            "\n",
            "Val Loss: 0.00837\n",
            "***************************************************\n",
            "\n",
            "Epoch: 108\n",
            "\n",
            "Val Loss: 0.00803\n",
            "***************************************************\n",
            "\n",
            "Epoch: 109\n",
            "\n",
            "Val Loss: 0.00621\n",
            "***************************************************\n",
            "\n",
            "Epoch: 110\n",
            "\n",
            "Val Loss: 0.00705\n",
            "***************************************************\n",
            "\n",
            "Epoch: 111\n",
            "\n",
            "Val Loss: 0.00835\n",
            "***************************************************\n",
            "\n",
            "Epoch: 112\n",
            "\n",
            "Val Loss: 0.00686\n",
            "***************************************************\n",
            "\n",
            "Epoch: 113\n",
            "\n",
            "Val Loss: 0.00688\n",
            "***************************************************\n",
            "\n",
            "Epoch: 114\n",
            "\n",
            "Val Loss: 0.00807\n",
            "***************************************************\n",
            "\n",
            "Epoch: 115\n",
            "\n",
            "Val Loss: 0.00583\n",
            "***************************************************\n",
            "\n",
            "Epoch: 116\n",
            "\n",
            "Val Loss: 0.00672\n",
            "***************************************************\n",
            "\n",
            "Epoch: 117\n",
            "\n",
            "Val Loss: 0.00636\n",
            "***************************************************\n",
            "\n",
            "Epoch: 118\n",
            "\n",
            "Val Loss: 0.00627\n",
            "***************************************************\n",
            "\n",
            "Epoch: 119\n",
            "\n",
            "Val Loss: 0.00766\n",
            "***************************************************\n",
            "\n",
            "Epoch: 120\n",
            "\n",
            "Val Loss: 0.00650\n",
            "***************************************************\n",
            "\n",
            "Epoch: 121\n",
            "\n",
            "Val Loss: 0.00683\n",
            "***************************************************\n",
            "\n",
            "Epoch: 122\n",
            "\n",
            "Val Loss: 0.00678\n",
            "***************************************************\n",
            "\n",
            "Epoch: 123\n",
            "\n",
            "Val Loss: 0.00658\n",
            "***************************************************\n",
            "\n",
            "Epoch: 124\n",
            "\n",
            "Val Loss: 0.00677\n",
            "***************************************************\n",
            "\n",
            "Epoch: 125\n",
            "\n",
            "Val Loss: 0.00649\n",
            "***************************************************\n",
            "\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "gru_model = GRUModel(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "gru_model.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(gru_model.parameters(), lr=learning_rate)\n",
        "early_stopping = EarlyStopping(patience=10, mode='min', verbose=False, save_path='/content/checkpoint_gru.pt')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(gru_model)\n",
        "    stop_criteria = validate_one_epoch(gru_model, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ik57HvnA52C"
      },
      "outputs": [],
      "source": [
        "lstm_model = LSTM(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "lstm_model.load_state_dict(torch.load('/content/checkpoint_lstm.pt'))\n",
        "\n",
        "result = predict_train_test(lstm_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "lstm_metrics = evaluate_forecast(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-X2adOZAltq"
      },
      "outputs": [],
      "source": [
        "gru_model = GRUModel(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "gru_model.load_state_dict(torch.load('/content/checkpoint_gru.pt'))\n",
        "result = predict_train_test(gru_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "gru_metrics = evaluate_forecast(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size=output_size,\n",
        "    dropout=dropout)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/checkpoint_transformer.pt'))\n",
        "result = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "transformer_metrics = evaluate_forecast(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsPk1D-E2lvT",
        "outputId": "e1758466-d580-4c0e-817f-74fd06165b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnpl-CmQHqeK"
      },
      "outputs": [],
      "source": [
        "def get_params_flops(model, input_size):\n",
        "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    with torch.cuda.device(0):\n",
        "        macs, _ = get_model_complexity_info(model, input_size, as_strings=False, print_per_layer_stat=False)\n",
        "    flops = macs * 2  # MACs to FLOPs\n",
        "    return params, flops\n",
        "\n",
        "def human_readable(num):\n",
        "    for unit in [\"\", \"K\", \"M\", \"B\"]:\n",
        "        if abs(num) < 1000:\n",
        "            return f\"{num:.2f} {unit}\"\n",
        "        num /= 1000\n",
        "    return f\"{num:.2f} T\"  # trillion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36r5KiGNHw3F"
      },
      "outputs": [],
      "source": [
        "model_info = {}\n",
        "for name, model in {\n",
        "    \"LSTM\": lstm_model,\n",
        "    \"GRU\": gru_model,\n",
        "    \"Transformer\": model\n",
        "}.items():\n",
        "    params, flops = get_params_flops(model, (30, 1))  # (seq_len, input_size)\n",
        "    model_info[name] = {\"Params\": human_readable(params), \"FLOPs\": human_readable(flops)}\n",
        "\n",
        "\n",
        "df_info = pd.DataFrame(model_info).T\n",
        "df_info = df_info.reset_index().rename(columns={'index':'Model'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "GRas0whxAm0f",
        "outputId": "42b60aed-f9fc-4041-eb1a-a4bee5de9a58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           MAE       RMSE      MAPE     sMAPE        R2  \\\n",
              "Model       Dataset                                                       \n",
              "LSTM        train    27.989830  38.989189  0.060203  0.060190  0.990512   \n",
              "            test     45.204918  59.094742  0.094563  0.094631  0.904518   \n",
              "GRU         train    26.490988  38.961105  0.056988  0.056981  0.990523   \n",
              "            test     66.061127  83.549095  0.138142  0.138289  0.809143   \n",
              "Transformer train    25.080482  38.251038  0.053899  0.053898  0.990865   \n",
              "            test     41.446842  51.369881  0.086780  0.086812  0.927849   \n",
              "\n",
              "                         MASE    Params   FLOPs  \n",
              "Model       Dataset                              \n",
              "LSTM        train    1.227784   117.00   9.13 K  \n",
              "            test     1.982931   117.00   9.13 K  \n",
              "GRU         train    1.161850    89.00   6.73 K  \n",
              "            test     2.897329    89.00   6.73 K  \n",
              "Transformer train    1.099853  100.29 K  5.51 M  \n",
              "            test     1.817566  100.29 K  5.51 M  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cdcd1f68-9ffe-4d87-b331-3cc4e9503e9a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>sMAPE</th>\n",
              "      <th>R2</th>\n",
              "      <th>MASE</th>\n",
              "      <th>Params</th>\n",
              "      <th>FLOPs</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">LSTM</th>\n",
              "      <th>train</th>\n",
              "      <td>27.989830</td>\n",
              "      <td>38.989189</td>\n",
              "      <td>0.060203</td>\n",
              "      <td>0.060190</td>\n",
              "      <td>0.990512</td>\n",
              "      <td>1.227784</td>\n",
              "      <td>117.00</td>\n",
              "      <td>9.13 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>45.204918</td>\n",
              "      <td>59.094742</td>\n",
              "      <td>0.094563</td>\n",
              "      <td>0.094631</td>\n",
              "      <td>0.904518</td>\n",
              "      <td>1.982931</td>\n",
              "      <td>117.00</td>\n",
              "      <td>9.13 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">GRU</th>\n",
              "      <th>train</th>\n",
              "      <td>26.490988</td>\n",
              "      <td>38.961105</td>\n",
              "      <td>0.056988</td>\n",
              "      <td>0.056981</td>\n",
              "      <td>0.990523</td>\n",
              "      <td>1.161850</td>\n",
              "      <td>89.00</td>\n",
              "      <td>6.73 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>66.061127</td>\n",
              "      <td>83.549095</td>\n",
              "      <td>0.138142</td>\n",
              "      <td>0.138289</td>\n",
              "      <td>0.809143</td>\n",
              "      <td>2.897329</td>\n",
              "      <td>89.00</td>\n",
              "      <td>6.73 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Transformer</th>\n",
              "      <th>train</th>\n",
              "      <td>25.080482</td>\n",
              "      <td>38.251038</td>\n",
              "      <td>0.053899</td>\n",
              "      <td>0.053898</td>\n",
              "      <td>0.990865</td>\n",
              "      <td>1.099853</td>\n",
              "      <td>100.29 K</td>\n",
              "      <td>5.51 M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>41.446842</td>\n",
              "      <td>51.369881</td>\n",
              "      <td>0.086780</td>\n",
              "      <td>0.086812</td>\n",
              "      <td>0.927849</td>\n",
              "      <td>1.817566</td>\n",
              "      <td>100.29 K</td>\n",
              "      <td>5.51 M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdcd1f68-9ffe-4d87-b331-3cc4e9503e9a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cdcd1f68-9ffe-4d87-b331-3cc4e9503e9a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cdcd1f68-9ffe-4d87-b331-3cc4e9503e9a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5c1c6f13-c46b-4297-bcc4-c10f012726b3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c1c6f13-c46b-4297-bcc4-c10f012726b3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5c1c6f13-c46b-4297-bcc4-c10f012726b3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"comparison\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          27.989830017089844,\n          45.204917907714844,\n          41.446842193603516\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          38.98918914794922,\n          59.09474182128906,\n          51.36988067626953\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.06020345538854599,\n          0.09456338733434677,\n          0.08678030222654343\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sMAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.06019008159637451,\n          0.09463087469339371,\n          0.08681194484233856\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.9905117154121399,\n          0.9045180082321167,\n          0.9278492331504822\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MASE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1.2277843952178955,\n          1.9829306602478027,\n          1.8175660371780396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Params\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"117.00 \",\n          \"89.00 \",\n          \"100.29 K\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FLOPs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"9.13 K\",\n          \"6.73 K\",\n          \"5.51 M\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "results = {\n",
        "    \"LSTM\": lstm_metrics,\n",
        "    \"GRU\": gru_metrics,\n",
        "    \"Transformer\": transformer_metrics\n",
        "}\n",
        "\n",
        "\n",
        "comparison = pd.concat({model: pd.DataFrame(metrics).T for model, metrics in results.items()})\n",
        "\n",
        "comparison = comparison.reset_index().rename(columns={'level_0':'Model', 'level_1':'Dataset'})\n",
        "\n",
        "comparison = comparison.merge(df_info, on=\"Model\")\n",
        "\n",
        "comparison.set_index(['Model',\t'Dataset'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = BaselineModel(sample_mean=6)\n",
        "result = predict_train_test(baseline_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "\n",
        "plot_predictions_plotly(result, horizon_step=0, title=\"Gold Price Forecast\")\n",
        "\n",
        "\n",
        "transformer_metrics = evaluate_forecast(result)\n",
        "transformer_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "WBOdtFHOIK1Y",
        "outputId": "5e50d828-d498-41a8-d990-2e10ccf2b898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"6a30c5cc-66d1-458a-a615-00a2c1951d57\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6a30c5cc-66d1-458a-a615-00a2c1951d57\")) {                    Plotly.newPlot(                        \"6a30c5cc-66d1-458a-a615-00a2c1951d57\",                        [{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Train Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[45984.46,45962.145,46026.055,45971.46,45889.594,45870.5,45901.965,45973.76,45983.777,45970.656,45956.285,45929.1,45936.215,45933.805,45947.234,45934.84,45937.71,45931.523,45919.64,45907.28,45919.33,45931.14,45951.434,46006.883,45957.688,45828.02,45881.95,45893.62,45963.086,46004.266,46003.23,45965.203,45905.594,45952.133,45979.535,45978.918,45979.547,45979.793,45980.266,45979.77,45978.082,45981.293,45978.684,45971.07,45986.367,45988.098,45929.492,45798.316,45796.145,45871.625,45843.586,45816.574,45860.24,45879.29,45881.152,45878.277,45992.938,46077.16,46067.63,46102.625,46101.445,46100.766,46099.516,46099.92,46101.023,46099.703,46100.293,46100.617,46154.883,46193.246,46214.453,46241.637,46335.73,46375.734,46324.023,46359.93,46413.68,46437.793,46308.715,46303.098,46307.63,46248.0,46279.824,46298.992,46331.113,46329.27,46329.95,46328.965,46330.74,46330.188,46330.668,46330.37,46330.082,46327.344,46425.145,46461.01,46314.37,46262.668,46286.703,46277.824,46274.383,46212.707,46283.535,46352.766,46373.543,46362.727,46438.3,46367.4,46359.977,46360.05,46359.6,46359.03,46359.97,46359.668,46359.258,46360.336,46359.91,46235.863,46178.08,46148.49,46116.375,45974.58,45895.27,45873.902,45866.35,45833.957,45938.023,45942.13,45992.86,46058.816,46110.203,46084.363,46050.28,46050.293,46049.12,46049.184,46050.016,46050.406,46050.29,46050.01,45944.016,45984.465,46023.33,46130.117,46215.832,46278.168,46221.895,46164.453,46186.816,46164.336,46241.727,46229.258,46224.992,46226.49,46229.094,46202.895,46197.543,46213.0,46216.57,46220.91,46226.785,46224.523,46219.605,46255.758,46271.973,46353.055,46471.375,46423.14,46425.266,46434.43,46446.51,46444.3,46378.805,46357.555,46385.816,46427.188,46470.746,46458.25,46458.598,46459.38,46467.016,46463.406,46462.03,46462.25,46465.832,46461.434,46459.777,46459.58,46464.49,46464.797,46472.215,46490.258,46481.84,46434.617,46440.316,46416.53,46372.406,46323.652,46294.383,46284.47,46232.44,46233.176,46237.297,46290.266,46285.414,46281.29,46276.91,46259.797,46241.406,46235.492,46230.766,46223.367,46229.31,46251.566,46353.824,46285.797,46254.84,46343.36,46390.043,46369.57,46380.42,46308.496,46314.86,46335.78,46350.516,46331.734,46329.855,46339.016,46340.387,46341.24,46344.145,46340.387,46339.758,46340.2,46339.586,46341.715,46342.465,46342.863,46507.418,46605.957,46671.47,46690.613,46664.41,46720.348,46746.473,46778.035,46769.28,46825.406,46868.09,46837.8,46805.79,46832.22,46839.84,46840.977,46833.293,46829.848,46835.9,46830.59,46843.164,46847.258,46849.863,46838.64,46828.508,46812.047,46726.125,46717.75,46685.31,46679.625,46628.82,46647.65,46640.723,46644.605,46619.703,46618.89,46615.34,46619.2,46630.22,46625.285,46626.06,46628.81,46631.83,46629.875,46636.902,46625.016,46623.223,46615.016,46643.31,46651.973,46636.254,46590.535,46577.38,46506.875,46521.67,46493.035,46495.55,46527.45,46516.227,46509.914,46511.484,46508.652,46510.176,46509.13,46512.207,46518.684,46517.812,46515.367,46518.508,46512.863,46509.168,46501.53,46497.33,46415.266,46376.914,46388.76,46407.34,46401.047,46377.01,46382.965,46387.504,46401.277,46422.043,46415.58,46416.484,46400.168,46399.83,46400.055,46400.535,46404.117,46400.18,46399.918,46400.21,46397.19,46401.145,46413.32,46464.82,46504.742,46536.72,46507.793,46521.094,46533.223,46527.938,46492.19,46470.35,46464.715,46458.42,46464.18,46459.22,46459.812,46463.223,46463.207,46459.29,46459.84,46463.176,46462.83,46459.71,46460.83,46462.54,46461.09,46460.895,46463.066,46465.688,46458.215,46451.594,46430.066,46408.973,46369.11,46379.402,46308.484,46304.234,46298.414,46292.832,46368.63,46402.684,46390.152,46380.008,46380.562,46378.89,46368.0,46366.766,46360.457,46333.637,46328.66,46271.566,46409.016,46291.03,46247.52,46227.105,46175.625,46148.375,46173.152,46111.406,46095.29,46130.434,46114.14,46105.58,46134.4,46189.0,46182.707,46166.02,46158.543,46160.8,46159.105,46158.742,46152.156,46142.348,46200.676,46249.504,46196.54,46147.926,46158.66,46163.957,46196.414,46180.703,46168.035,46180.6,46208.883,46210.09,46225.516,46222.81,46217.223,46210.35,46209.49,46209.45,46209.594,46209.766,46210.88,46210.0,46209.07,46203.36,46199.207,46206.99,46163.035,46264.562,46396.438,46398.152,46418.62,46421.02,46379.42,46363.836,46375.996,46354.645,46327.348,46307.918,46355.047,46354.27,46356.82,46347.086,46347.73,46344.543,46342.95,46339.207,46340.645,46322.2,46329.695,46390.883,46374.254,46323.875,46461.965,46494.88,46504.51,46506.215,46448.504,46506.453,46587.88,46588.977,46558.7,46559.56,46574.074,46580.035,46575.85,46572.516,46572.59,46571.324,46568.81,46570.516,46577.18,46621.355,46660.92,46669.664,46699.78,46710.805,46720.793,46718.246,46651.926,46614.344,46622.457,46639.145,46674.914,46634.1,46610.426,46616.074,46638.49,46645.594,46635.797,46639.625,46639.773,46640.4,46640.06,46632.688,46630.13,46632.043,46622.91,46617.99,46530.48,46542.703,46562.156,46592.4,46600.965,46622.258,46656.195,46675.0,46690.953,46705.3,46682.516,46678.918,46670.387,46667.914,46670.793,46681.164,46682.734,46685.082,46698.254,46691.184,46693.637,46697.145,46703.49,46734.18,46782.457,46791.887,46738.562,46768.023,46784.42,46830.86,46828.895,46937.78,47000.91,46985.273,47111.68,47146.766,47183.22,47201.438,47170.066,47162.18,47147.965,47139.14,47139.85,47149.133,47141.742,47156.156,47174.41,47151.59,47142.992,47273.48,47301.04,47345.44,47306.375,47249.242,47251.316,47198.875,47150.938,47168.234,47142.395,47129.98,47133.523,47131.5,47129.63,47130.195,47130.023,47129.6,47128.75,47129.977,47121.41,47114.773,47116.125,47136.402,47027.61,46990.2,46938.414,46955.836,46984.805,46965.758,46996.83,47002.207,47018.71,47009.637,47008.13,47006.375,47009.12,47010.04,47021.168,47029.5,47029.633,47030.684,47034.633,47036.812,47038.008,47038.832,47023.727,46931.684,46986.074,47085.945,47108.133,47101.207,47081.836,47081.69,47087.973,47102.88,47183.086,47176.875,47179.06,47172.45,47187.773,47189.99,47189.84,47185.273,47188.062,47183.824,47182.39,47180.266,47181.457,47182.67,47187.05,47216.375,47268.715,47248.883,47317.004,47340.508,47299.58,47438.715,47539.438,47472.746,47434.074,47395.8,47411.598,47402.727,47415.97,47419.562,47420.223,47420.496,47419.062,47420.055,47412.03,47410.266,47409.293,47401.203,47440.27,47494.02,47470.465,47396.94,47422.95,47415.24,47410.336,47399.64,47429.91,47414.207,47411.113,47417.203,47453.48],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Train Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[45921.89,45939.55,45955.707,45982.945,45990.152,45979.82,45949.38,45936.49,45938.617,45931.926,45930.723,45942.6,45952.32,45958.633,45952.086,45945.945,45939.793,45937.023,45937.594,45934.555,45930.527,45925.46,45924.61,45926.9,45938.99,45945.39,45931.668,45925.93,45919.65,45921.83,45921.184,45929.42,45952.176,45956.023,45965.145,45968.31,45963.98,45959.832,45962.35,45975.453,45979.363,45979.816,45979.95,45979.816,45978.273,45979.2,45981.023,45972.65,45941.645,45911.31,45894.555,45870.652,45842.363,45830.188,45842.977,45858.676,45859.49,45884.07,45927.613,45962.785,45999.797,46036.566,46073.51,46090.79,46094.504,46100.99,46100.19,46099.477,46099.832,46109.516,46125.19,46143.953,46166.977,46206.465,46252.777,46280.363,46308.047,46341.266,46373.707,46369.566,46357.67,46354.96,46336.926,46314.11,46291.402,46294.81,46299.375,46302.91,46315.918,46324.645,46330.33,46330.227,46329.805,46329.51,46329.574,46345.445,46366.77,46364.58,46353.2,46346.332,46338.2,46312.71,46271.754,46266.33,46281.49,46295.723,46310.02,46337.258,46363.008,46376.027,46377.016,46374.78,46374.168,46361.33,46359.81,46359.836,46360.027,46359.375,46339.19,46308.953,46273.77,46233.223,46169.0,46092.08,46031.74,45980.156,45926.742,45897.215,45892.0,45907.74,45938.566,45979.29,46021.504,46039.824,46057.438,46066.766,46065.715,46055.277,46049.56,46049.703,46050.336,46032.14,46021.74,46016.863,46030.707,46058.12,46096.086,46142.484,46172.273,46199.35,46205.04,46209.27,46201.473,46200.91,46212.234,46219.086,46225.586,46218.254,46215.688,46214.36,46213.094,46212.668,46215.996,46220.16,46227.035,46236.37,46258.125,46298.965,46332.56,46366.27,46396.324,46425.336,46440.9,46425.473,46414.45,46407.55,46406.723,46410.387,46413.117,46426.098,46443.67,46456.953,46463.113,46461.383,46462.13,46463.56,46463.78,46462.613,46461.953,46462.562,46462.746,46463.6,46468.645,46471.965,46467.645,46463.95,46455.79,46438.96,46411.57,46380.01,46355.14,46320.234,46290.066,46267.77,46261.543,46260.645,46259.582,46267.46,46271.33,46272.438,46263.152,46253.562,46244.766,46236.28,46235.06,46253.938,46262.637,46266.703,46286.31,46313.125,46332.996,46337.266,46340.797,46350.945,46349.99,46343.695,46336.656,46328.754,46333.688,46337.836,46338.637,46337.426,46339.227,46340.87,46341.008,46340.95,46340.938,46340.555,46341.004,46368.926,46413.4,46468.605,46526.863,46580.555,46643.47,46683.29,46712.04,46728.305,46750.867,46784.66,46804.19,46814.043,46823.047,46834.824,46837.37,46831.555,46830.273,46835.348,46835.066,46835.6,46836.684,46839.46,46840.938,46839.727,46836.562,46817.027,46795.47,46767.97,46741.504,46708.17,46681.03,46666.66,46654.55,46643.83,46633.67,46631.363,46626.73,46624.9,46621.586,46622.55,46624.344,46626.92,46628.83,46629.832,46629.688,46629.12,46626.883,46628.777,46632.59,46632.363,46626.855,46619.15,46601.18,46581.055,46554.125,46530.945,46520.645,46510.246,46510.543,46509.367,46511.953,46514.344,46511.45,46510.477,46511.89,46512.85,46513.75,46515.305,46515.57,46515.55,46512.43,46508.863,46492.62,46468.9,46448.45,46431.547,46415.332,46395.04,46389.207,46391.19,46393.312,46395.57,46397.75,46404.656,46407.207,46409.355,46409.336,46405.508,46403.902,46401.438,46400.617,46400.906,46400.438,46400.66,46402.08,46412.42,46430.125,46452.816,46471.156,46491.598,46511.332,46521.93,46519.934,46508.516,46501.684,46491.28,46479.625,46468.36,46462.86,46461.277,46461.004,46461.363,46460.676,46461.773,46462.117,46461.637,46461.344,46461.85,46461.867,46461.418,46461.59,46462.49,46462.19,46460.203,46454.773,46446.31,46430.86,46416.223,46391.387,46366.73,46344.836,46325.55,46325.367,46329.246,46342.992,46355.402,46369.227,46383.457,46383.402,46377.75,46372.844,46364.855,46355.9,46337.94,46345.027,46332.426,46312.86,46295.094,46270.203,46249.316,46210.22,46179.93,46155.137,46139.152,46128.54,46121.785,46115.082,46128.117,46142.234,46148.55,46156.195,46165.38,46169.234,46165.586,46159.73,46155.965,46162.996,46177.688,46183.973,46181.74,46183.207,46186.125,46185.16,46175.086,46169.875,46175.363,46183.688,46191.46,46195.773,46202.676,46210.953,46215.82,46216.15,46215.754,46213.004,46211.12,46209.723,46209.855,46209.52,46208.387,46206.906,46207.164,46198.902,46207.94,46238.95,46271.73,46307.945,46343.72,46379.395,46396.684,46392.914,46385.785,46370.734,46351.516,46347.566,46345.86,46342.656,46341.2,46344.508,46350.86,46348.848,46346.285,46343.965,46339.4,46336.51,46344.46,46349.613,46346.625,46366.95,46396.156,46425.098,46444.348,46456.93,46487.277,46508.258,46524.22,46533.22,46541.723,46562.785,46574.707,46572.773,46569.973,46572.426,46574.336,46573.43,46571.62,46572.01,46580.082,46594.73,46611.188,46633.023,46656.434,46680.38,46696.65,46695.156,46685.99,46673.05,46661.125,46653.59,46639.484,46632.723,46632.94,46635.527,46636.582,46630.137,46631.094,46635.92,46640.207,46640.164,46638.23,46637.117,46636.008,46633.1,46629.344,46611.145,46595.91,46584.734,46578.023,46574.3,46574.785,46596.184,46618.254,46639.7,46658.5,46672.09,46681.387,46683.824,46682.54,46679.137,46675.09,46675.28,46676.195,46680.89,46684.86,46688.668,46691.18,46694.816,46702.86,46717.008,46733.83,46741.324,46753.074,46766.48,46782.633,46790.363,46814.72,46858.48,46894.645,46949.105,47001.84,47060.92,47105.004,47133.08,47162.656,47168.82,47167.582,47160.117,47151.492,47146.85,47145.75,47149.95,47152.16,47152.617,47173.61,47199.844,47231.555,47253.574,47269.92,47287.887,47275.414,47250.28,47220.71,47193.305,47173.47,47153.95,47142.74,47139.125,47132.824,47130.832,47130.82,47130.055,47129.85,47128.406,47125.965,47123.44,47124.67,47107.652,47084.5,47053.88,47027.37,47005.56,46977.08,46971.973,46973.99,46987.273,46996.29,47000.23,47006.965,47009.023,47010.36,47010.734,47014.08,47017.62,47021.684,47026.0,47030.44,47033.285,47034.73,47033.73,47017.207,47009.14,47017.383,47028.953,47039.363,47049.016,47074.12,47091.023,47093.973,47106.535,47119.152,47135.23,47150.137,47167.004,47181.35,47182.6,47183.906,47185.594,47187.555,47186.66,47185.008,47183.508,47183.09,47183.03,47188.285,47202.684,47214.176,47236.773,47262.96,47281.586,47319.023,47364.24,47401.094,47420.758,47429.914,47448.78,47442.508,47422.156,47413.168,47411.22,47415.195,47416.223,47419.566,47418.613,47417.305,47415.176,47412.48,47415.75,47427.855,47437.586,47435.45,47437.723,47439.656,47435.133,47419.047,47412.43,47415.316,47413.594,47413.492],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\"},\"mode\":\"lines\",\"name\":\"Test Actual\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47354.723,47386.867,47480.477,47473.496,47487.812,47481.54,47479.62,47470.945,47472.492,47478.9,47458.195,47448.29,47460.25,47502.414,47531.24,47498.434,47501.688,47516.1,47485.664,47526.723,47534.07,47558.37,47580.25,47563.18,47553.977,47579.523,47578.06,47567.715,47581.76,47578.43,47569.055,47570.44,47555.64,47533.176,47540.406,47546.89,47586.316,47688.715,47680.273,47713.484,47752.676,47771.42,47776.61,47787.48,47793.324,47894.84,47900.336,47876.62,47871.887,47895.88,47923.523,47921.543,47915.22,47883.438,47889.65,47888.965,47884.215,47877.965,47851.34,47849.41,47919.895,48024.168,48001.297,47956.777,47951.668,47942.934,47936.01,47891.285,47869.402,47898.98,47915.797,47921.13,47899.66,47902.406],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Test Pred\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47420.9,47413.48,47406.254,47417.316,47427.71,47439.484,47444.152,47464.97,47478.984,47477.652,47478.55,47473.613,47468.07,47464.848,47470.09,47479.88,47483.137,47490.383,47501.688,47505.926,47509.977,47510.45,47520.438,47533.53,47541.375,47552.76,47561.56,47568.89,47570.453,47570.703,47573.246,47575.758,47574.242,47570.508,47564.75,47557.86,47552.6,47555.48,47575.19,47595.965,47626.01,47661.39,47698.816,47730.53,47746.992,47765.832,47796.06,47820.668,47838.203,47854.08,47872.15,47893.844,47898.3,47900.777,47901.914,47904.875,47903.723,47897.17,47889.91,47879.266,47873.59,47878.633,47901.164,47920.68,47933.812,47950.535,47966.12,47968.805,47946.66,47924.68,47915.047,47909.066,47905.438,47899.375],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"shapes\":[{\"line\":{\"color\":\"black\",\"dash\":\"dot\",\"width\":2},\"type\":\"line\",\"x0\":657.5,\"x1\":657.5,\"xref\":\"x\",\"y0\":0,\"y1\":1,\"yref\":\"y domain\"}],\"legend\":{\"x\":0,\"y\":1},\"title\":{\"text\":\"Gold Price Forecast\"},\"xaxis\":{\"title\":{\"text\":\"Sample Index (time ordered)\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6a30c5cc-66d1-458a-a615-00a2c1951d57');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'MAE': np.float32(42.86037),\n",
              "  'RMSE': np.float32(65.072395),\n",
              "  'MAPE': np.float32(0.092183016),\n",
              "  'sMAPE': np.float32(0.09220744),\n",
              "  'R2': np.float32(0.9735641),\n",
              "  'MASE': np.float32(1.8828214)},\n",
              " 'test': {'MAE': np.float32(40.150284),\n",
              "  'RMSE': np.float32(53.662918),\n",
              "  'MAPE': np.float32(0.08411537),\n",
              "  'sMAPE': np.float32(0.08416282),\n",
              "  'R2': np.float32(0.9212642),\n",
              "  'MASE': np.float32(1.7637695)}}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "6DKAPUcsyJD7",
        "outputId": "8af03cb4-a7f1-41e9-bb37-640f73fda275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"9f4bbd59-7d7c-4900-a5c6-f4ceb86c9b01\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9f4bbd59-7d7c-4900-a5c6-f4ceb86c9b01\")) {                    Plotly.newPlot(                        \"9f4bbd59-7d7c-4900-a5c6-f4ceb86c9b01\",                        [{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Train Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[45983.527,45961.555,46025.223,45971.02,45890.977,45872.117,45900.883,45972.24,45983.926,45972.055,45954.21,45930.137,45936.19,45936.098,45947.336,45933.2,45938.81,45933.42,45917.195,45909.04,45922.145,45929.734,45951.113,46005.812,45958.87,45825.51,45880.37,45893.082,45963.918,46006.133,46003.12,45965.25,45903.63,45950.4,45980.082,45979.76,45978.52,45979.96,45979.11,45979.55,45978.812,45980.88,45981.35,45971.54,45987.242,45988.438,45929.875,45797.484,45791.9,45869.83,45842.38,45815.312,45859.56,45881.535,45883.008,45877.21,45994.004,46075.473,46068.977,46101.66,46101.2,46099.2,46097.953,46098.105,46099.348,46100.965,46100.258,46100.113,46154.914,46194.414,46213.938,46240.566,46336.215,46374.977,46323.93,46359.082,46412.945,46437.45,46309.35,46303.016,46308.812,46246.473,46279.49,46299.44,46329.973,46329.97,46330.004,46329.363,46331.727,46329.816,46329.746,46330.164,46329.637,46326.91,46426.633,46460.31,46312.504,46261.035,46288.348,46276.75,46274.07,46212.734,46283.418,46352.918,46372.535,46363.164,46437.754,46367.176,46360.855,46359.81,46358.793,46359.23,46359.58,46360.105,46360.3,46360.445,46359.332,46235.06,46177.87,46150.543,46116.902,45976.027,45895.51,45874.887,45866.26,45832.13,45937.94,45942.586,45992.53,46058.02,46111.62,46084.6,46050.375,46049.766,46050.35,46048.86,46048.56,46049.5,46049.703,46050.09,45943.51,45983.2,46023.72,46129.355,46214.52,46277.785,46222.785,46164.14,46187.594,46162.82,46240.977,46228.37,46223.81,46226.477,46228.156,46202.44,46196.875,46212.28,46217.055,46219.29,46225.77,46225.17,46219.816,46255.273,46270.61,46353.19,46470.395,46422.957,46425.195,46433.89,46446.15,46444.742,46377.863,46356.582,46385.05,46427.97,46470.43,46458.49,46457.426,46459.89,46466.68,46463.93,46461.633,46462.21,46465.625,46460.45,46460.82,46460.184,46463.67,46464.17,46472.65,46489.87,46481.977,46434.965,46440.273,46416.613,46372.18,46323.0,46294.117,46285.414,46231.914,46232.934,46237.312,46290.37,46285.508,46280.91,46275.336,46259.652,46241.02,46234.37,46229.6,46224.188,46230.15,46251.035,46353.234,46286.727,46254.047,46342.992,46390.88,46368.91,46380.383,46306.51,46314.703,46337.066,46350.145,46330.02,46329.992,46339.184,46340.594,46341.33,46344.547,46339.785,46339.305,46339.81,46339.473,46342.105,46342.035,46342.855,46507.52,46606.39,46671.38,46690.715,46664.715,46720.19,46746.8,46778.05,46769.008,46825.508,46868.156,46837.863,46805.582,46832.035,46839.805,46840.887,46833.125,46829.777,46835.86,46830.69,46843.09,46847.316,46849.965,46838.605,46828.51,46812.055,46725.945,46718.082,46684.73,46679.92,46629.273,46648.227,46640.527,46644.91,46620.305,46618.83,46615.414,46619.24,46630.43,46625.49,46626.164,46628.246,46631.98,46630.18,46636.406,46624.88,46623.18,46614.883,46643.47,46651.953,46636.51,46590.703,46577.492,46506.97,46521.555,46492.96,46496.05,46527.957,46517.082,46509.94,46511.55,46508.6,46509.875,46510.055,46512.02,46519.176,46517.008,46514.887,46518.5,46512.38,46509.184,46501.133,46497.7,46415.508,46377.848,46389.266,46407.715,46401.28,46376.508,46382.926,46388.195,46400.89,46421.742,46416.016,46416.055,46400.74,46400.145,46399.883,46400.633,46404.664,46400.695,46399.06,46399.254,46397.06,46400.977,46413.64,46465.043,46504.19,46536.87,46507.234,46520.76,46533.656,46528.93,46492.227,46470.355,46464.33,46457.812,46463.574,46458.883,46459.902,46461.8,46463.56,46459.48,46459.93,46463.785,46463.457,46458.785,46460.652,46462.996,46462.21,46461.71,46462.74,46465.633,46457.527,46451.633,46430.164,46409.543,46369.453,46379.15,46308.465,46304.164,46298.1,46292.652,46368.31,46402.44,46390.496,46379.35,46379.863,46378.926,46368.63,46366.59,46359.918,46333.316,46328.883,46271.656,46408.92,46289.45,46245.633,46225.938,46175.53,46147.254,46172.504,46113.387,46095.836,46130.023,46113.496,46106.645,46133.625,46188.062,46183.168,46165.332,46160.344,46160.6,46159.832,46160.41,46154.07,46144.457,46200.105,46249.367,46197.05,46147.465,46159.6,46164.023,46196.266,46182.445,46169.344,46180.56,46209.473,46210.145,46226.367,46222.74,46217.914,46209.523,46209.742,46208.86,46209.777,46208.234,46210.145,46209.383,46210.793,46204.92,46199.164,46207.715,46164.613,46264.418,46395.93,46398.797,46418.234,46421.38,46379.613,46363.64,46375.633,46354.78,46327.13,46308.367,46356.234,46353.793,46356.03,46347.656,46347.3,46345.17,46342.508,46339.594,46340.957,46321.95,46329.605,46390.56,46372.88,46324.82,46462.15,46494.81,46504.53,46506.066,46449.26,46506.426,46588.125,46589.543,46558.99,46558.996,46573.33,46579.516,46575.605,46572.37,46572.26,46571.375,46568.79,46570.26,46577.04,46620.652,46661.023,46669.63,46699.8,46710.49,46720.58,46718.21,46652.34,46614.438,46622.008,46639.184,46675.027,46634.293,46610.395,46615.848,46638.484,46645.56,46635.812,46639.812,46640.03,46640.152,46640.07,46632.56,46629.875,46631.957,46622.89,46617.68,46530.38,46542.836,46562.14,46592.91,46600.01,46622.402,46655.855,46675.46,46690.754,46705.293,46682.332,46678.65,46670.477,46667.707,46670.973,46681.83,46682.66,46684.906,46698.156,46691.152,46693.34,46697.77,46703.562,46734.02,46782.48,46791.906,46738.324,46768.24,46784.543,46830.723,46828.85,46937.78,47000.844,46985.312,47111.06,47146.723,47183.875,47201.613,47170.137,47162.637,47147.934,47139.293,47140.0,47149.43,47141.566,47156.082,47173.734,47152.25,47143.254,47273.55,47301.086,47345.29,47306.56,47249.535,47251.25,47198.08,47150.906,47168.29,47142.36,47130.195,47134.086,47131.355,47129.94,47130.332,47129.957,47129.824,47128.906,47130.055,47121.406,47115.145,47115.57,47136.273,47027.566,46990.277,46938.453,46955.926,46984.668,46965.902,46996.9,47002.242,47018.547,47009.7,47008.188,47006.297,47009.277,47009.94,47021.38,47029.5,47029.793,47030.797,47034.7,47036.74,47037.9,47038.844,47023.793,46931.684,46986.11,47085.89,47107.668,47101.258,47082.062,47081.883,47087.527,47103.41,47183.16,47177.023,47178.86,47172.55,47187.98,47190.223,47189.707,47185.188,47187.867,47184.71,47182.406,47180.83,47181.066,47183.082,47187.137,47216.53,47268.953,47249.227,47317.535,47339.977,47299.047,47439.035,47539.95,47472.598,47435.61,47396.6,47411.79,47402.625,47415.37,47419.45,47421.055,47419.953,47420.13,47419.832,47411.816,47409.637,47409.113,47400.977,47440.176,47493.645,47469.7,47397.152,47422.84,47415.184,47410.938,47399.24,47429.695,47414.44,47411.613,47417.625,47453.227],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Train Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[46042.887,45986.188,45966.453,46023.14,45973.215,45901.08,45886.504,45911.617,45974.625,45983.676,45972.816,45958.832,45938.023,45941.516,45941.375,45951.26,45940.812,45943.74,45938.824,45926.71,45917.56,45928.94,45938.445,45956.65,46005.004,45962.582,45850.23,45895.723,45905.055,45967.973,46003.26,46002.668,45968.316,45915.957,45954.797,45981.33,45980.984,45981.96,45980.406,45981.53,45981.004,45979.555,45980.39,45980.664,45972.582,45987.504,45988.934,45935.78,45827.293,45825.805,45886.562,45864.34,45843.28,45878.17,45895.324,45898.254,45894.207,45994.004,46074.746,46065.414,46099.355,46094.89,46093.3,46093.76,46091.824,46093.652,46093.383,46094.11,46094.72,46148.703,46190.58,46212.29,46240.4,46335.3,46376.69,46323.992,46359.008,46416.55,46443.168,46306.59,46298.934,46304.71,46243.27,46274.52,46295.24,46327.973,46328.066,46327.055,46325.86,46327.5,46327.273,46326.836,46326.965,46326.684,46324.543,46430.09,46467.273,46311.375,46256.965,46282.695,46271.36,46268.984,46207.19,46278.992,46351.844,46373.32,46361.562,46444.164,46366.863,46358.36,46357.383,46356.707,46357.953,46358.684,46358.734,46358.465,46357.8,46357.41,46231.816,46168.312,46139.316,46105.21,45972.016,45901.824,45884.863,45878.84,45853.05,45942.15,45945.66,45993.29,46055.723,46105.816,46079.023,46046.2,46046.734,46045.523,46044.797,46046.1,46045.258,46044.523,46044.953,45946.26,45982.816,46020.363,46125.16,46216.324,46278.117,46221.062,46160.008,46182.47,46157.11,46238.926,46226.67,46221.668,46224.082,46226.95,46198.277,46192.85,46209.168,46212.363,46216.105,46223.676,46223.21,46215.777,46252.02,46268.695,46352.484,46477.125,46427.836,46428.895,46438.902,46451.305,46449.695,46377.695,46354.023,46384.105,46430.46,46475.883,46464.527,46464.137,46465.637,46471.395,46469.7,46467.414,46467.387,46470.86,46465.098,46464.938,46465.484,46469.074,46469.88,46476.086,46494.402,46486.43,46437.453,46442.05,46417.844,46370.34,46318.44,46288.496,46279.81,46225.297,46228.13,46232.703,46285.395,46282.715,46277.47,46272.99,46256.844,46237.85,46231.746,46227.16,46221.46,46226.066,46248.957,46352.344,46283.215,46252.055,46341.375,46391.508,46369.63,46380.086,46304.566,46312.094,46333.61,46349.4,46327.74,46326.15,46335.9,46337.895,46339.383,46341.617,46338.133,46337.793,46337.11,46338.023,46339.79,46340.203,46340.59,46513.035,46612.53,46677.16,46695.81,46670.125,46724.473,46749.785,46781.445,46771.19,46833.074,46877.375,46845.09,46810.992,46838.918,46846.77,46847.902,46839.97,46836.637,46842.72,46837.617,46850.207,46854.336,46857.027,46845.56,46835.043,46817.734,46725.953,46718.13,46687.145,46682.277,46630.945,46649.926,46643.15,46646.777,46622.7,46621.8,46618.41,46622.688,46633.66,46628.16,46629.465,46632.086,46635.52,46633.535,46640.227,46628.14,46626.33,46618.684,46646.39,46655.21,46639.55,46593.836,46581.19,46509.797,46524.695,46496.29,46498.934,46531.492,46521.707,46514.07,46515.465,46513.17,46514.074,46513.19,46516.38,46522.812,46521.19,46519.492,46522.547,46516.676,46513.16,46506.676,46501.48,46416.152,46375.31,46387.203,46407.72,46400.723,46373.965,46381.484,46387.156,46401.258,46424.348,46417.855,46418.797,46400.844,46400.85,46399.992,46401.52,46405.484,46400.016,46400.41,46400.605,46397.094,46402.188,46415.57,46470.457,46510.34,46542.22,46511.984,46524.96,46537.56,46533.223,46496.285,46474.363,46468.883,46463.375,46468.094,46463.566,46464.805,46467.75,46468.008,46464.316,46464.81,46468.895,46468.625,46464.492,46466.35,46467.492,46466.453,46466.594,46468.434,46470.54,46463.133,46455.9,46432.15,46409.824,46366.574,46377.688,46303.254,46297.375,46294.336,46288.742,46367.418,46404.152,46392.152,46379.53,46378.977,46377.996,46365.89,46365.688,46359.0,46331.117,46324.633,46267.4,46410.242,46286.79,46242.184,46220.438,46166.836,46136.285,46163.906,46102.402,46087.02,46121.992,46105.51,46097.758,46126.188,46184.99,46178.062,46161.652,46155.86,46154.734,46152.42,46152.582,46146.203,46137.15,46196.027,46248.277,46192.47,46141.426,46152.63,46155.527,46191.645,46177.746,46162.465,46176.434,46206.0,46207.07,46222.56,46218.555,46213.92,46205.195,46206.598,46205.48,46205.49,46205.332,46206.34,46206.215,46204.56,46199.207,46193.938,46202.504,46156.48,46261.7,46399.863,46402.297,46422.47,46425.32,46380.996,46362.6,46374.418,46352.73,46323.848,46302.867,46352.082,46351.164,46354.742,46345.043,46345.914,46342.293,46340.31,46336.453,46337.453,46317.22,46326.746,46391.363,46373.86,46321.195,46468.25,46500.477,46509.97,46511.332,46455.062,46511.23,46593.883,46595.234,46563.637,46563.51,46578.5,46584.156,46580.164,46577.457,46577.484,46576.336,46573.406,46574.477,46581.906,46625.29,46665.367,46673.918,46704.395,46714.293,46723.78,46721.22,46655.137,46617.945,46625.16,46641.816,46678.934,46636.895,46613.39,46618.95,46641.113,46648.473,46638.797,46642.816,46643.273,46643.086,46643.63,46636.375,46633.184,46635.52,46626.688,46621.215,46533.19,46546.293,46565.64,46596.793,46604.617,46626.188,46660.234,46679.777,46695.246,46709.69,46686.516,46682.55,46674.074,46671.363,46674.215,46685.47,46686.688,46689.004,46702.36,46695.12,46697.31,46701.688,46707.426,46735.582,46786.19,46796.758,46740.066,46769.305,46787.89,46838.24,46836.305,46952.574,47017.832,47001.93,47129.645,47167.168,47206.867,47225.76,47192.46,47184.453,47169.473,47160.03,47161.605,47171.36,47163.598,47178.88,47198.605,47174.688,47165.1,47301.58,47330.24,47374.688,47336.5,47277.312,47279.027,47224.12,47173.566,47192.33,47164.668,47152.227,47155.86,47153.39,47151.066,47151.895,47151.492,47151.324,47150.137,47151.13,47142.094,47134.977,47135.66,47158.266,47043.31,47005.78,46950.793,46969.95,47001.27,46981.484,47013.926,47019.082,47035.266,47026.1,47024.68,47022.406,47025.34,47026.1,47037.402,47045.45,47045.703,47046.63,47050.54,47053.047,47054.254,47054.703,47039.754,46943.957,47002.0,47103.234,47127.05,47119.527,47099.184,47098.727,47105.22,47121.547,47207.59,47200.566,47202.152,47195.95,47212.805,47215.164,47214.6,47210.363,47213.31,47209.047,47207.41,47205.3,47206.28,47208.125,47212.047,47242.906,47297.613,47276.18,47346.55,47370.168,47329.297,47467.43,47561.56,47499.63,47463.527,47425.82,47441.94,47433.742,47446.188,47450.375,47451.867,47450.98,47451.215,47451.195,47443.246,47441.387,47440.875,47432.38,47470.086,47521.258,47499.035,47427.48,47452.79,47446.715,47442.02,47430.387,47460.55,47445.043,47442.496,47448.496],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\"},\"mode\":\"lines\",\"name\":\"Test Actual\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47354.723,47386.867,47480.477,47473.496,47487.812,47481.54,47479.62,47470.945,47472.492,47478.9,47458.195,47448.29,47460.25,47502.414,47531.24,47498.434,47501.688,47516.1,47485.664,47526.723,47534.07,47558.37,47580.25,47563.18,47553.977,47579.523,47578.06,47567.715,47581.76,47578.43,47569.055,47570.44,47555.64,47533.176,47540.406,47546.89,47586.316,47688.715,47680.273,47713.484,47752.676,47771.42,47776.61,47787.48,47793.324,47894.84,47900.336,47876.62,47871.887,47895.88,47923.523,47921.543,47915.22,47883.438,47889.65,47888.965,47884.215,47877.965,47851.34,47849.41,47919.895,48024.168,48001.297,47956.777,47951.668,47942.934,47936.01,47891.285,47869.402,47898.98,47915.797,47921.13,47899.66,47902.406],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Test Pred\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47483.227,47386.242,47418.324,47508.484,47501.95,47515.332,47509.53,47507.8,47499.7,47501.184,47507.227,47487.766,47478.363,47489.84,47529.258,47555.53,47525.598,47528.598,47541.83,47513.8,47551.527,47558.176,47579.906,47599.184,47584.293,47576.22,47598.746,47597.484,47588.46,47600.8,47597.94,47589.793,47591.055,47578.01,47557.918,47564.414,47570.21,47604.9,47690.04,47683.297,47709.617,47739.652,47753.676,47757.59,47765.64,47770.004,47841.027,47844.855,47828.367,47825.316,47841.832,47860.83,47859.473,47855.164,47833.395,47837.71,47837.285,47834.055,47829.848,47811.87,47810.566,47858.496,47925.254,47911.188,47883.004,47879.652,47873.86,47869.22,47838.938,47824.203,47844.21,47855.656,47859.31,47844.69],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"shapes\":[{\"line\":{\"color\":\"black\",\"dash\":\"dot\",\"width\":2},\"type\":\"line\",\"x0\":657.5,\"x1\":657.5,\"xref\":\"x\",\"y0\":0,\"y1\":1,\"yref\":\"y domain\"}],\"legend\":{\"x\":0,\"y\":1},\"title\":{\"text\":\"Gold Price Forecast\"},\"xaxis\":{\"title\":{\"text\":\"Sample Index (time ordered)\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9f4bbd59-7d7c-4900-a5c6-f4ceb86c9b01');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'MAE': np.float32(25.007195),\n",
              "  'RMSE': np.float32(38.16545),\n",
              "  'MAPE': np.float32(0.05374098),\n",
              "  'sMAPE': np.float32(0.053739373),\n",
              "  'R2': np.float32(0.99090904),\n",
              "  'MASE': np.float32(1.0977441)},\n",
              " 'test': {'MAE': np.float32(41.446842),\n",
              "  'RMSE': np.float32(51.36988),\n",
              "  'MAPE': np.float32(0.0867803),\n",
              "  'sMAPE': np.float32(0.086811945),\n",
              "  'R2': np.float32(0.92784923),\n",
              "  'MASE': np.float32(1.8193976)}}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# you already have: trainset, testset, model\n",
        "# Create the same model architecture\n",
        "\n",
        "\n",
        "model = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size=output_size,\n",
        "    dropout=dropout)\n",
        "\n",
        "# model = LSTM(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_state_dict(torch.load('/content/checkpoint_transformer.pt'))\n",
        "\n",
        "result = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "\n",
        "plot_predictions_plotly(result, horizon_step=0, title=\"Gold Price Forecast\")\n",
        "\n",
        "\n",
        "transformer_metrics = evaluate_forecast(result)\n",
        "transformer_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "tKLh7-2SFu0K",
        "outputId": "c8590724-a709-4fc8-b25d-c445a6e77726"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"59cf57a7-9936-46f5-8390-7a195c1a8134\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"59cf57a7-9936-46f5-8390-7a195c1a8134\")) {                    Plotly.newPlot(                        \"59cf57a7-9936-46f5-8390-7a195c1a8134\",                        [{\"line\":{\"color\":\"black\"},\"mode\":\"lines\",\"name\":\"Test Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47354.723,47386.867,47480.477,47473.496,47487.812,47481.54,47479.62,47470.945,47472.492,47478.9,47458.195,47448.29,47460.25,47502.414,47531.24,47498.434,47501.688,47516.1,47485.664,47526.723,47534.07,47558.37,47580.25,47563.18,47553.977,47579.523,47578.06,47567.715,47581.76,47578.43,47569.055,47570.44,47555.64,47533.176,47540.406,47546.89,47586.316,47688.715,47680.273,47713.484,47752.676,47771.42,47776.61,47787.48,47793.324,47894.84,47900.336,47876.62,47871.887,47895.88,47923.523,47921.543,47915.22,47883.438,47889.65,47888.965,47884.215,47877.965,47851.34,47849.41,47919.895,48024.168,48001.297,47956.777,47951.668,47942.934,47936.01,47891.285,47869.402,47898.98,47915.797,47921.13,47899.66,47902.406],\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"LSTM Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47454.664,47391.64,47401.434,47466.3,47475.887,47488.35,47488.13,47487.375,47481.816,47481.58,47485.664,47473.06,47463.45,47468.98,47497.453,47522.555,47508.824,47508.074,47517.15,47500.113,47521.977,47531.977,47549.74,47568.06,47563.48,47557.3,47571.125,47574.074,47569.023,47576.188,47576.258,47570.87,47570.27,47561.168,47544.867,47544.594,47548.027,47572.363,47636.94,47651.625,47675.117,47703.562,47722.895,47732.863,47742.414,47749.16,47798.156,47817.734,47815.906,47814.367,47824.484,47839.81,47844.965,47844.855,47832.145,47830.203,47829.02,47826.453,47822.71,47809.793,47804.06,47831.6,47881.805,47891.94,47880.39,47874.836,47869.5,47864.707,47844.83,47827.957,47833.46,47841.72,47846.703,47839.855],\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"GRU Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47445.223,47380.72,47386.25,47456.91,47470.54,47480.875,47478.19,47475.43,47468.668,47468.03,47472.562,47459.51,47449.023,47455.176,47486.2,47513.348,47497.176,47493.87,47503.055,47485.016,47507.594,47519.01,47537.082,47555.293,47548.312,47539.734,47554.004,47557.074,47550.8,47557.977,47557.895,47551.645,47550.87,47541.324,47524.184,47524.816,47529.926,47556.734,47625.832,47637.887,47658.098,47684.64,47701.12,47707.453,47714.41,47719.094,47771.59,47788.035,47779.88,47774.863,47785.285,47801.336,47804.543,47802.117,47786.008,47784.715,47784.316,47781.965,47778.297,47764.082,47759.383,47792.844,47848.61,47853.38,47833.938,47826.047,47820.09,47815.414,47792.977,47776.27,47786.676,47797.902,47803.355,47794.383],\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Transformer Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47483.227,47386.242,47418.324,47508.484,47501.95,47515.332,47509.53,47507.8,47499.7,47501.184,47507.227,47487.766,47478.363,47489.84,47529.258,47555.53,47525.598,47528.598,47541.83,47513.8,47551.527,47558.176,47579.906,47599.184,47584.293,47576.22,47598.746,47597.484,47588.46,47600.8,47597.94,47589.793,47591.055,47578.01,47557.918,47564.414,47570.21,47604.9,47690.04,47683.297,47709.617,47739.652,47753.676,47757.59,47765.64,47770.004,47841.027,47844.855,47828.367,47825.316,47841.832,47860.83,47859.473,47855.164,47833.395,47837.71,47837.285,47834.055,47829.848,47811.87,47810.566,47858.496,47925.254,47911.188,47883.004,47879.652,47873.86,47869.22,47838.938,47824.203,47844.21,47855.656,47859.31,47844.69],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"legend\":{\"x\":0,\"y\":1},\"title\":{\"text\":\"Gold Price Forecasts (Test)\"},\"xaxis\":{\"title\":{\"text\":\"Test Sample Index (time ordered)\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('59cf57a7-9936-46f5-8390-7a195c1a8134');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot results of models on testset\n",
        "lstm_test_pred_inv = predict_train_test(lstm_model, trainset, testset, batch_size=128, scaler=scaler)['test_pred_inv']\n",
        "gru_test_pred_inv = predict_train_test(gru_model, trainset, testset, batch_size=128, scaler=scaler)['test_pred_inv']\n",
        "transformer_test_pred_inv = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)['test_pred_inv']\n",
        "test_true_inv = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)['test_true_inv']\n",
        "\n",
        "\n",
        "results = {\n",
        "    \"test_true_inv\": test_true_inv,           # shape [N] or [N, H]\n",
        "    \"preds\": {\n",
        "        \"LSTM\": lstm_test_pred_inv,           # shape [N] or [N, H]\n",
        "        \"GRU\": gru_test_pred_inv,\n",
        "        \"Transformer\": transformer_test_pred_inv\n",
        "    }\n",
        "}\n",
        "\n",
        "plot_test_predictions_plotly(results, horizon_step=0, title=\"Gold Price Forecasts (Test)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbgSeOvA8Knn"
      },
      "source": [
        "## Grid Search (Params Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s0nKb1M8PX2"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "param_grid = {\n",
        "    'd_model': [64, 256],\n",
        "    'lr': [0.001, 0.01],\n",
        "    'nhead': [8, 16],\n",
        "    'num_encoder_layers': [1, 6],\n",
        "    'dim_feedforward':[64, 256],\n",
        "    'dropout': [0.1, 0.5]\n",
        "\n",
        "}\n",
        "\n",
        "keys = param_grid.keys()\n",
        "\n",
        "combinations = [dict(zip(keys, values)) for values in product(*param_grid.values())]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT2tW6lh-N-q"
      },
      "outputs": [],
      "source": [
        "augmentations = Compose([AddGaussianNoise(), RandomScaling()])\n",
        "\n",
        "trainset, train_loader, testset, test_loader = get_loaders(df, time_bin=60, scaler=scaler, lookback=6,\n",
        "                                        lookforward=1, batch_size=16, train_size=0.9,\n",
        "                                        transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0LNDPU2X-iJJ",
        "outputId": "8ade23da-b59f-45ca-afff-66e07026ea1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 1] Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 2] Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 3] Early stopping at epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 4] Early stopping at epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 5] Early stopping at epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 6] Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 7] Early stopping at epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 8] Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 9] Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 10] Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 11] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 12] Early stopping at epoch 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 13] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 14] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 15] Early stopping at epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 16] Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 17] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 18] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 19] Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 20] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 21] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 22] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 23] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 24] Early stopping at epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 25] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 26] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 27] Early stopping at epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 28] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 29] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 30] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 31] Early stopping at epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 32] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 33] Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 34] Early stopping at epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 35] Early stopping at epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 36] Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 37] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 38] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 39] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 40] Early stopping at epoch 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 41] Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 42] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 43] Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 44] Early stopping at epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 45] Early stopping at epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 46] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 47] Early stopping at epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 48] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 49] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 50] Early stopping at epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 51] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 52] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 53] Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 54] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 55] Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 56] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 57] Early stopping at epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 58] Early stopping at epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 59] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 60] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 61] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 62] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 63] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 64] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   d_model     lr  nhead  num_encoder_layers  dim_feedforward  dropout  \\\n",
              "0      256  0.001      8                   1              256      0.1   \n",
              "1      256  0.001     16                   1               64      0.1   \n",
              "2       64  0.010      8                   1               64      0.1   \n",
              "3      256  0.001      8                   1               64      0.1   \n",
              "4      256  0.001      8                   1              256      0.5   \n",
              "\n",
              "    train_MAE  train_RMSE  train_MAPE  train_sMAPE  train_R2  train_MASE  \\\n",
              "0   59.876881  311.896606    0.134548     0.131579  0.635762    1.277514   \n",
              "1  104.224777  318.076447    0.230257     0.227805  0.621185    2.223706   \n",
              "2   47.454025  307.333282    0.107617     0.104765  0.646342    1.012464   \n",
              "3  106.763321  324.294800    0.235487     0.232300  0.606228    2.277868   \n",
              "4  156.878799  349.178284    0.343728     0.340185  0.543481    3.347116   \n",
              "\n",
              "    test_MAE  test_RMSE  test_MAPE  test_sMAPE   test_R2  test_MASE  \n",
              "0  28.325327  38.131069   0.059378    0.059385  0.960992   0.604340  \n",
              "1  30.906672  38.924900   0.064777    0.064772  0.959350   0.659415  \n",
              "2  30.021906  41.927132   0.062885    0.062913  0.952838   0.640538  \n",
              "3  36.672825  45.775204   0.076973    0.076952  0.943784   0.782440  \n",
              "4  37.275181  46.248051   0.078194    0.078180  0.942616   0.795291  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec1bb963-416d-44e0-a227-27b0d5bcbf58\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>d_model</th>\n",
              "      <th>lr</th>\n",
              "      <th>nhead</th>\n",
              "      <th>num_encoder_layers</th>\n",
              "      <th>dim_feedforward</th>\n",
              "      <th>dropout</th>\n",
              "      <th>train_MAE</th>\n",
              "      <th>train_RMSE</th>\n",
              "      <th>train_MAPE</th>\n",
              "      <th>train_sMAPE</th>\n",
              "      <th>train_R2</th>\n",
              "      <th>train_MASE</th>\n",
              "      <th>test_MAE</th>\n",
              "      <th>test_RMSE</th>\n",
              "      <th>test_MAPE</th>\n",
              "      <th>test_sMAPE</th>\n",
              "      <th>test_R2</th>\n",
              "      <th>test_MASE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>256</td>\n",
              "      <td>0.1</td>\n",
              "      <td>59.876881</td>\n",
              "      <td>311.896606</td>\n",
              "      <td>0.134548</td>\n",
              "      <td>0.131579</td>\n",
              "      <td>0.635762</td>\n",
              "      <td>1.277514</td>\n",
              "      <td>28.325327</td>\n",
              "      <td>38.131069</td>\n",
              "      <td>0.059378</td>\n",
              "      <td>0.059385</td>\n",
              "      <td>0.960992</td>\n",
              "      <td>0.604340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>104.224777</td>\n",
              "      <td>318.076447</td>\n",
              "      <td>0.230257</td>\n",
              "      <td>0.227805</td>\n",
              "      <td>0.621185</td>\n",
              "      <td>2.223706</td>\n",
              "      <td>30.906672</td>\n",
              "      <td>38.924900</td>\n",
              "      <td>0.064777</td>\n",
              "      <td>0.064772</td>\n",
              "      <td>0.959350</td>\n",
              "      <td>0.659415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>0.010</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>47.454025</td>\n",
              "      <td>307.333282</td>\n",
              "      <td>0.107617</td>\n",
              "      <td>0.104765</td>\n",
              "      <td>0.646342</td>\n",
              "      <td>1.012464</td>\n",
              "      <td>30.021906</td>\n",
              "      <td>41.927132</td>\n",
              "      <td>0.062885</td>\n",
              "      <td>0.062913</td>\n",
              "      <td>0.952838</td>\n",
              "      <td>0.640538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>106.763321</td>\n",
              "      <td>324.294800</td>\n",
              "      <td>0.235487</td>\n",
              "      <td>0.232300</td>\n",
              "      <td>0.606228</td>\n",
              "      <td>2.277868</td>\n",
              "      <td>36.672825</td>\n",
              "      <td>45.775204</td>\n",
              "      <td>0.076973</td>\n",
              "      <td>0.076952</td>\n",
              "      <td>0.943784</td>\n",
              "      <td>0.782440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>256</td>\n",
              "      <td>0.5</td>\n",
              "      <td>156.878799</td>\n",
              "      <td>349.178284</td>\n",
              "      <td>0.343728</td>\n",
              "      <td>0.340185</td>\n",
              "      <td>0.543481</td>\n",
              "      <td>3.347116</td>\n",
              "      <td>37.275181</td>\n",
              "      <td>46.248051</td>\n",
              "      <td>0.078194</td>\n",
              "      <td>0.078180</td>\n",
              "      <td>0.942616</td>\n",
              "      <td>0.795291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec1bb963-416d-44e0-a227-27b0d5bcbf58')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec1bb963-416d-44e0-a227-27b0d5bcbf58 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec1bb963-416d-44e0-a227-27b0d5bcbf58');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-de38aaf9-2bb0-4547-9786-ed1dc501e5a1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de38aaf9-2bb0-4547-9786-ed1dc501e5a1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-de38aaf9-2bb0-4547-9786-ed1dc501e5a1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_grid",
              "summary": "{\n  \"name\": \"df_grid\",\n  \"rows\": 64,\n  \"fields\": [\n    {\n      \"column\": \"d_model\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96,\n        \"min\": 64,\n        \"max\": 256,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64,\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004535573676110723,\n        \"min\": 0.001,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nhead\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 8,\n        \"max\": 16,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          16,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_encoder_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dim_feedforward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96,\n        \"min\": 64,\n        \"max\": 256,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64,\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20158105227158793,\n        \"min\": 0.1,\n        \"max\": 0.5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_MAE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          614.7304077148438,\n          451.2472839355469\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_RMSE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          760.3920288085938,\n          598.1897583007812\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.3335003852844238,\n          0.9788501858711243\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_sMAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.3195170164108276,\n          0.9711848497390747\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_R2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          -1.1649088859558105,\n          -0.3398076295852661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_MASE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          13.115689277648926,\n          9.627666473388672\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_MAE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          632.9443969726562,\n          893.9075927734375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_RMSE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          661.4508056640625,\n          914.5366821289062\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.325530767440796,\n          1.8727104663848877\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_sMAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.3351795673370361,\n          1.8912229537963867\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_R2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          -10.738036155700684,\n          -21.43895149230957\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_MASE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          13.504297256469727,\n          19.072124481201172\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "num_epochs = 200\n",
        "results = []\n",
        "\n",
        "for cfg_idx, config in enumerate(combinations, start=1):\n",
        "\n",
        "  model = TransformerTS(\n",
        "      input_size=1,\n",
        "      d_model=config['d_model'],\n",
        "      nhead=config['nhead'],\n",
        "      num_encoder_layers=config['num_encoder_layers'],\n",
        "      dim_feedforward=config['dim_feedforward'],\n",
        "      output_size=1,\n",
        "      dropout=config['dropout'])\n",
        "\n",
        "  model.to(device)\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
        "  early_stopping = EarlyStopping(patience=10, mode='min', verbose=False, save_path= f'/content/checkpoint_{cfg_idx}.pt')\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      train_one_epoch(model, show=False)\n",
        "      stop_criteria = validate_one_epoch(model, show=False, early_stopping=early_stopping)\n",
        "\n",
        "      if stop_criteria:\n",
        "          print(f\"[CFG {cfg_idx}] Early stopping at epoch {epoch}\")\n",
        "          break\n",
        "\n",
        "\n",
        "\n",
        "  model = TransformerTS(\n",
        "      input_size=1,\n",
        "      d_model=config['d_model'],\n",
        "      nhead=config['nhead'],\n",
        "      num_encoder_layers=config['num_encoder_layers'],\n",
        "      dim_feedforward=config['dim_feedforward'],\n",
        "      output_size=1,\n",
        "      dropout=config['dropout'])\n",
        "\n",
        "  model.load_state_dict(torch.load(f'/content/checkpoint_{cfg_idx}.pt'))\n",
        "\n",
        "  result = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "  transformer_metrics = evaluate_forecast(result)\n",
        "\n",
        "\n",
        "  # متریک‌های train\n",
        "  for k, v in transformer_metrics['train'].items():\n",
        "      config[f\"train_{k}\"] = v\n",
        "\n",
        "  # متریک‌های test\n",
        "  for k, v in transformer_metrics['test'].items():\n",
        "      config[f\"test_{k}\"] = v\n",
        "\n",
        "  results.append(config)\n",
        "\n",
        "df_grid = pd.DataFrame(results)\n",
        "\n",
        "if \"test_RMSE\" in df_grid.columns:\n",
        "    df_grid = df_grid.sort_values(by=\"test_RMSE\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "df_grid.to_csv(\"transformer_grid_results.csv\", index=False)\n",
        "df_grid.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GtgFQkzQd8G"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8g_ilnxFzZh"
      },
      "source": [
        "\n",
        "*   Multiple step prediction\n",
        "*   How much Lookback\n",
        "*   Effect of TimeBin\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple step prediction"
      ],
      "metadata": {
        "id": "pR3teEB87jr2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WzAgIG6F33q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855bb28b-7fad-4b56-bccf-82287be7521b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.05561\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.05150\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02522\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02186\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00452\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00149\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00246\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00101\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00127\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00151\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00088\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00143\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00069\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00132\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00076\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00084\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00062\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00310\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00143\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00190\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00155\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00189\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00059\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00395\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00080\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00386\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00116\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00153\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00324\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00269\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00208\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00174\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00135\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00170\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00237\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00048\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00176\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00545\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00066\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00219\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00045\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00037\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00047\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00066\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00207\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00149\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00069\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00076\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00144\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00306\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00113\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00111\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00277\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00042\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00347\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00050\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00410\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00102\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00224\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00155\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00059\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00225\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00164\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00215\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00165\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00036\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00039\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00158\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00130\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00116\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00130\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00088\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00065\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00060\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00057\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00070\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00114\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00052\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00030\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00212\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00028\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00044\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00067\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00074\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00093\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00242\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00039\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00113\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00035\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00076\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00078\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00112\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00092\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00041\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00213\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00286\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00152\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00040\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00124\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00054\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00108\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00378\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00057\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00137\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00043\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00042\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00197\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00038\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00139\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00131\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00126\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00508\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00065\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00066\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00034\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00135\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00242\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00236\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00029\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00058\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00032\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00044\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00037\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00321\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00037\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00030\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00042\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00040\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00095\n",
            "***************************************************\n",
            "\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "early_stopping = EarlyStopping(patience=50, mode='min', verbose=False, save_path='/content/checkpoint_lookforward.pt')\n",
        "\n",
        "\n",
        "trainset, train_loader, testset, test_loader = get_loaders(df, time_bin=60, scaler=scaler, lookback=6,\n",
        "                                        lookforward=10, batch_size=16, train_size=0.9,\n",
        "                                        transform=augmentations)\n",
        "model_lookforward = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size= 10,\n",
        "    dropout=dropout)\n",
        "\n",
        "# model = GRUModel(1, 4, 1, 1)\n",
        "model_lookforward.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model_lookforward.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model_lookforward, show=False)\n",
        "    stop_criteria = validate_one_epoch(model_lookforward, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self, sample_mean=3, lookahead_steps=3):\n",
        "        \"\"\"\n",
        "        Baseline model to predict `lookahead_steps` using the mean of the previous `sample_mean` time steps.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sample_mean = sample_mean\n",
        "        self.lookahead_steps = lookahead_steps\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: input of shape [batch_size, time_steps, features]\n",
        "        :return: predictions of shape [batch_size, lookahead_steps, features]\n",
        "        \"\"\"\n",
        "        # Get the last `sample_mean` number of time steps\n",
        "        last_samples = x[:, -self.sample_mean:, :]  # shape: [batch, sample_mean, features]\n",
        "\n",
        "        # Calculate the mean over the last `sample_mean` steps\n",
        "        mean_value = last_samples.mean(dim=1)  # shape: [batch, features]\n",
        "\n",
        "        # Repeat the mean value to generate predictions for the next `lookahead_steps`\n",
        "        repeated_predictions = mean_value.unsqueeze(1).repeat(1, self.lookahead_steps, 1)\n",
        "\n",
        "        return repeated_predictions\n"
      ],
      "metadata": {
        "id": "opzt7LyEAqY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "baselineModel = BaselineModel(sample_mean=10, lookahead_steps=10)\n",
        "\n",
        "\n",
        "model_lookforward.eval()\n",
        "all_predictions = []\n",
        "all_predictions_baseline = []\n",
        "all_actual_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(test_loader):  # test_loader should provide the test data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        predicted_baseline = baselineModel(inputs)\n",
        "        predicted = model_lookforward(inputs)  # predicted shape (batch_size, 3)\n",
        "        all_predictions_baseline.append(predicted_baseline.cpu().numpy())\n",
        "        all_predictions.append(predicted.cpu().numpy())\n",
        "        all_actual_values.append(labels.cpu().numpy())\n",
        "\n",
        "# Convert the list of predictions and labels to numpy arrays for easier plotting\n",
        "all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "all_actual_values = np.concatenate(all_actual_values, axis=0)\n",
        "all_predictions_baseline = np.concatenate(all_predictions_baseline, axis=0)\n",
        "\n",
        "\n",
        "print(mean_absolute_error(all_actual_values, all_predictions),\n",
        "mean_absolute_error(all_actual_values, all_predictions_baseline.squeeze(-1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc57iHEF7zJq",
        "outputId": "89b1397c-1250-49fc-9a05-e715dc02e5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.024832408875226974 0.013545608147978783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the BaselineModel\n",
        "baseline_model = BaselineModel(sample_mean=3, lookahead_steps=3)\n",
        "\n",
        "result = predict_train_test(baseline_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "\n",
        "baseline_metrics = evaluate_forecast(result)\n",
        "baseline_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KumMSy7s9Ry-",
        "outputId": "473c6ee1-2f33-467d-f155-e041b8b0ac41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'MAE': np.float32(60.92469),\n",
              "  'RMSE': np.float32(357.41925),\n",
              "  'MAPE': np.float32(0.13697843),\n",
              "  'sMAPE': np.float32(0.13486174),\n",
              "  'R2': np.float32(0.50790465),\n",
              "  'MASE': np.float32(1.053577)},\n",
              " 'test': {'MAE': np.float32(29.475332),\n",
              "  'RMSE': np.float32(41.677708),\n",
              "  'MAPE': np.float32(0.06180005),\n",
              "  'sMAPE': np.float32(0.0618294),\n",
              "  'R2': np.float32(0.95263547),\n",
              "  'MASE': np.float32(0.50971997)}}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NO2h3ZL_r4U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}