{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook focuses on **gold price forecasting** using a combination of baseline and deep learning models implemented in PyTorch. The project aims to predict future gold prices based on historical market data, enabling informed decision-making for trading and investment strategies.\n",
        "\n",
        "The workflow includes:\n",
        "\n",
        "* **Data Loading and Preprocessing** — Importing gold price data, applying statistical checks, scaling features, and generating technical indicators to enhance predictive power.\n",
        "* **Feature Engineering** — Leveraging rolling statistical tests and custom transformations to capture temporal dynamics in gold price movements.\n",
        "* **Model Development** — Implementing multiple architectures including:\n",
        "\n",
        "  * **Baseline Model** — A simple average-based predictor for performance benchmarking.\n",
        "  * **LSTM and GRU Networks** — Recurrent neural network models designed to capture sequential dependencies in time series data.\n",
        "* **Training and Evaluation** — Applying a structured training/testing regime, integrating techniques such as early stopping, and comparing model performances against the baseline.\n",
        "\n",
        "By establishing a robust comparison between naive prediction methods and advanced neural architectures, this notebook not only measures forecasting accuracy but also highlights the added value of deep learning for financial time series prediction.\n"
      ],
      "metadata": {
        "id": "8HWgI6bWS55c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMuRZfc26cgT",
        "outputId": "8c387934-60c3-4b82-9b38-88d266d73293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=d673f9afbdb01e48a3bc6a5a81181bf5c67d5aec79d58915660a7d83af70d668\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.5-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from ptflops) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.5-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.5\n"
          ]
        }
      ],
      "source": [
        " ! pip install ta\n",
        " ! pip install ptflops\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tteu6Rhk6HHi"
      },
      "source": [
        "# import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqHG44Kw6Cl2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from ptflops import get_model_complexity_info\n",
        "import math\n",
        "import seaborn as sns\n",
        "import ta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yHH7wOW6L31"
      },
      "source": [
        "# Load Data and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfp_h0Qm6UhS"
      },
      "outputs": [],
      "source": [
        "def rolling_adf(df, col, window_size=30):\n",
        "    \"\"\"\n",
        "    Calculate the Augmented Dickey-Fuller test statistic on a rolling window.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing the column on which to perform the ADF test.\n",
        "    col : str\n",
        "        The name of the column on which to perform the ADF test.\n",
        "    window_size : int\n",
        "        The size of the rolling window.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_copy : pandas.DataFrame\n",
        "        A new DataFrame with an additional column containing the rolling ADF test statistic.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Create an empty series to store rolling ADF test statistic\n",
        "    rolling_adf_stat = pd.Series(dtype='float64', index=df_copy.index)\n",
        "\n",
        "    # Loop through the DataFrame by `window_size` and apply `adfuller`.\n",
        "    for i in range(window_size, len(df)):\n",
        "        window = df_copy[col].iloc[i-window_size:i]\n",
        "        adf_result = adfuller(window)\n",
        "        adf_stat = adf_result[0]\n",
        "        rolling_adf_stat.at[df_copy.index[i]] = adf_stat\n",
        "\n",
        "    # Add the rolling ADF test statistic series to the original DataFrame\n",
        "    # df_copy['rolling_adf_stat'] = rolling_adf_stat\n",
        "\n",
        "    return rolling_adf_stat\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Kaufman’s Adaptive Moving Average (KAMA)\n",
        "\n",
        "def kama(df, col, n):\n",
        "  df_copy = df.copy()\n",
        "  # df_copy[f'kama_{n}'] = ta.momentum.KAMAIndicator(df_copy[col], n).kama()\n",
        "  return ta.momentum.KAMAIndicator(df_copy[col], n).kama()\n",
        "\n",
        "def moving_parkinson_estimator(df, window_size=10):\n",
        "\n",
        "  def parkinson_estimator(df):\n",
        "    N = len(df)\n",
        "    sum_squared = np.sum(np.log(df['High'] / df['Low'])**2)\n",
        "    volatility = math.sqrt((1/(4 * N * math.log(2))) * sum_squared)\n",
        "    return volatility\n",
        "\n",
        "  df_copy = df.copy()\n",
        "\n",
        "  rolling_volatility = pd.Series(dtype='float64')\n",
        "\n",
        "  for i in range(window_size, len(df)):\n",
        "    window = df_copy.loc[df_copy.index[i-window_size] : df_copy.index[i]]\n",
        "    volatility = parkinson_estimator(window)\n",
        "    rolling_volatility.at[df_copy.index[i]] = volatility\n",
        "\n",
        "  # df_copy['rolling_volatility_parkinson'] = rolling_volatility\n",
        "\n",
        "  return rolling_volatility\n",
        "\n",
        "def moving_yang_zhang_estimator(df, window_size=30):\n",
        "    \"\"\"\n",
        "    Calculate Parkinson's volatility estimator based on high and low prices.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing 'high' and 'low' columns for each trading period.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    volatility : float\n",
        "        Estimated volatility based on Parkinson's method.\n",
        "    \"\"\"\n",
        "    def yang_zhang_estimator(df):\n",
        "        N = len(window)\n",
        "\n",
        "        term1 = np.log(window['High'] / window['Close']) * np.log(window['High'] / window['Open'])\n",
        "        term2 = np.log(window['Low'] / window['Close']) * np.log(window['Low'] / window['Open'])\n",
        "\n",
        "        sum_squared = np.sum(term1 + term2)\n",
        "        volatility = np.sqrt(sum_squared / N)\n",
        "\n",
        "        return volatility\n",
        "\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    rolling_volatility = pd.Series(dtype='float64')\n",
        "\n",
        "    for i in range(window_size, len(df)):\n",
        "        window = df_copy.loc[df_copy.index[i-window_size]: df_copy.index[i]]\n",
        "        volatility = yang_zhang_estimator(window)\n",
        "        rolling_volatility.at[df_copy.index[i]] = volatility\n",
        "\n",
        "\n",
        "    return rolling_volatility\n",
        "\n",
        "def RSI(df, col, window_size):\n",
        "  delta = df[col].diff(1)\n",
        "  positive = delta.copy()\n",
        "  negative = delta.copy()\n",
        "  positive[positive < 0] = 0\n",
        "  negative[negative > 0] = 0\n",
        "  average_gain = positive.rolling(window=window_size).mean()\n",
        "  average_loss = abs(negative.rolling(window=window_size).mean())\n",
        "  relative_strength = average_gain / average_loss\n",
        "  RSI = 100.0 - (100.0 - (1.0 + relative_strength))\n",
        "\n",
        "  return RSI\n",
        "\n",
        "\n",
        "def create_features(df, window_size=30):\n",
        "  df['moving_parkinson'] = moving_parkinson_estimator(df, window_size=window_size)\n",
        "  df['rolling_adf'] = rolling_adf(df, 'Price', window_size=window_size)\n",
        "  df['moving_yang_zhang'] = moving_yang_zhang_estimator(df, window_size=window_size)\n",
        "  df['kama'] =kama(df, 'Price', window_size)\n",
        "  df['rsi'] = RSI(df, 'Price', window_size)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_MonFxF6tIb"
      },
      "outputs": [],
      "source": [
        "pth = '/content/drive/MyDrive/data.xlsx'\n",
        "\n",
        "df = pd.read_excel(pth, header=1, sheet_name='Data')\n",
        "\n",
        "df['Timestamp'] = pd.to_timedelta(df['Day'], unit='d') + \\\n",
        "                pd.to_timedelta(df['Hour'], unit='h') + \\\n",
        "                pd.to_timedelta(df['Minute'], unit='m') + \\\n",
        "                pd.to_timedelta(df['Second'], unit='s')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers(df):\n",
        "  df1 = deepcopy(df)\n",
        "  z_scores = (df1['Price.'] - df1['Price.'].mean()) / df1['Price.'].std()\n",
        "\n",
        "  # Filter out rows where |z| > 2\n",
        "  df_no_outliers = df1[np.abs(z_scores) <= 2.0]\n",
        "\n",
        "  # Optional: reset index\n",
        "  df_no_outliers = df_no_outliers.reset_index()\n",
        "\n",
        "  return df_no_outliers\n",
        "\n",
        "\n",
        "\n",
        "df_no_outliers = remove_outliers(df)"
      ],
      "metadata": {
        "id": "Dn1IqKe37fHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 1)\n",
        "axs[0].plot(df['Price.'])\n",
        "axs[1].plot(df_no_outliers['Price.'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "bYJITXDTyXHB",
        "outputId": "b48e1df2-c8d4-4c7a-fd36-b7d48a8ca035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7a84fd3ff890>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8wfPXVX6J_z"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsjTYESI04kF"
      },
      "outputs": [],
      "source": [
        "class AddGaussianNoise:\n",
        "    def __init__(self):\n",
        "        self.noise_std = random.uniform(0, 0.005)\n",
        "    def __call__(self, x):\n",
        "        noise = torch.randn_like(x) * self.noise_std * x\n",
        "        return x + noise\n",
        "\n",
        "class RandomScaling:\n",
        "    def __init__(self, scale_range=(0.995, 1.005)):\n",
        "        self.scale_range = scale_range\n",
        "    def __call__(self, x):\n",
        "        scale = np.random.uniform(*self.scale_range)\n",
        "        return x * scale\n",
        "\n",
        "class Compose:\n",
        "    \"\"\"Chain multiple augmentations.\"\"\"\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "    def __call__(self, x):\n",
        "        for t in self.transforms:\n",
        "            x = t(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9AWmTzW6Lae"
      },
      "outputs": [],
      "source": [
        "class GoldDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataset, lookback, transform=None):\n",
        "\n",
        "    self.transform = transform\n",
        "    self.dataset = dataset\n",
        "    self.lookback = lookback\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    data = self.transform(self.dataset[idx]) if self.transform else self.dataset[idx]\n",
        "\n",
        "    x = data[:self.lookback].unsqueeze(-1)\n",
        "    y = data[self.lookback:]\n",
        "\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSar_zTr8X0y"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "def get_loaders(dataframe, time_bin, scaler, lookback, lookforward,batch_size: int, train_size:float,  transform=None):\n",
        "\n",
        "    dataframe['TimeBin'] = df['Timestamp'].dt.floor(f'{time_bin}min')\n",
        "    grouped_df = dataframe.groupby('TimeBin').agg(\n",
        "                Price = ('Price.', 'mean'))\n",
        "    lookforward -= 1\n",
        "\n",
        "    grouped_df['ScaledPrice'] = scaler.fit_transform(grouped_df[['Price']])\n",
        "\n",
        "    for i in range((lookback+lookforward), 0, -1):\n",
        "\n",
        "      grouped_df[f'price_lag_{i}'] = grouped_df['ScaledPrice'].shift(i)\n",
        "\n",
        "    grouped_df.dropna(inplace=True)\n",
        "\n",
        "    lag_cols = [f'price_lag_{i}' for i in range(lookback+lookforward, 0, -1)]\n",
        "    lag_cols.append('ScaledPrice')\n",
        "\n",
        "    data = torch.tensor(grouped_df[lag_cols].values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    index = int(len(grouped_df) * train_size)\n",
        "\n",
        "    train_raw = data[:index, :]\n",
        "    test_raw = data[index:, :]\n",
        "\n",
        "    trainset = GoldDataset(train_raw, lookback, transform)\n",
        "    testset = GoldDataset(test_raw, lookback, None)\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return trainset, train_loader, testset, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKjGBmkY7-m7"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "  def __init__(self, sample_mean=3):\n",
        "\n",
        "    super().__init__()\n",
        "    self.sample_mean = sample_mean\n",
        "\n",
        "  def forward(self, x):\n",
        "    last_samples = x[:, -self.sample_mean:, :]   # shape: [batch, 2, 1]\n",
        "    return last_samples.mean(dim=1)"
      ],
      "metadata": {
        "id": "OO4lvKmeFkDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vp1OQNz6iIw"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_stacked_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_stacked_layers = num_stacked_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvlZWAeQ0d3e"
      },
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_stacked_layers, output_size, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_stacked_layers\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_stacked_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_stacked_layers > 1 else 0.0\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, C]\n",
        "        B = x.size(0)\n",
        "        h0 = torch.zeros(self.num_layers, B, self.hidden_size, device=x.device)\n",
        "        out, _ = self.gru(x, h0)     # out: [B, T, H]\n",
        "        out = out[:, -1, :]          # last timestep\n",
        "        out = self.fc(out)           # [B, output_size]\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lqj166D06lE"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)                  # [T, D]\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)    # [T, 1]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)  # not a parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, D]\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:T].unsqueeze(0)   # broadcast to [B, T, D]\n",
        "\n",
        "class TransformerTS(nn.Module):\n",
        "    \"\"\"\n",
        "    Projects input_size -> d_model, adds positional encoding,\n",
        "    passes through TransformerEncoder, takes last timestep, then a head.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 num_encoder_layers,\n",
        "                 dim_feedforward,\n",
        "                 output_size,\n",
        "                 dropout=0.1,\n",
        "                 layer_norm_eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_encoder_layers,\n",
        "            norm=nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        # x: [B, T, C]\n",
        "        x = self.input_proj(x)            # [B, T, D]\n",
        "        x = self.pos_enc(x)               # [B, T, D]\n",
        "        # src_key_padding_mask: [B, T] with True for PAD positions (optional)\n",
        "        h = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # [B, T, D]\n",
        "        h_last = h[:, -1, :]              # last timestep representation\n",
        "        out = self.head(h_last)           # [B, output_size]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51tGBmNs8CG7"
      },
      "source": [
        "# training & test regime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzKs_3miAgc_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0, mode='min', verbose=False, save_path='checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        patience : int         # how many epochs to wait without improvement\n",
        "        min_delta : float      # minimum change to count as an improvement\n",
        "        mode : 'max' | 'min'   # 'max' for accuracy, 'min' for loss\n",
        "        verbose : bool         # print messages when improvement happens\n",
        "        save_path : str        # path to save the best model\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.verbose = verbose\n",
        "        self.save_path = save_path\n",
        "\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_best = np.inf if mode == 'min' else -np.inf\n",
        "\n",
        "    def __call__(self, val_metric, model):\n",
        "        score = val_metric\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            if self.best_score is None:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "            elif score > self.best_score - self.min_delta:\n",
        "                self.counter += 1\n",
        "                if self.verbose:\n",
        "                    print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "                if self.counter >= self.patience:\n",
        "                    self.early_stop = True\n",
        "            else:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "                self.counter = 0\n",
        "        else:  # mode == 'max'\n",
        "            if self.best_score is None:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "            elif score < self.best_score + self.min_delta:\n",
        "                self.counter += 1\n",
        "                if self.verbose:\n",
        "                    print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "                if self.counter >= self.patience:\n",
        "                    self.early_stop = True\n",
        "            else:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "                self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        \"\"\"Saves model when validation metric improves.\"\"\"\n",
        "        torch.save(model.state_dict(), self.save_path)\n",
        "        if self.verbose:\n",
        "            print(f\"Model improved. Saving model to {self.save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PjYtPD18BgK"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, show=True):\n",
        "\n",
        "    model.train(True)\n",
        "\n",
        "    if show:\n",
        "      print(f'Epoch: {epoch + 1}')\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_index, batch in enumerate(train_loader):\n",
        "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "        output = model(x_batch)\n",
        "        loss = loss_function(output, y_batch)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_index % 100 == 99:  # print every 100 batches\n",
        "            avg_loss_across_batches = running_loss / 100\n",
        "            if show:\n",
        "              print('Batch {0}, Loss: {1:.3f}'.format(batch_index+1,\n",
        "                                                    avg_loss_across_batches))\n",
        "            running_loss = 0.0\n",
        "    if show:\n",
        "      print()\n",
        "\n",
        "def validate_one_epoch(model,early_stopping, show=True):\n",
        "    model.train(False)\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_index, batch in enumerate(test_loader):\n",
        "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(x_batch)\n",
        "            loss = loss_function(output, y_batch)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    avg_loss_across_batches = running_loss / len(test_loader)\n",
        "    early_stopping(avg_loss_across_batches, model)\n",
        "\n",
        "\n",
        "    if show:\n",
        "      print('Val Loss: {0:.5f}'.format(avg_loss_across_batches))\n",
        "      print('***************************************************')\n",
        "      print()\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "      return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWLksV-tKKRC"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7OXYXinKPqH"
      },
      "outputs": [],
      "source": [
        "def _mae(y, yhat):\n",
        "    return np.mean(np.abs(y - yhat))\n",
        "\n",
        "def _rmse(y, yhat):\n",
        "    return np.sqrt(np.mean((y - yhat) ** 2))\n",
        "\n",
        "def _mape(y, yhat, eps=1e-8):\n",
        "    denom = np.maximum(np.abs(y), eps)\n",
        "    return 100.0 * np.mean(np.abs((y - yhat) / denom))\n",
        "\n",
        "def _smape(y, yhat, eps=1e-8):\n",
        "    denom = np.maximum((np.abs(y) + np.abs(yhat)) / 2.0, eps)\n",
        "    return 100.0 * np.mean(np.abs(y - yhat) / denom)\n",
        "\n",
        "def _r2(y, yhat):\n",
        "    ss_res = np.sum((y - yhat) ** 2)\n",
        "    ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
        "    return 1.0 - ss_res / ss_tot if ss_tot > 0 else np.nan\n",
        "\n",
        "def _mase(y_insample, y_insample_naive, y, yhat, eps=1e-12):\n",
        "    \"\"\"\n",
        "    y_insample: series used to compute naive scale (e.g., train_true[1:])\n",
        "    y_insample_naive: naive preds for insample (e.g., train_true[:-1])\n",
        "    MASE = MAE(model)/MAE(naive) using in-sample naive MAE as the scale.\n",
        "    \"\"\"\n",
        "    scale = _mae(y_insample, y_insample_naive)\n",
        "    return _mae(y, yhat) / (scale + eps)\n",
        "\n",
        "def evaluate_forecast(out_dict, horizon_step=0):\n",
        "\n",
        "    def select_step(arr):\n",
        "      if arr.ndim == 2:\n",
        "          return arr[:, horizon_step]\n",
        "      return arr\n",
        "\n",
        "    train_pred = select_step(out_dict[\"train_pred_inv\"])\n",
        "    train_true = select_step(out_dict[\"train_true_inv\"])\n",
        "    test_pred  = select_step(out_dict[\"test_pred_inv\"])\n",
        "    test_true  = select_step(out_dict[\"test_true_inv\"])\n",
        "    # ensure 1D & drop NaNs if any\n",
        "    def _clean(a):\n",
        "        a = np.asarray(a).reshape(-1)\n",
        "        return a[~np.isnan(a)]\n",
        "    train_true = _clean(train_true); train_pred = _clean(train_pred)\n",
        "    test_true  = _clean(test_true);  test_pred  = _clean(test_pred)\n",
        "\n",
        "    # In-sample naive (one-step) for MASE scale: y_{t-1}\n",
        "    # Align lengths: compare train_true[1:] vs naive = train_true[:-1]\n",
        "    if len(train_true) >= 2:\n",
        "        insample_y = train_true[1:]\n",
        "        insample_naive = train_true[:-1]\n",
        "        mase_train = _mase(insample_y, insample_naive, train_true, train_pred)\n",
        "        mase_test  = _mase(insample_y, insample_naive, test_true,  test_pred)\n",
        "    else:\n",
        "        mase_train = np.nan\n",
        "        mase_test  = np.nan\n",
        "\n",
        "    metrics_train = {\n",
        "        \"MAE\":  _mae(train_true, train_pred),\n",
        "        \"RMSE\": _rmse(train_true, train_pred),\n",
        "        \"MAPE\": _mape(train_true, train_pred),\n",
        "        \"sMAPE\": _smape(train_true, train_pred),\n",
        "        \"R2\":   _r2(train_true, train_pred),\n",
        "        \"MASE\": mase_train,\n",
        "    }\n",
        "    metrics_test = {\n",
        "        \"MAE\":  _mae(test_true, test_pred),\n",
        "        \"RMSE\": _rmse(test_true, test_pred),\n",
        "        \"MAPE\": _mape(test_true, test_pred),\n",
        "        \"sMAPE\": _smape(test_true, test_pred),\n",
        "        \"R2\":   _r2(test_true, test_pred),\n",
        "        \"MASE\": mase_test,\n",
        "    }\n",
        "    return {\"train\": metrics_train, \"test\": metrics_test}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jht49w2-Wdf_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def _predict_on_loader(model, loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    preds, trues = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)                 # [B, lookback, 1]\n",
        "        y = y.to(device)                 # [B, horizon]\n",
        "        out = model(x)                   # expect [B, horizon] (or [B, horizon, 1] -> squeeze)\n",
        "        if out.dim() == 3 and out.size(-1) == 1:\n",
        "            out = out.squeeze(-1)\n",
        "        preds.append(out.detach().cpu())\n",
        "        trues.append(y.detach().cpu())\n",
        "\n",
        "    return torch.cat(preds, dim=0), torch.cat(trues, dim=0)  # [N, horizon], [N, horizon]\n",
        "\n",
        "def _inverse_scale_2d(arr_2d, scaler):\n",
        "    \"\"\"\n",
        "    arr_2d: numpy array [N, H] in scaler space.\n",
        "    scaler: fitted MinMaxScaler on price column.\n",
        "    \"\"\"\n",
        "    flat = arr_2d.reshape(-1, 1)\n",
        "    inv = scaler.inverse_transform(flat)\n",
        "    return inv.reshape(arr_2d.shape)\n",
        "\n",
        "def predict_train_test(\n",
        "    model,\n",
        "    trainset,\n",
        "    testset,\n",
        "    batch_size=256,\n",
        "    device=None,\n",
        "    scaler=None,          # pass if you want inverse-scaled outputs\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dict with predictions and targets for train and test.\n",
        "    Shapes are [N, horizon] for both preds and targets.\n",
        "    If `scaler` is provided, adds *_inv with inverse-scaled prices.\n",
        "    \"\"\"\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n",
        "    test_loader  = DataLoader(testset,  batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n",
        "\n",
        "    train_pred, train_true = _predict_on_loader(model, train_loader, device)\n",
        "    test_pred,  test_true  = _predict_on_loader(model, test_loader,  device)\n",
        "\n",
        "    out = {\n",
        "        \"train_pred\": train_pred.numpy(),   # scaled\n",
        "        \"train_true\": train_true.numpy(),\n",
        "        \"test_pred\":  test_pred.numpy(),\n",
        "        \"test_true\":  test_true.numpy(),\n",
        "    }\n",
        "\n",
        "    if scaler is not None:\n",
        "        out[\"train_pred_inv\"] = _inverse_scale_2d(out[\"train_pred\"], scaler)\n",
        "        out[\"train_true_inv\"] = _inverse_scale_2d(out[\"train_true\"], scaler)\n",
        "        out[\"test_pred_inv\"]  = _inverse_scale_2d(out[\"test_pred\"],  scaler)\n",
        "        out[\"test_true_inv\"]  = _inverse_scale_2d(out[\"test_true\"],  scaler)\n",
        "\n",
        "\n",
        "    return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rdnp5LgEIC2J"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_predictions_plotly(out_dict, horizon_step=0, title=\"Prediction vs Actual (Train/Test)\", name=''):\n",
        "    \"\"\"\n",
        "    out_dict: dictionary with keys:\n",
        "      train_pred_inv, train_true_inv, test_pred_inv, test_true_inv\n",
        "      Each should be numpy arrays [N, H] or [N] if single step\n",
        "    horizon_step: which forecast step to plot (0 = next step)\n",
        "    \"\"\"\n",
        "\n",
        "    # pick correct horizon step\n",
        "    def select_step(arr):\n",
        "        if arr.ndim == 2:\n",
        "            return arr[:, horizon_step]\n",
        "        return arr\n",
        "\n",
        "    train_pred = select_step(out_dict[\"train_pred_inv\"])\n",
        "    train_true = select_step(out_dict[\"train_true_inv\"])\n",
        "    test_pred  = select_step(out_dict[\"test_pred_inv\"])\n",
        "    test_true  = select_step(out_dict[\"test_true_inv\"])\n",
        "\n",
        "    n_train = len(train_pred)\n",
        "    n_test  = len(test_pred)\n",
        "\n",
        "    # x axes\n",
        "    x_train = list(range(n_train))\n",
        "    x_test  = list(range(n_train, n_train + n_test))\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Train actual\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_train, y=train_true,\n",
        "        mode='lines', name='Train Actual',\n",
        "        line=dict(color='blue')\n",
        "    ))\n",
        "\n",
        "    # Train prediction\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_train, y=train_pred,\n",
        "        mode='lines', name='Train Pred',\n",
        "        line=dict(color='blue', dash='dash')\n",
        "    ))\n",
        "\n",
        "    # Test actual\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_test, y=test_true,\n",
        "        mode='lines', name='Test Actual',\n",
        "        line=dict(color='orange')\n",
        "    ))\n",
        "\n",
        "    # Test prediction\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_test, y=test_pred,\n",
        "        mode='lines', name='Test Pred',\n",
        "        line=dict(color='orange', dash='dash')\n",
        "    ))\n",
        "\n",
        "    # vertical line for train/test split\n",
        "    fig.add_vline(\n",
        "        x=n_train - 0.5,\n",
        "        line_width=2, line_dash=\"dot\", line_color=\"black\"\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"Sample Index (time ordered)\",\n",
        "        yaxis_title=\"Price\",\n",
        "        legend=dict(x=0, y=1),\n",
        "        hovermode=\"x unified\",\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "    fig.write_html(f\"{name}.html\")\n",
        "    fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "L9OWAs-oL0kk"
      },
      "outputs": [],
      "source": [
        "def plot_test_predictions_plotly(\n",
        "    results,\n",
        "    horizon_step: int = 0,\n",
        "    title: str = \"Prediction vs Actual (Test Only)\",\n",
        "    name=''\n",
        "):\n",
        "    \"\"\"\n",
        "    results: dict with structure\n",
        "        {\n",
        "            \"test_true_inv\": np.ndarray [N] or [N, H],\n",
        "            \"preds\": {\n",
        "                \"LSTM\": np.ndarray [N] or [N, H],\n",
        "                \"GRU\":  np.ndarray [N] or [N, H],\n",
        "                \"Transformer\": np.ndarray [N] or [N, H],\n",
        "                ...\n",
        "            }\n",
        "        }\n",
        "    horizon_step: which forecast step to plot (0 = next step)\n",
        "    \"\"\"\n",
        "\n",
        "    def select_step(arr):\n",
        "        # Accept shape [N] or [N, H]\n",
        "        if arr.ndim == 2:\n",
        "            if horizon_step < 0 or horizon_step >= arr.shape[1]:\n",
        "                raise ValueError(f\"horizon_step {horizon_step} out of range for array with shape {arr.shape}.\")\n",
        "            return arr[:, horizon_step]\n",
        "        return arr\n",
        "\n",
        "    # Extract and validate y_true\n",
        "    if \"test_true_inv\" not in results or \"preds\" not in results:\n",
        "        raise KeyError('Expected keys: \"test_true_inv\" and \"preds\" in results.')\n",
        "\n",
        "    y_true_full = np.asarray(results[\"test_true_inv\"])\n",
        "    y_true = select_step(y_true_full)\n",
        "    N = len(y_true)\n",
        "\n",
        "    # Prepare x axis (test indices only)\n",
        "    x = np.arange(N)\n",
        "\n",
        "    # Build figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Actual test series (solid line)\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x, y=y_true,\n",
        "        mode=\"lines\",\n",
        "        name=\"Test Actual\",\n",
        "        line=dict(color=\"black\")\n",
        "    ))\n",
        "\n",
        "    # Add one dashed line per model\n",
        "    for model_name, y_pred_full in results[\"preds\"].items():\n",
        "        y_pred = select_step(np.asarray(y_pred_full))\n",
        "\n",
        "        if len(y_pred) != N:\n",
        "            raise ValueError(\n",
        "                f'Length mismatch for \"{model_name}\": got {len(y_pred)} vs y_true length {N}.'\n",
        "            )\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=x, y=y_pred,\n",
        "            mode=\"lines\",\n",
        "            name=f\"{model_name} Pred\",\n",
        "            line=dict(dash=\"dash\")  # color auto-cycles\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"Test Sample Index (time ordered)\",\n",
        "        yaxis_title=\"Price\",\n",
        "        legend=dict(x=0, y=1),\n",
        "        hovermode=\"x unified\",\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "    fig.write_html(f\"{name}.html\")\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLAQbHGyHIQ"
      },
      "source": [
        "# Concat all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvb1VaCsyQw6"
      },
      "outputs": [],
      "source": [
        "augmentations = Compose([AddGaussianNoise()])#, RandomScaling()])\n",
        "\n",
        "trainset, train_loader, testset, test_loader = get_loaders(df_no_outliers, time_bin=60, scaler=scaler, lookback=12,\n",
        "                                        lookforward=1, batch_size=8, train_size=0.9,\n",
        "                                        transform=augmentations)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 200\n",
        "input_size=1\n",
        "d_model=128\n",
        "nhead=8\n",
        "num_encoder_layers=1\n",
        "dim_feedforward=64\n",
        "output_size=1\n",
        "dropout=0.1"
      ],
      "metadata": {
        "id": "QqKKnKl56CJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0yyg3gsySEq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(patience=50, mode='min', verbose=False, save_path='/content/checkpoint_transformer.pt')\n",
        "\n",
        "\n",
        "model = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size=output_size,\n",
        "    dropout=dropout)\n",
        "\n",
        "# model = GRUModel(1, 4, 1, 1)\n",
        "model.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, show=False)\n",
        "    stop_criteria = validate_one_epoch(model, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtHiKv5A-kfM"
      },
      "source": [
        "# Models Comparison"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "output_size = 1\n",
        "num_hidden_layers = 4\n",
        "num_stacked_layers = 1\n",
        "patience = 30\n",
        "num_epochs = 5000\n",
        "learning_rate = 0.0005"
      ],
      "metadata": {
        "id": "f4H-tZb0wdrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB3twgcd-nal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9825efef-fc2a-4615-a6bf-784152132f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.00408\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00356\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00457\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00413\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00511\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00425\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00434\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00435\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00428\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00394\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00377\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00420\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00399\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00361\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00384\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00402\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00446\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00362\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00392\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00370\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00381\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00345\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00348\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00349\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00405\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00460\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00372\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00379\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00351\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00439\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00394\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00423\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00371\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00383\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00309\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00280\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00354\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00406\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00357\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00334\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00344\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00410\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00389\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00322\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00410\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00353\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00364\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00384\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00347\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00453\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00362\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00349\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00371\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00416\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00428\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00409\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00327\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00381\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00358\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00332\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00445\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00369\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00322\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00401\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00395\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00319\n",
            "***************************************************\n",
            "\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "# train LSTM\n",
        "lstm_model = LSTM(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "\n",
        "lstm_model.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
        "early_stopping = EarlyStopping(patience=patience, mode='min', verbose=False, save_path='/content/checkpoint_lstm.pt')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(lstm_model, show=False)\n",
        "\n",
        "    stop_criteria = validate_one_epoch(lstm_model, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnpSfTEE_XHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7c882111-1c0b-466e-c457-c59f3138d223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "\n",
            "Val Loss: 1.00405\n",
            "***************************************************\n",
            "\n",
            "Epoch: 2\n",
            "\n",
            "Val Loss: 0.75313\n",
            "***************************************************\n",
            "\n",
            "Epoch: 3\n",
            "\n",
            "Val Loss: 0.53319\n",
            "***************************************************\n",
            "\n",
            "Epoch: 4\n",
            "\n",
            "Val Loss: 0.36551\n",
            "***************************************************\n",
            "\n",
            "Epoch: 5\n",
            "\n",
            "Val Loss: 0.25250\n",
            "***************************************************\n",
            "\n",
            "Epoch: 6\n",
            "\n",
            "Val Loss: 0.18785\n",
            "***************************************************\n",
            "\n",
            "Epoch: 7\n",
            "\n",
            "Val Loss: 0.15519\n",
            "***************************************************\n",
            "\n",
            "Epoch: 8\n",
            "\n",
            "Val Loss: 0.13744\n",
            "***************************************************\n",
            "\n",
            "Epoch: 9\n",
            "\n",
            "Val Loss: 0.12689\n",
            "***************************************************\n",
            "\n",
            "Epoch: 10\n",
            "\n",
            "Val Loss: 0.11706\n",
            "***************************************************\n",
            "\n",
            "Epoch: 11\n",
            "\n",
            "Val Loss: 0.11144\n",
            "***************************************************\n",
            "\n",
            "Epoch: 12\n",
            "\n",
            "Val Loss: 0.10242\n",
            "***************************************************\n",
            "\n",
            "Epoch: 13\n",
            "\n",
            "Val Loss: 0.09784\n",
            "***************************************************\n",
            "\n",
            "Epoch: 14\n",
            "\n",
            "Val Loss: 0.09155\n",
            "***************************************************\n",
            "\n",
            "Epoch: 15\n",
            "\n",
            "Val Loss: 0.08592\n",
            "***************************************************\n",
            "\n",
            "Epoch: 16\n",
            "\n",
            "Val Loss: 0.08139\n",
            "***************************************************\n",
            "\n",
            "Epoch: 17\n",
            "\n",
            "Val Loss: 0.07861\n",
            "***************************************************\n",
            "\n",
            "Epoch: 18\n",
            "\n",
            "Val Loss: 0.07440\n",
            "***************************************************\n",
            "\n",
            "Epoch: 19\n",
            "\n",
            "Val Loss: 0.07065\n",
            "***************************************************\n",
            "\n",
            "Epoch: 20\n",
            "\n",
            "Val Loss: 0.06725\n",
            "***************************************************\n",
            "\n",
            "Epoch: 21\n",
            "\n",
            "Val Loss: 0.06436\n",
            "***************************************************\n",
            "\n",
            "Epoch: 22\n",
            "\n",
            "Val Loss: 0.06234\n",
            "***************************************************\n",
            "\n",
            "Epoch: 23\n",
            "\n",
            "Val Loss: 0.05935\n",
            "***************************************************\n",
            "\n",
            "Epoch: 24\n",
            "\n",
            "Val Loss: 0.05583\n",
            "***************************************************\n",
            "\n",
            "Epoch: 25\n",
            "\n",
            "Val Loss: 0.05475\n",
            "***************************************************\n",
            "\n",
            "Epoch: 26\n",
            "\n",
            "Val Loss: 0.05185\n",
            "***************************************************\n",
            "\n",
            "Epoch: 27\n",
            "\n",
            "Val Loss: 0.04995\n",
            "***************************************************\n",
            "\n",
            "Epoch: 28\n",
            "\n",
            "Val Loss: 0.04824\n",
            "***************************************************\n",
            "\n",
            "Epoch: 29\n",
            "\n",
            "Val Loss: 0.04749\n",
            "***************************************************\n",
            "\n",
            "Epoch: 30\n",
            "\n",
            "Val Loss: 0.04555\n",
            "***************************************************\n",
            "\n",
            "Epoch: 31\n",
            "\n",
            "Val Loss: 0.04439\n",
            "***************************************************\n",
            "\n",
            "Epoch: 32\n",
            "\n",
            "Val Loss: 0.04177\n",
            "***************************************************\n",
            "\n",
            "Epoch: 33\n",
            "\n",
            "Val Loss: 0.04130\n",
            "***************************************************\n",
            "\n",
            "Epoch: 34\n",
            "\n",
            "Val Loss: 0.03887\n",
            "***************************************************\n",
            "\n",
            "Epoch: 35\n",
            "\n",
            "Val Loss: 0.03850\n",
            "***************************************************\n",
            "\n",
            "Epoch: 36\n",
            "\n",
            "Val Loss: 0.03617\n",
            "***************************************************\n",
            "\n",
            "Epoch: 37\n",
            "\n",
            "Val Loss: 0.03563\n",
            "***************************************************\n",
            "\n",
            "Epoch: 38\n",
            "\n",
            "Val Loss: 0.03456\n",
            "***************************************************\n",
            "\n",
            "Epoch: 39\n",
            "\n",
            "Val Loss: 0.03204\n",
            "***************************************************\n",
            "\n",
            "Epoch: 40\n",
            "\n",
            "Val Loss: 0.03252\n",
            "***************************************************\n",
            "\n",
            "Epoch: 41\n",
            "\n",
            "Val Loss: 0.03209\n",
            "***************************************************\n",
            "\n",
            "Epoch: 42\n",
            "\n",
            "Val Loss: 0.03005\n",
            "***************************************************\n",
            "\n",
            "Epoch: 43\n",
            "\n",
            "Val Loss: 0.02869\n",
            "***************************************************\n",
            "\n",
            "Epoch: 44\n",
            "\n",
            "Val Loss: 0.02758\n",
            "***************************************************\n",
            "\n",
            "Epoch: 45\n",
            "\n",
            "Val Loss: 0.02831\n",
            "***************************************************\n",
            "\n",
            "Epoch: 46\n",
            "\n",
            "Val Loss: 0.02732\n",
            "***************************************************\n",
            "\n",
            "Epoch: 47\n",
            "\n",
            "Val Loss: 0.02539\n",
            "***************************************************\n",
            "\n",
            "Epoch: 48\n",
            "\n",
            "Val Loss: 0.02554\n",
            "***************************************************\n",
            "\n",
            "Epoch: 49\n",
            "\n",
            "Val Loss: 0.02469\n",
            "***************************************************\n",
            "\n",
            "Epoch: 50\n",
            "\n",
            "Val Loss: 0.02377\n",
            "***************************************************\n",
            "\n",
            "Epoch: 51\n",
            "\n",
            "Val Loss: 0.02224\n",
            "***************************************************\n",
            "\n",
            "Epoch: 52\n",
            "\n",
            "Val Loss: 0.02250\n",
            "***************************************************\n",
            "\n",
            "Epoch: 53\n",
            "\n",
            "Val Loss: 0.02124\n",
            "***************************************************\n",
            "\n",
            "Epoch: 54\n",
            "\n",
            "Val Loss: 0.02160\n",
            "***************************************************\n",
            "\n",
            "Epoch: 55\n",
            "\n",
            "Val Loss: 0.02212\n",
            "***************************************************\n",
            "\n",
            "Epoch: 56\n",
            "\n",
            "Val Loss: 0.02148\n",
            "***************************************************\n",
            "\n",
            "Epoch: 57\n",
            "\n",
            "Val Loss: 0.02022\n",
            "***************************************************\n",
            "\n",
            "Epoch: 58\n",
            "\n",
            "Val Loss: 0.01856\n",
            "***************************************************\n",
            "\n",
            "Epoch: 59\n",
            "\n",
            "Val Loss: 0.01864\n",
            "***************************************************\n",
            "\n",
            "Epoch: 60\n",
            "\n",
            "Val Loss: 0.01712\n",
            "***************************************************\n",
            "\n",
            "Epoch: 61\n",
            "\n",
            "Val Loss: 0.01836\n",
            "***************************************************\n",
            "\n",
            "Epoch: 62\n",
            "\n",
            "Val Loss: 0.01885\n",
            "***************************************************\n",
            "\n",
            "Epoch: 63\n",
            "\n",
            "Val Loss: 0.01704\n",
            "***************************************************\n",
            "\n",
            "Epoch: 64\n",
            "\n",
            "Val Loss: 0.01662\n",
            "***************************************************\n",
            "\n",
            "Epoch: 65\n",
            "\n",
            "Val Loss: 0.01530\n",
            "***************************************************\n",
            "\n",
            "Epoch: 66\n",
            "\n",
            "Val Loss: 0.01724\n",
            "***************************************************\n",
            "\n",
            "Epoch: 67\n",
            "\n",
            "Val Loss: 0.01598\n",
            "***************************************************\n",
            "\n",
            "Epoch: 68\n",
            "\n",
            "Val Loss: 0.01614\n",
            "***************************************************\n",
            "\n",
            "Epoch: 69\n",
            "\n",
            "Val Loss: 0.01422\n",
            "***************************************************\n",
            "\n",
            "Epoch: 70\n",
            "\n",
            "Val Loss: 0.01464\n",
            "***************************************************\n",
            "\n",
            "Epoch: 71\n",
            "\n",
            "Val Loss: 0.01478\n",
            "***************************************************\n",
            "\n",
            "Epoch: 72\n",
            "\n",
            "Val Loss: 0.01297\n",
            "***************************************************\n",
            "\n",
            "Epoch: 73\n",
            "\n",
            "Val Loss: 0.01425\n",
            "***************************************************\n",
            "\n",
            "Epoch: 74\n",
            "\n",
            "Val Loss: 0.01291\n",
            "***************************************************\n",
            "\n",
            "Epoch: 75\n",
            "\n",
            "Val Loss: 0.01251\n",
            "***************************************************\n",
            "\n",
            "Epoch: 76\n",
            "\n",
            "Val Loss: 0.01274\n",
            "***************************************************\n",
            "\n",
            "Epoch: 77\n",
            "\n",
            "Val Loss: 0.01306\n",
            "***************************************************\n",
            "\n",
            "Epoch: 78\n",
            "\n",
            "Val Loss: 0.01201\n",
            "***************************************************\n",
            "\n",
            "Epoch: 79\n",
            "\n",
            "Val Loss: 0.01211\n",
            "***************************************************\n",
            "\n",
            "Epoch: 80\n",
            "\n",
            "Val Loss: 0.01110\n",
            "***************************************************\n",
            "\n",
            "Epoch: 81\n",
            "\n",
            "Val Loss: 0.01057\n",
            "***************************************************\n",
            "\n",
            "Epoch: 82\n",
            "\n",
            "Val Loss: 0.01164\n",
            "***************************************************\n",
            "\n",
            "Epoch: 83\n",
            "\n",
            "Val Loss: 0.00967\n",
            "***************************************************\n",
            "\n",
            "Epoch: 84\n",
            "\n",
            "Val Loss: 0.01056\n",
            "***************************************************\n",
            "\n",
            "Epoch: 85\n",
            "\n",
            "Val Loss: 0.01081\n",
            "***************************************************\n",
            "\n",
            "Epoch: 86\n",
            "\n",
            "Val Loss: 0.01080\n",
            "***************************************************\n",
            "\n",
            "Epoch: 87\n",
            "\n",
            "Val Loss: 0.01029\n",
            "***************************************************\n",
            "\n",
            "Epoch: 88\n",
            "\n",
            "Val Loss: 0.01114\n",
            "***************************************************\n",
            "\n",
            "Epoch: 89\n",
            "\n",
            "Val Loss: 0.00937\n",
            "***************************************************\n",
            "\n",
            "Epoch: 90\n",
            "\n",
            "Val Loss: 0.01008\n",
            "***************************************************\n",
            "\n",
            "Epoch: 91\n",
            "\n",
            "Val Loss: 0.00876\n",
            "***************************************************\n",
            "\n",
            "Epoch: 92\n",
            "\n",
            "Val Loss: 0.00827\n",
            "***************************************************\n",
            "\n",
            "Epoch: 93\n",
            "\n",
            "Val Loss: 0.00888\n",
            "***************************************************\n",
            "\n",
            "Epoch: 94\n",
            "\n",
            "Val Loss: 0.01093\n",
            "***************************************************\n",
            "\n",
            "Epoch: 95\n",
            "\n",
            "Val Loss: 0.00815\n",
            "***************************************************\n",
            "\n",
            "Epoch: 96\n",
            "\n",
            "Val Loss: 0.00887\n",
            "***************************************************\n",
            "\n",
            "Epoch: 97\n",
            "\n",
            "Val Loss: 0.00744\n",
            "***************************************************\n",
            "\n",
            "Epoch: 98\n",
            "\n",
            "Val Loss: 0.00843\n",
            "***************************************************\n",
            "\n",
            "Epoch: 99\n",
            "\n",
            "Val Loss: 0.00787\n",
            "***************************************************\n",
            "\n",
            "Epoch: 100\n",
            "\n",
            "Val Loss: 0.00868\n",
            "***************************************************\n",
            "\n",
            "Epoch: 101\n",
            "\n",
            "Val Loss: 0.00884\n",
            "***************************************************\n",
            "\n",
            "Epoch: 102\n",
            "\n",
            "Val Loss: 0.00743\n",
            "***************************************************\n",
            "\n",
            "Epoch: 103\n",
            "\n",
            "Val Loss: 0.00799\n",
            "***************************************************\n",
            "\n",
            "Epoch: 104\n",
            "\n",
            "Val Loss: 0.00768\n",
            "***************************************************\n",
            "\n",
            "Epoch: 105\n",
            "\n",
            "Val Loss: 0.00820\n",
            "***************************************************\n",
            "\n",
            "Epoch: 106\n",
            "\n",
            "Val Loss: 0.00820\n",
            "***************************************************\n",
            "\n",
            "Epoch: 107\n",
            "\n",
            "Val Loss: 0.00837\n",
            "***************************************************\n",
            "\n",
            "Epoch: 108\n",
            "\n",
            "Val Loss: 0.00803\n",
            "***************************************************\n",
            "\n",
            "Epoch: 109\n",
            "\n",
            "Val Loss: 0.00621\n",
            "***************************************************\n",
            "\n",
            "Epoch: 110\n",
            "\n",
            "Val Loss: 0.00705\n",
            "***************************************************\n",
            "\n",
            "Epoch: 111\n",
            "\n",
            "Val Loss: 0.00835\n",
            "***************************************************\n",
            "\n",
            "Epoch: 112\n",
            "\n",
            "Val Loss: 0.00686\n",
            "***************************************************\n",
            "\n",
            "Epoch: 113\n",
            "\n",
            "Val Loss: 0.00688\n",
            "***************************************************\n",
            "\n",
            "Epoch: 114\n",
            "\n",
            "Val Loss: 0.00807\n",
            "***************************************************\n",
            "\n",
            "Epoch: 115\n",
            "\n",
            "Val Loss: 0.00583\n",
            "***************************************************\n",
            "\n",
            "Epoch: 116\n",
            "\n",
            "Val Loss: 0.00672\n",
            "***************************************************\n",
            "\n",
            "Epoch: 117\n",
            "\n",
            "Val Loss: 0.00636\n",
            "***************************************************\n",
            "\n",
            "Epoch: 118\n",
            "\n",
            "Val Loss: 0.00627\n",
            "***************************************************\n",
            "\n",
            "Epoch: 119\n",
            "\n",
            "Val Loss: 0.00766\n",
            "***************************************************\n",
            "\n",
            "Epoch: 120\n",
            "\n",
            "Val Loss: 0.00650\n",
            "***************************************************\n",
            "\n",
            "Epoch: 121\n",
            "\n",
            "Val Loss: 0.00683\n",
            "***************************************************\n",
            "\n",
            "Epoch: 122\n",
            "\n",
            "Val Loss: 0.00678\n",
            "***************************************************\n",
            "\n",
            "Epoch: 123\n",
            "\n",
            "Val Loss: 0.00658\n",
            "***************************************************\n",
            "\n",
            "Epoch: 124\n",
            "\n",
            "Val Loss: 0.00677\n",
            "***************************************************\n",
            "\n",
            "Epoch: 125\n",
            "\n",
            "Val Loss: 0.00649\n",
            "***************************************************\n",
            "\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "gru_model = GRUModel(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "gru_model.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(gru_model.parameters(), lr=learning_rate)\n",
        "early_stopping = EarlyStopping(patience=10, mode='min', verbose=False, save_path='/content/checkpoint_gru.pt')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(gru_model)\n",
        "    stop_criteria = validate_one_epoch(gru_model, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ik57HvnA52C"
      },
      "outputs": [],
      "source": [
        "lstm_model = LSTM(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "lstm_model.load_state_dict(torch.load('/content/checkpoint_lstm.pt'))\n",
        "\n",
        "result = predict_train_test(lstm_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "lstm_metrics = evaluate_forecast(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-X2adOZAltq"
      },
      "outputs": [],
      "source": [
        "gru_model = GRUModel(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "gru_model.load_state_dict(torch.load('/content/checkpoint_gru.pt'))\n",
        "result = predict_train_test(gru_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "gru_metrics = evaluate_forecast(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size=output_size,\n",
        "    dropout=dropout)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/checkpoint_transformer.pt'))\n",
        "result = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "transformer_metrics = evaluate_forecast(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsPk1D-E2lvT",
        "outputId": "e1758466-d580-4c0e-817f-74fd06165b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnpl-CmQHqeK"
      },
      "outputs": [],
      "source": [
        "def get_params_flops(model, input_size):\n",
        "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    with torch.cuda.device(0):\n",
        "        macs, _ = get_model_complexity_info(model, input_size, as_strings=False, print_per_layer_stat=False)\n",
        "    flops = macs * 2  # MACs to FLOPs\n",
        "    return params, flops\n",
        "\n",
        "def human_readable(num):\n",
        "    for unit in [\"\", \"K\", \"M\", \"B\"]:\n",
        "        if abs(num) < 1000:\n",
        "            return f\"{num:.2f} {unit}\"\n",
        "        num /= 1000\n",
        "    return f\"{num:.2f} T\"  # trillion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36r5KiGNHw3F"
      },
      "outputs": [],
      "source": [
        "model_info = {}\n",
        "for name, model in {\n",
        "    \"LSTM\": lstm_model,\n",
        "    \"GRU\": gru_model,\n",
        "    \"Transformer\": model\n",
        "}.items():\n",
        "    params, flops = get_params_flops(model, (30, 1))  # (seq_len, input_size)\n",
        "    model_info[name] = {\"Params\": human_readable(params), \"FLOPs\": human_readable(flops)}\n",
        "\n",
        "\n",
        "df_info = pd.DataFrame(model_info).T\n",
        "df_info = df_info.reset_index().rename(columns={'index':'Model'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "GRas0whxAm0f",
        "outputId": "42b60aed-f9fc-4041-eb1a-a4bee5de9a58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           MAE       RMSE      MAPE     sMAPE        R2  \\\n",
              "Model       Dataset                                                       \n",
              "LSTM        train    27.989830  38.989189  0.060203  0.060190  0.990512   \n",
              "            test     45.204918  59.094742  0.094563  0.094631  0.904518   \n",
              "GRU         train    26.490988  38.961105  0.056988  0.056981  0.990523   \n",
              "            test     66.061127  83.549095  0.138142  0.138289  0.809143   \n",
              "Transformer train    25.080482  38.251038  0.053899  0.053898  0.990865   \n",
              "            test     41.446842  51.369881  0.086780  0.086812  0.927849   \n",
              "\n",
              "                         MASE    Params   FLOPs  \n",
              "Model       Dataset                              \n",
              "LSTM        train    1.227784   117.00   9.13 K  \n",
              "            test     1.982931   117.00   9.13 K  \n",
              "GRU         train    1.161850    89.00   6.73 K  \n",
              "            test     2.897329    89.00   6.73 K  \n",
              "Transformer train    1.099853  100.29 K  5.51 M  \n",
              "            test     1.817566  100.29 K  5.51 M  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cdcd1f68-9ffe-4d87-b331-3cc4e9503e9a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>sMAPE</th>\n",
              "      <th>R2</th>\n",
              "      <th>MASE</th>\n",
              "      <th>Params</th>\n",
              "      <th>FLOPs</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">LSTM</th>\n",
              "      <th>train</th>\n",
              "      <td>27.989830</td>\n",
              "      <td>38.989189</td>\n",
              "      <td>0.060203</td>\n",
              "      <td>0.060190</td>\n",
              "      <td>0.990512</td>\n",
              "      <td>1.227784</td>\n",
              "      <td>117.00</td>\n",
              "      <td>9.13 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>45.204918</td>\n",
              "      <td>59.094742</td>\n",
              "      <td>0.094563</td>\n",
              "      <td>0.094631</td>\n",
              "      <td>0.904518</td>\n",
              "      <td>1.982931</td>\n",
              "      <td>117.00</td>\n",
              "      <td>9.13 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">GRU</th>\n",
              "      <th>train</th>\n",
              "      <td>26.490988</td>\n",
              "      <td>38.961105</td>\n",
              "      <td>0.056988</td>\n",
              "      <td>0.056981</td>\n",
              "      <td>0.990523</td>\n",
              "      <td>1.161850</td>\n",
              "      <td>89.00</td>\n",
              "      <td>6.73 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>66.061127</td>\n",
              "      <td>83.549095</td>\n",
              "      <td>0.138142</td>\n",
              "      <td>0.138289</td>\n",
              "      <td>0.809143</td>\n",
              "      <td>2.897329</td>\n",
              "      <td>89.00</td>\n",
              "      <td>6.73 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Transformer</th>\n",
              "      <th>train</th>\n",
              "      <td>25.080482</td>\n",
              "      <td>38.251038</td>\n",
              "      <td>0.053899</td>\n",
              "      <td>0.053898</td>\n",
              "      <td>0.990865</td>\n",
              "      <td>1.099853</td>\n",
              "      <td>100.29 K</td>\n",
              "      <td>5.51 M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>41.446842</td>\n",
              "      <td>51.369881</td>\n",
              "      <td>0.086780</td>\n",
              "      <td>0.086812</td>\n",
              "      <td>0.927849</td>\n",
              "      <td>1.817566</td>\n",
              "      <td>100.29 K</td>\n",
              "      <td>5.51 M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdcd1f68-9ffe-4d87-b331-3cc4e9503e9a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cdcd1f68-9ffe-4d87-b331-3cc4e9503e9a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cdcd1f68-9ffe-4d87-b331-3cc4e9503e9a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5c1c6f13-c46b-4297-bcc4-c10f012726b3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c1c6f13-c46b-4297-bcc4-c10f012726b3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5c1c6f13-c46b-4297-bcc4-c10f012726b3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"comparison\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          27.989830017089844,\n          45.204917907714844,\n          41.446842193603516\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          38.98918914794922,\n          59.09474182128906,\n          51.36988067626953\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.06020345538854599,\n          0.09456338733434677,\n          0.08678030222654343\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sMAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.06019008159637451,\n          0.09463087469339371,\n          0.08681194484233856\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.9905117154121399,\n          0.9045180082321167,\n          0.9278492331504822\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MASE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1.2277843952178955,\n          1.9829306602478027,\n          1.8175660371780396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Params\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"117.00 \",\n          \"89.00 \",\n          \"100.29 K\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FLOPs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"9.13 K\",\n          \"6.73 K\",\n          \"5.51 M\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "results = {\n",
        "    \"LSTM\": lstm_metrics,\n",
        "    \"GRU\": gru_metrics,\n",
        "    \"Transformer\": transformer_metrics\n",
        "}\n",
        "\n",
        "\n",
        "comparison = pd.concat({model: pd.DataFrame(metrics).T for model, metrics in results.items()})\n",
        "\n",
        "comparison = comparison.reset_index().rename(columns={'level_0':'Model', 'level_1':'Dataset'})\n",
        "\n",
        "comparison = comparison.merge(df_info, on=\"Model\")\n",
        "\n",
        "comparison.set_index(['Model',\t'Dataset'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = BaselineModel(sample_mean=6)\n",
        "result = predict_train_test(baseline_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "\n",
        "plot_predictions_plotly(result, horizon_step=0, title=\"Gold Price Forecast\", name='baseline')\n",
        "\n",
        "\n",
        "transformer_metrics = evaluate_forecast(result)\n",
        "transformer_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "WBOdtFHOIK1Y",
        "outputId": "dec6758c-e4f0-499c-bd8b-7cc5c82b9256"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"ac456f54-9746-4413-a099-668d3e040f10\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ac456f54-9746-4413-a099-668d3e040f10\")) {                    Plotly.newPlot(                        \"ac456f54-9746-4413-a099-668d3e040f10\",                        [{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Train Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[45983.387,45961.56,46025.863,45970.223,45889.22,45870.547,45899.93,45971.887,45985.32,45970.63,45954.99,45929.92,45937.12,45936.38,45946.434,45934.723,45938.44,45933.19,45918.84,45907.914,45921.664,45930.266,45951.223,46007.312,45957.062,45827.67,45880.203,45895.25,45964.0,46004.723,46001.906,45965.492,45903.586,45949.875,45979.586,45979.645,45980.344,45979.42,45976.793,45979.246,45978.863,45979.344,45980.242,45969.227,45985.574,45989.86,45929.8,45798.516,45795.535,45872.484,45842.633,45816.53,45858.72,45879.06,45882.06,45877.992,45993.227,46075.383,46069.086,46103.105,46099.35,46100.035,46098.88,46098.47,46100.32,46100.223,46100.98,46100.453,46154.98,46194.133,46214.945,46242.12,46335.566,46375.76,46324.492,46359.113,46413.92,46437.465,46309.543,46304.215,46309.51,46249.383,46280.64,46299.715,46329.89,46330.83,46330.21,46330.285,46331.293,46330.48,46330.047,46329.98,46329.71,46327.06,46426.285,46461.227,46314.152,46261.51,46287.3,46277.797,46274.543,46213.223,46283.707,46353.49,46373.254,46362.477,46438.895,46367.28,46359.816,46359.207,46359.266,46358.703,46359.863,46360.15,46359.84,46360.695,46359.84,46235.555,46177.42,46150.39,46116.85,45975.293,45895.57,45873.938,45866.73,45833.887,45937.895,45941.086,45991.145,46058.508,46109.586,46084.35,46049.867,46051.645,46049.58,46051.492,46048.03,46050.184,46051.066,46051.293,45942.95,45985.367,46024.74,46130.914,46216.277,46278.105,46221.816,46164.65,46186.727,46165.09,46241.293,46228.684,46223.902,46226.297,46228.176,46202.6,46198.664,46210.406,46216.715,46220.34,46225.703,46224.75,46219.74,46254.695,46272.29,46353.24,46470.81,46422.49,46424.465,46434.402,46446.44,46444.613,46377.734,46357.016,46385.74,46428.215,46470.9,46459.055,46457.887,46460.18,46466.785,46464.617,46461.344,46461.793,46465.742,46460.453,46459.84,46459.78,46464.73,46464.938,46472.074,46490.098,46481.773,46434.145,46439.984,46417.125,46372.074,46321.496,46294.53,46284.277,46232.21,46232.992,46237.164,46290.176,46285.445,46279.95,46276.445,46259.05,46241.49,46234.13,46231.57,46223.777,46229.797,46250.52,46353.316,46286.03,46254.7,46343.477,46390.676,46369.32,46380.83,46307.79,46315.355,46335.96,46350.668,46331.066,46329.695,46338.22,46339.887,46341.89,46344.098,46339.71,46340.56,46340.89,46339.184,46341.402,46341.934,46343.324,46507.668,46606.207,46671.176,46690.367,46664.574,46720.504,46746.79,46777.914,46769.184,46825.44,46868.168,46837.816,46805.61,46832.227,46839.87,46841.0,46833.17,46829.844,46835.875,46830.77,46843.16,46847.28,46849.996,46838.562,46828.52,46812.117,46726.027,46717.875,46684.586,46679.938,46628.934,46648.07,46640.348,46644.85,46619.95,46618.58,46615.76,46619.13,46630.348,46625.21,46626.066,46629.008,46631.906,46629.754,46637.117,46624.76,46622.742,46614.613,46643.273,46651.65,46636.145,46590.582,46576.996,46506.723,46521.77,46492.883,46494.74,46527.504,46516.785,46510.23,46511.668,46508.918,46510.348,46509.516,46511.86,46518.074,46516.645,46515.316,46518.496,46513.52,46509.32,46501.0,46497.12,46415.65,46377.01,46389.203,46407.62,46401.664,46375.96,46382.26,46388.184,46401.523,46421.64,46416.207,46416.688,46400.785,46400.336,46400.582,46400.883,46403.87,46399.11,46399.523,46399.855,46396.79,46401.4,46413.547,46464.992,46504.19,46536.883,46507.734,46520.28,46534.176,46528.324,46492.17,46470.92,46464.406,46458.547,46463.863,46458.68,46460.4,46462.875,46463.633,46459.516,46459.95,46463.51,46463.156,46459.848,46459.98,46462.8,46461.527,46461.816,46463.332,46466.32,46458.527,46451.465,46429.96,46409.266,46369.902,46379.715,46307.74,46303.418,46299.51,46292.586,46368.18,46402.516,46390.92,46380.203,46380.61,46379.734,46368.832,46367.688,46361.03,46333.227,46328.844,46271.867,46409.03,46290.188,46246.54,46225.203,46175.305,46147.29,46172.473,46112.133,46093.89,46132.164,46115.33,46106.332,46133.15,46188.152,46182.86,46166.773,46159.566,46160.168,46158.465,46160.29,46152.664,46143.74,46199.99,46249.03,46197.09,46148.36,46159.49,46163.668,46196.32,46182.1,46168.445,46180.51,46207.797,46209.812,46225.637,46222.34,46216.598,46210.574,46210.117,46209.32,46209.64,46209.316,46210.59,46210.2,46208.7,46202.773,46198.414,46207.766,46164.09,46264.484,46396.16,46398.54,46418.1,46421.293,46379.285,46363.965,46375.406,46355.645,46327.273,46307.09,46354.293,46353.402,46355.668,46346.773,46348.082,46345.477,46342.383,46339.74,46340.848,46321.414,46329.402,46390.82,46373.617,46323.887,46462.09,46494.22,46505.195,46505.37,46449.31,46505.727,46588.023,46589.273,46558.695,46559.598,46573.715,46579.77,46575.75,46572.66,46572.363,46571.258,46568.555,46570.133,46577.477,46620.83,46660.977,46669.242,46700.188,46710.234,46720.67,46717.934,46651.914,46614.273,46621.9,46639.195,46674.543,46634.21,46610.45,46615.676,46638.55,46645.492,46636.13,46640.17,46639.918,46640.504,46640.19,46632.754,46630.156,46632.29,46622.973,46618.027,46530.086,46542.668,46561.87,46592.785,46600.13,46622.516,46656.445,46675.336,46691.105,46705.336,46682.27,46678.82,46670.36,46667.656,46670.887,46681.516,46682.74,46685.04,46698.086,46691.24,46693.51,46697.832,46703.652,46733.83,46782.26,46791.895,46738.418,46768.08,46784.55,46830.816,46828.79,46937.77,47000.918,46985.1,47111.63,47146.516,47183.773,47201.625,47170.223,47162.15,47147.855,47139.082,47140.098,47148.797,47142.273,47156.07,47173.902,47151.855,47143.027,47273.5,47301.406,47344.656,47306.777,47249.004,47251.348,47198.223,47150.633,47168.61,47142.17,47130.125,47133.926,47131.31,47129.562,47130.5,47130.234,47129.707,47129.3,47130.184,47121.74,47115.08,47115.516,47136.312,47027.58,46990.14,46938.418,46955.86,46984.785,46965.7,46996.86,47002.273,47018.555,47009.77,47008.082,47006.316,47009.113,47009.984,47021.332,47029.56,47029.715,47030.71,47034.44,47036.996,47038.1,47038.656,47023.816,46931.68,46985.984,47086.008,47107.91,47100.99,47082.023,47081.484,47087.527,47103.344,47183.113,47177.066,47178.516,47172.06,47188.08,47189.555,47189.383,47185.17,47187.965,47184.188,47182.438,47180.293,47181.656,47182.55,47187.68,47216.69,47268.66,47248.934,47317.484,47340.43,47298.613,47439.605,47539.69,47472.254,47435.062,47395.848,47411.637,47403.46,47415.996,47419.7,47420.953,47420.223,47419.48,47420.285,47412.53,47410.156,47410.184,47401.312,47441.29,47494.19,47470.633,47396.434,47422.504,47415.234,47411.5,47398.02,47430.562,47414.723,47411.03,47416.92,47453.387],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Train Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[45921.84,45940.504,45955.855,45982.46,45990.688,45979.24,45950.61,45936.246,45938.02,45930.54,45931.6,45941.598,45952.777,45957.914,45952.133,45946.297,45940.387,45937.03,45936.89,45934.418,45929.875,45926.027,45924.41,45926.715,45939.77,45946.137,45932.6,45926.082,45919.24,45921.844,45921.41,45929.016,45952.47,45955.87,45965.613,45967.383,45964.133,45960.266,45962.625,45974.336,45980.26,45979.65,45979.574,45979.797,45978.395,45979.414,45980.45,45972.79,45942.684,45911.77,45894.723,45870.973,45842.086,45830.41,45843.945,45857.574,45859.62,45885.17,45927.926,45962.87,46000.203,46036.137,46073.523,46091.406,46095.03,46100.438,46100.047,46100.32,46100.566,46109.62,46124.773,46144.32,46167.836,46206.566,46252.547,46280.625,46308.344,46341.39,46374.195,46369.6,46357.688,46355.312,46336.62,46314.504,46291.11,46294.793,46299.33,46302.727,46316.047,46324.863,46329.906,46329.742,46330.047,46329.83,46329.414,46345.297,46367.348,46364.887,46352.848,46346.344,46337.875,46312.984,46271.92,46266.105,46281.36,46295.855,46310.137,46337.51,46362.97,46376.082,46377.047,46374.55,46373.84,46360.67,46359.914,46359.98,46359.996,46359.746,46339.133,46308.715,46274.113,46233.266,46169.453,46091.566,46031.55,45979.3,45927.44,45897.85,45892.434,45907.742,45938.535,45979.55,46020.758,46040.086,46058.016,46067.402,46065.906,46055.125,46049.875,46049.27,46050.008,46032.36,46021.246,46016.906,46030.402,46057.176,46095.957,46142.055,46172.39,46199.66,46205.34,46209.824,46200.945,46201.555,46212.438,46219.527,46225.285,46218.457,46215.184,46214.17,46213.1,46213.105,46216.46,46219.863,46227.004,46236.207,46258.332,46298.99,46332.176,46366.46,46396.23,46425.504,46440.64,46425.516,46414.652,46407.65,46406.62,46410.688,46412.984,46426.38,46443.383,46456.94,46462.812,46461.668,46461.89,46463.58,46463.223,46462.504,46461.676,46461.887,46462.62,46463.836,46468.41,46472.09,46467.746,46463.75,46455.758,46439.004,46411.316,46380.586,46355.11,46320.664,46289.887,46267.11,46261.72,46260.613,46260.203,46266.918,46271.64,46272.06,46263.19,46253.43,46244.258,46236.54,46235.336,46253.973,46262.906,46266.574,46285.81,46312.992,46332.48,46337.37,46340.95,46350.723,46349.797,46343.145,46337.11,46328.727,46333.527,46337.54,46338.387,46337.594,46339.438,46340.777,46340.836,46341.074,46341.188,46340.555,46340.863,46368.91,46413.344,46468.562,46526.93,46580.453,46643.37,46683.312,46711.957,46728.22,46750.66,46784.605,46804.227,46814.004,46823.05,46834.848,46837.43,46831.61,46830.26,46835.32,46835.12,46835.645,46836.72,46839.473,46840.93,46839.727,46836.63,46817.023,46795.535,46767.855,46741.547,46708.223,46680.86,46666.777,46654.453,46643.633,46633.676,46631.406,46626.508,46624.92,46621.582,46622.656,46624.188,46626.984,46628.867,46629.78,46629.812,46629.234,46626.965,46628.75,46632.63,46632.66,46626.836,46619.188,46601.055,46580.957,46554.43,46530.805,46520.39,46510.3,46510.625,46508.906,46511.676,46514.15,46511.37,46510.496,46511.91,46512.95,46513.742,46514.906,46515.598,46515.402,46512.387,46508.99,46492.695,46469.086,46448.336,46431.383,46414.94,46394.777,46389.504,46390.945,46393.098,46395.605,46397.727,46404.45,46407.605,46409.316,46409.2,46405.547,46404.09,46401.445,46400.906,46400.68,46400.496,46399.918,46401.84,46412.246,46430.035,46452.984,46471.387,46491.594,46511.17,46522.008,46519.773,46508.906,46501.508,46491.273,46479.703,46468.074,46462.57,46461.2,46461.105,46461.406,46460.31,46461.44,46461.965,46461.367,46461.2,46461.58,46461.953,46461.684,46461.43,46462.645,46462.18,46460.453,46455.055,46446.56,46430.57,46415.793,46390.97,46366.285,46344.746,46325.457,46325.266,46329.418,46343.023,46355.574,46369.18,46383.453,46383.145,46377.63,46372.45,46364.977,46355.9,46338.305,46345.234,46332.484,46313.184,46295.25,46269.953,46249.074,46210.445,46180.406,46155.29,46139.38,46128.355,46122.273,46114.992,46127.59,46142.69,46148.668,46156.184,46164.867,46169.3,46164.992,46160.02,46156.125,46163.152,46177.316,46183.76,46181.492,46182.67,46186.3,46185.29,46174.754,46170.062,46175.71,46183.15,46191.438,46196.24,46203.227,46210.254,46215.48,46215.76,46215.8,46212.965,46211.08,46209.72,46209.81,46209.7,46208.395,46206.86,46206.484,46198.7,46207.61,46239.055,46271.164,46308.176,46343.76,46380.06,46396.64,46392.547,46385.504,46370.43,46351.574,46347.668,46345.72,46342.508,46341.02,46345.082,46350.81,46348.66,46346.715,46343.76,46339.348,46336.965,46344.715,46349.36,46346.805,46366.984,46395.89,46425.28,46444.223,46457.027,46487.27,46508.02,46523.902,46533.027,46541.902,46562.453,46574.99,46572.84,46570.15,46572.19,46574.227,46573.293,46571.945,46572.137,46580.04,46594.902,46611.2,46633.027,46656.457,46680.277,46696.547,46695.223,46685.887,46673.01,46661.12,46653.59,46639.504,46632.61,46632.863,46635.523,46636.61,46630.004,46630.95,46635.773,46640.004,46640.42,46638.15,46637.2,46635.805,46633.03,46629.555,46611.145,46595.934,46584.562,46578.246,46574.492,46575.35,46596.016,46618.312,46639.645,46658.523,46672.19,46681.496,46683.902,46682.504,46679.137,46675.266,46675.375,46676.457,46681.004,46684.938,46688.76,46691.438,46694.89,46702.934,46717.04,46733.797,46741.332,46753.027,46766.48,46782.62,46790.35,46814.754,46858.484,46894.688,46949.13,47001.78,47061.0,47104.89,47133.21,47162.844,47168.836,47167.676,47160.324,47151.473,47146.72,47145.668,47150.027,47152.293,47152.812,47173.355,47199.97,47231.28,47253.676,47269.754,47287.984,47275.234,47250.23,47220.848,47193.316,47173.46,47154.004,47142.86,47139.133,47132.934,47130.953,47130.785,47129.883,47129.81,47128.395,47125.977,47123.555,47124.62,47107.707,47084.484,47053.93,47027.484,47005.54,46977.1,46971.98,46973.957,46987.32,46996.348,47000.19,47007.03,47009.023,47010.35,47010.74,47014.027,47017.7,47021.723,47025.9,47030.477,47033.266,47034.742,47033.79,47017.36,47009.207,47017.375,47028.953,47039.438,47049.2,47074.164,47091.023,47094.027,47106.51,47119.043,47135.176,47150.305,47166.918,47181.37,47182.758,47184.156,47185.453,47187.363,47186.516,47185.043,47183.645,47183.32,47183.0,47188.32,47202.688,47214.28,47236.824,47263.113,47281.863,47318.766,47364.105,47400.918,47420.953,47430.375,47448.73,47442.797,47422.098,47413.36,47411.312,47415.266,47416.77,47419.484,47418.727,47417.14,47415.254,47412.2,47415.74,47428.062,47437.61,47435.133,47437.74,47440.33,47434.883,47419.363,47412.633,47415.543,47413.586,47413.55],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\"},\"mode\":\"lines\",\"name\":\"Test Actual\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47354.723,47386.867,47480.477,47473.496,47487.812,47481.54,47479.62,47470.945,47472.492,47478.9,47458.195,47448.29,47460.25,47502.414,47531.24,47498.434,47501.688,47516.1,47485.664,47526.723,47534.07,47558.37,47580.25,47563.18,47553.977,47579.523,47578.06,47567.715,47581.76,47578.43,47569.055,47570.44,47555.64,47533.176,47540.406,47546.89,47586.316,47688.715,47680.273,47713.484,47752.676,47771.42,47776.61,47787.48,47793.324,47894.84,47900.336,47876.62,47871.887,47895.88,47923.523,47921.543,47915.22,47883.438,47889.65,47888.965,47884.215,47877.965,47851.34,47849.41,47919.895,48024.168,48001.297,47956.777,47951.668,47942.934,47936.01,47891.285,47869.402,47898.98,47915.797,47921.13,47899.66,47902.406],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Test Pred\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47420.9,47413.48,47406.254,47417.316,47427.71,47439.484,47444.152,47464.97,47478.984,47477.652,47478.55,47473.613,47468.07,47464.848,47470.09,47479.88,47483.137,47490.383,47501.688,47505.926,47509.977,47510.45,47520.438,47533.53,47541.375,47552.76,47561.56,47568.89,47570.453,47570.703,47573.246,47575.758,47574.242,47570.508,47564.75,47557.86,47552.6,47555.48,47575.19,47595.965,47626.01,47661.39,47698.816,47730.53,47746.992,47765.832,47796.06,47820.668,47838.203,47854.08,47872.15,47893.844,47898.3,47900.777,47901.914,47904.875,47903.723,47897.17,47889.91,47879.266,47873.59,47878.633,47901.164,47920.68,47933.812,47950.535,47966.12,47968.805,47946.66,47924.68,47915.047,47909.066,47905.438,47899.375],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"shapes\":[{\"line\":{\"color\":\"black\",\"dash\":\"dot\",\"width\":2},\"type\":\"line\",\"x0\":657.5,\"x1\":657.5,\"xref\":\"x\",\"y0\":0,\"y1\":1,\"yref\":\"y domain\"}],\"legend\":{\"x\":0,\"y\":1},\"title\":{\"text\":\"Gold Price Forecast\"},\"xaxis\":{\"title\":{\"text\":\"Sample Index (time ordered)\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ac456f54-9746-4413-a099-668d3e040f10');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'MAE': np.float32(42.82147),\n",
              "  'RMSE': np.float32(65.06896),\n",
              "  'MAPE': np.float32(0.092098504),\n",
              "  'sMAPE': np.float32(0.09212282),\n",
              "  'R2': np.float32(0.97357225),\n",
              "  'MASE': np.float32(1.877586)},\n",
              " 'test': {'MAE': np.float32(40.150284),\n",
              "  'RMSE': np.float32(53.662918),\n",
              "  'MAPE': np.float32(0.08411537),\n",
              "  'sMAPE': np.float32(0.08416282),\n",
              "  'R2': np.float32(0.9212642),\n",
              "  'MASE': np.float32(1.760463)}}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "6DKAPUcsyJD7",
        "outputId": "2808151d-8baa-4333-8e0a-bd7fab2b916d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"8696feff-dcb0-4591-bede-06c9dfd6e873\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8696feff-dcb0-4591-bede-06c9dfd6e873\")) {                    Plotly.newPlot(                        \"8696feff-dcb0-4591-bede-06c9dfd6e873\",                        [{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Train Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[45984.227,45960.82,46026.0,45970.14,45887.527,45871.918,45900.355,45971.79,45983.082,45969.434,45954.664,45931.125,45936.87,45935.016,45947.05,45934.812,45937.61,45929.83,45918.48,45908.62,45921.16,45930.16,45951.348,46005.6,45957.52,45827.305,45881.125,45893.31,45966.965,46004.266,46003.22,45964.938,45903.85,45952.133,45979.734,45979.26,45978.773,45980.918,45980.16,45979.836,45978.848,45980.004,45979.875,45969.938,45987.734,45989.51,45930.2,45797.305,45796.195,45867.9,45842.184,45818.777,45860.004,45878.105,45882.305,45878.63,45992.832,46078.37,46068.184,46101.63,46100.438,46099.992,46101.16,46099.51,46100.355,46100.305,46100.805,46100.703,46154.58,46194.625,46214.484,46241.246,46336.16,46374.25,46324.91,46358.957,46413.51,46437.992,46309.812,46302.836,46310.043,46247.637,46279.73,46299.785,46330.566,46330.145,46329.773,46329.73,46330.094,46330.05,46328.72,46329.402,46329.94,46326.668,46426.277,46461.33,46313.566,46261.734,46287.332,46278.31,46274.54,46213.426,46284.07,46353.215,46373.168,46362.41,46437.6,46367.902,46360.246,46359.69,46358.92,46359.29,46360.7,46359.62,46359.066,46359.664,46360.285,46235.34,46179.348,46150.363,46115.312,45976.027,45895.22,45876.004,45866.117,45833.227,45938.47,45941.29,45993.844,46058.98,46110.35,46083.207,46050.34,46049.355,46050.492,46049.566,46050.08,46050.266,46050.906,46051.027,45943.016,45984.53,46023.59,46130.305,46216.715,46278.58,46222.066,46164.582,46186.344,46163.61,46241.934,46228.64,46224.316,46227.004,46228.375,46202.684,46197.438,46212.785,46216.27,46219.93,46226.21,46225.523,46219.555,46254.832,46271.492,46353.145,46470.49,46422.34,46424.56,46433.42,46446.465,46443.945,46377.77,46357.113,46385.895,46426.75,46470.715,46459.066,46458.516,46459.703,46467.25,46463.66,46460.844,46463.05,46466.52,46461.066,46460.008,46459.562,46464.574,46465.05,46471.363,46489.617,46481.78,46434.68,46439.984,46416.508,46372.42,46323.355,46293.19,46284.535,46232.605,46233.844,46237.047,46290.312,46285.83,46280.86,46275.66,46259.38,46240.773,46233.89,46230.47,46224.11,46229.555,46251.242,46353.297,46286.9,46255.867,46343.51,46390.867,46369.34,46380.36,46308.36,46315.45,46336.305,46350.684,46330.508,46330.14,46337.945,46340.016,46340.945,46343.617,46340.723,46340.676,46338.863,46339.14,46341.83,46341.91,46343.13,46507.35,46606.59,46671.707,46690.383,46665.023,46720.05,46746.582,46778.03,46769.035,46825.504,46868.145,46837.87,46805.65,46832.273,46839.84,46840.992,46833.223,46829.824,46835.906,46830.64,46843.195,46847.336,46849.945,46838.562,46828.387,46812.074,46725.92,46718.0,46684.57,46680.273,46629.22,46648.19,46640.727,46644.676,46620.06,46618.727,46615.605,46619.426,46631.242,46625.28,46625.715,46628.707,46632.38,46629.965,46636.812,46625.117,46622.68,46614.785,46643.375,46652.125,46636.176,46590.742,46577.28,46507.555,46521.867,46492.703,46495.35,46527.53,46516.96,46510.19,46511.617,46509.19,46510.203,46509.94,46511.516,46518.934,46516.707,46514.906,46519.02,46513.51,46508.625,46501.79,46497.54,46415.625,46377.816,46389.668,46408.207,46402.11,46377.895,46383.508,46388.246,46401.37,46422.523,46416.008,46415.73,46400.57,46400.85,46400.195,46401.24,46405.086,46400.074,46399.6,46400.465,46397.04,46401.8,46413.16,46464.926,46504.344,46537.355,46507.51,46521.28,46533.227,46528.934,46492.375,46470.6,46463.867,46457.953,46463.848,46458.8,46460.99,46462.73,46463.152,46459.176,46460.137,46463.297,46463.44,46459.543,46461.168,46462.203,46461.34,46461.34,46462.9,46465.508,46457.78,46451.797,46429.973,46409.086,46368.89,46379.11,46307.66,46304.777,46298.438,46293.09,46368.35,46403.305,46390.375,46379.504,46381.36,46379.19,46368.25,46367.62,46360.383,46334.312,46327.613,46272.246,46409.445,46290.504,46246.016,46227.426,46176.176,46147.98,46172.953,46111.406,46095.176,46131.77,46114.82,46106.184,46132.684,46187.42,46182.977,46166.723,46161.266,46160.902,46158.516,46160.05,46153.062,46142.484,46199.68,46250.94,46196.113,46147.176,46160.27,46163.2,46197.023,46183.004,46169.234,46182.055,46209.293,46210.55,46226.027,46222.6,46216.637,46210.234,46209.777,46210.453,46210.246,46209.387,46211.08,46210.02,46208.074,46202.5,46198.758,46207.55,46164.145,46264.29,46395.895,46397.62,46418.59,46421.613,46379.242,46364.016,46375.496,46354.05,46327.004,46307.402,46355.367,46353.71,46356.332,46346.805,46348.344,46344.973,46342.344,46339.0,46340.49,46321.38,46328.59,46391.082,46373.938,46324.586,46462.375,46494.46,46505.445,46506.06,46449.383,46506.16,46588.312,46589.87,46558.38,46559.355,46573.652,46579.746,46575.652,46572.527,46572.33,46571.4,46569.016,46570.156,46576.94,46621.1,46660.504,46669.56,46700.305,46710.242,46721.03,46718.01,46652.137,46614.688,46622.484,46638.85,46675.027,46634.312,46611.543,46615.625,46638.223,46645.79,46636.06,46639.95,46639.793,46640.59,46639.844,46632.277,46629.617,46632.184,46623.258,46618.473,46530.57,46542.062,46562.06,46592.87,46600.45,46622.074,46655.89,46675.543,46690.793,46705.645,46682.277,46678.676,46670.633,46667.855,46670.76,46681.465,46682.54,46685.223,46698.07,46691.14,46693.688,46697.434,46703.656,46734.19,46782.383,46791.996,46738.74,46768.074,46784.406,46830.703,46828.727,46937.824,47000.8,46985.273,47111.617,47146.31,47183.824,47201.797,47170.438,47162.434,47147.973,47139.156,47140.195,47149.188,47141.836,47155.723,47173.863,47151.645,47143.19,47273.047,47301.215,47344.547,47306.83,47248.996,47251.273,47198.312,47151.047,47168.277,47142.26,47130.633,47133.715,47131.434,47129.887,47130.594,47129.75,47130.16,47128.72,47129.55,47121.688,47115.11,47115.688,47136.188,47027.75,46990.297,46938.39,46955.812,46984.668,46965.723,46996.875,47002.3,47018.637,47009.832,47008.418,47006.32,47009.227,47010.027,47021.508,47029.547,47029.703,47030.78,47034.617,47036.727,47038.09,47038.676,47023.66,46931.637,46985.934,47086.08,47107.93,47101.406,47081.703,47081.68,47087.598,47103.297,47183.117,47176.766,47178.348,47172.215,47187.94,47190.363,47189.605,47185.125,47188.188,47184.56,47182.46,47180.043,47180.652,47182.41,47186.94,47216.652,47269.375,47249.285,47316.812,47339.297,47299.344,47439.215,47538.246,47471.395,47435.594,47395.074,47411.484,47403.324,47415.383,47419.31,47420.824,47420.207,47419.906,47420.145,47411.938,47409.566,47409.37,47400.6,47439.45,47493.21,47470.254,47396.574,47423.09,47415.48,47410.266,47398.633,47429.688,47414.41,47410.562,47416.76,47453.15],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Train Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[46043.855,45985.293,45963.48,46022.32,45972.51,45901.13,45886.496,45910.52,45974.688,45983.414,45973.914,45959.023,45936.92,45941.27,45940.47,45950.06,45940.867,45943.504,45937.617,45925.36,45918.457,45929.18,45936.812,45955.633,46005.26,45962.19,45851.082,45895.86,45906.496,45967.45,46004.54,46003.08,45968.39,45914.402,45954.715,45980.98,45980.875,45980.844,45981.527,45980.4,45980.594,45980.05,45981.06,45981.02,45973.65,45986.875,45989.69,45934.797,45827.34,45825.7,45886.234,45864.293,45843.07,45877.746,45894.742,45896.73,45893.85,45994.266,46073.83,46065.094,46099.875,46097.01,46095.11,46094.758,46092.664,46094.28,46094.184,46094.875,46094.11,46149.28,46189.797,46213.008,46240.33,46335.5,46377.27,46323.164,46359.188,46416.707,46443.066,46305.957,46299.395,46304.56,46244.492,46274.746,46295.527,46328.25,46327.46,46328.223,46326.85,46328.24,46327.664,46327.938,46326.906,46326.418,46323.37,46429.746,46467.402,46312.008,46256.258,46282.656,46273.344,46269.47,46207.664,46280.01,46352.895,46373.4,46361.934,46444.418,46367.74,46358.15,46359.0,46357.496,46358.188,46358.234,46358.914,46358.516,46358.164,46357.27,46232.625,46168.24,46138.336,46104.305,45972.824,45902.336,45886.21,45879.7,45853.992,45942.72,45945.75,45993.156,46054.81,46106.08,46080.29,46046.49,46046.402,46045.78,46044.41,46043.88,46045.43,46045.87,46044.6,45947.77,45983.457,46019.016,46125.477,46216.582,46279.387,46221.273,46158.44,46182.066,46157.875,46238.895,46226.188,46221.504,46224.797,46226.883,46198.832,46193.918,46208.11,46214.2,46216.266,46223.516,46222.402,46216.562,46253.414,46268.652,46354.06,46477.684,46428.223,46430.324,46438.01,46452.508,46448.875,46376.19,46354.746,46384.125,46430.16,46475.758,46464.652,46464.047,46465.973,46471.97,46469.047,46467.117,46467.29,46471.445,46465.793,46465.02,46465.21,46468.805,46469.914,46476.652,46494.484,46487.06,46437.805,46443.535,46417.37,46370.395,46318.004,46288.035,46279.562,46225.56,46227.855,46233.164,46284.812,46282.734,46277.07,46273.21,46256.676,46237.25,46232.1,46228.83,46220.125,46226.684,46248.918,46353.203,46283.85,46251.887,46341.664,46391.992,46369.934,46380.895,46304.3,46310.305,46333.44,46348.88,46328.316,46327.152,46336.055,46337.055,46340.023,46341.613,46338.434,46337.086,46337.48,46337.1,46338.984,46339.74,46341.35,46513.11,46612.188,46677.36,46695.34,46669.566,46724.547,46749.824,46781.453,46771.3,46832.99,46877.38,46845.035,46810.965,46839.117,46846.83,46847.918,46839.99,46836.484,46842.72,46837.45,46850.055,46854.402,46857.035,46845.617,46835.06,46817.9,46726.266,46718.195,46686.688,46682.312,46631.133,46649.93,46642.355,46646.77,46622.438,46621.38,46618.496,46622.14,46633.695,46628.83,46629.527,46632.31,46635.38,46633.19,46640.535,46628.367,46626.387,46618.586,46646.605,46655.32,46639.69,46594.35,46581.25,46510.035,46524.94,46495.84,46498.71,46531.4,46520.465,46514.54,46515.598,46513.08,46514.11,46513.53,46515.918,46522.79,46522.01,46519.82,46522.016,46517.207,46514.176,46505.617,46500.605,46415.363,46373.527,46388.004,46407.254,46401.086,46374.223,46381.305,46386.87,46401.49,46424.12,46418.133,46418.258,46401.766,46400.582,46400.555,46401.125,46406.203,46400.117,46399.438,46400.848,46397.215,46401.215,46415.77,46470.05,46509.066,46542.07,46511.848,46525.906,46538.207,46533.38,46496.824,46475.645,46468.508,46462.934,46468.14,46463.42,46465.58,46467.2,46468.855,46464.645,46464.86,46468.418,46468.633,46464.984,46465.6,46468.266,46466.477,46466.688,46468.24,46470.48,46463.17,46456.938,46433.895,46408.848,46367.05,46376.156,46303.52,46298.766,46293.74,46287.754,46366.277,46403.96,46391.504,46379.758,46378.523,46377.684,46367.734,46365.406,46359.707,46330.23,46324.246,46266.934,46411.438,46286.71,46243.49,46221.75,46166.645,46136.402,46165.54,46102.547,46086.082,46121.535,46105.332,46097.848,46125.2,46183.492,46179.3,46161.902,46155.312,46153.883,46152.53,46152.51,46145.97,46136.83,46195.695,46248.594,46192.71,46140.23,46153.152,46156.543,46191.613,46177.04,46162.32,46175.55,46206.17,46207.81,46224.42,46220.133,46214.438,46205.62,46207.17,46205.598,46205.5,46204.855,46206.785,46206.113,46205.38,46197.51,46194.656,46203.418,46155.945,46261.945,46400.133,46402.2,46423.74,46425.082,46380.074,46363.047,46374.695,46352.723,46324.36,46302.242,46353.14,46351.188,46355.305,46345.32,46344.74,46342.574,46339.66,46336.637,46337.62,46320.062,46326.42,46391.15,46372.94,46320.867,46469.08,46500.707,46509.645,46510.83,46453.418,46510.35,46594.33,46595.242,46563.58,46564.44,46578.336,46584.63,46580.387,46577.39,46577.406,46575.83,46572.945,46574.19,46581.85,46625.35,46665.08,46674.27,46704.734,46714.535,46724.016,46721.035,46654.746,46617.76,46625.793,46642.105,46678.957,46637.676,46613.723,46619.207,46641.383,46649.0,46638.67,46642.668,46642.957,46643.31,46643.996,46636.21,46632.84,46635.414,46626.598,46621.434,46533.383,46545.83,46565.445,46596.81,46604.63,46626.234,46660.332,46680.344,46694.88,46709.676,46686.555,46682.684,46674.367,46671.254,46674.19,46685.47,46686.5,46688.76,46702.242,46695.07,46697.367,46701.31,46707.34,46735.77,46786.242,46796.75,46739.977,46769.355,46787.766,46838.2,46836.086,46952.586,47017.74,47001.83,47129.684,47167.633,47207.17,47225.945,47192.984,47184.49,47169.492,47159.996,47161.42,47171.63,47163.727,47178.695,47198.45,47174.895,47165.1,47301.64,47330.652,47374.74,47336.688,47277.14,47279.54,47224.297,47173.812,47192.453,47164.777,47151.645,47155.688,47152.934,47151.11,47152.062,47151.824,47151.133,47150.32,47150.945,47142.285,47135.293,47135.805,47158.242,47043.41,47005.812,46950.812,46969.945,47001.336,46981.47,47013.977,47019.1,47035.215,47026.09,47024.66,47022.68,47025.355,47025.9,47037.48,47045.562,47045.74,47047.008,47050.645,47052.918,47053.996,47054.64,47039.79,46943.957,47001.914,47103.277,47126.85,47119.848,47099.0,47098.258,47105.4,47121.586,47206.816,47201.098,47202.457,47196.027,47212.902,47215.242,47214.773,47210.06,47212.758,47209.406,47207.293,47205.04,47206.04,47207.82,47212.875,47242.402,47297.96,47277.953,47346.97,47370.29,47328.758,47467.5,47559.863,47498.746,47464.875,47425.715,47441.824,47433.785,47446.98,47450.027,47451.387,47451.008,47450.34,47451.58,47443.16,47442.13,47439.754,47432.63,47469.812,47521.676,47498.594,47428.46,47453.2,47446.734,47441.535,47430.75,47461.504,47444.375,47441.9,47448.195],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\"},\"mode\":\"lines\",\"name\":\"Test Actual\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47354.723,47386.867,47480.477,47473.496,47487.812,47481.54,47479.62,47470.945,47472.492,47478.9,47458.195,47448.29,47460.25,47502.414,47531.24,47498.434,47501.688,47516.1,47485.664,47526.723,47534.07,47558.37,47580.25,47563.18,47553.977,47579.523,47578.06,47567.715,47581.76,47578.43,47569.055,47570.44,47555.64,47533.176,47540.406,47546.89,47586.316,47688.715,47680.273,47713.484,47752.676,47771.42,47776.61,47787.48,47793.324,47894.84,47900.336,47876.62,47871.887,47895.88,47923.523,47921.543,47915.22,47883.438,47889.65,47888.965,47884.215,47877.965,47851.34,47849.41,47919.895,48024.168,48001.297,47956.777,47951.668,47942.934,47936.01,47891.285,47869.402,47898.98,47915.797,47921.13,47899.66,47902.406],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Test Pred\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47483.227,47386.242,47418.324,47508.484,47501.95,47515.332,47509.53,47507.8,47499.7,47501.184,47507.227,47487.766,47478.363,47489.84,47529.258,47555.53,47525.598,47528.598,47541.83,47513.8,47551.527,47558.176,47579.906,47599.184,47584.293,47576.22,47598.746,47597.484,47588.46,47600.8,47597.94,47589.793,47591.055,47578.01,47557.918,47564.414,47570.21,47604.9,47690.04,47683.297,47709.617,47739.652,47753.676,47757.59,47765.64,47770.004,47841.027,47844.855,47828.367,47825.316,47841.832,47860.83,47859.473,47855.164,47833.395,47837.71,47837.285,47834.055,47829.848,47811.87,47810.566,47858.496,47925.254,47911.188,47883.004,47879.652,47873.86,47869.22,47838.938,47824.203,47844.21,47855.656,47859.31,47844.69],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"shapes\":[{\"line\":{\"color\":\"black\",\"dash\":\"dot\",\"width\":2},\"type\":\"line\",\"x0\":657.5,\"x1\":657.5,\"xref\":\"x\",\"y0\":0,\"y1\":1,\"yref\":\"y domain\"}],\"legend\":{\"x\":0,\"y\":1},\"title\":{\"text\":\"Gold Price Forecast\"},\"xaxis\":{\"title\":{\"text\":\"Sample Index (time ordered)\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8696feff-dcb0-4591-bede-06c9dfd6e873');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'MAE': np.float32(25.034344),\n",
              "  'RMSE': np.float32(38.208153),\n",
              "  'MAPE': np.float32(0.053799286),\n",
              "  'sMAPE': np.float32(0.053797826),\n",
              "  'R2': np.float32(0.99088496),\n",
              "  'MASE': np.float32(1.0997593)},\n",
              " 'test': {'MAE': np.float32(41.446842),\n",
              "  'RMSE': np.float32(51.36988),\n",
              "  'MAPE': np.float32(0.0867803),\n",
              "  'sMAPE': np.float32(0.086811945),\n",
              "  'R2': np.float32(0.92784923),\n",
              "  'MASE': np.float32(1.8207608)}}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "# you already have: trainset, testset, model\n",
        "# Create the same model architecture\n",
        "\n",
        "\n",
        "model = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size=output_size,\n",
        "    dropout=dropout)\n",
        "\n",
        "# model = LSTM(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_state_dict(torch.load('/content/checkpoint_transformer.pt'))\n",
        "\n",
        "result = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "\n",
        "plot_predictions_plotly(result, horizon_step=0, title=\"Gold Price Forecast\", name='transformer')\n",
        "\n",
        "\n",
        "transformer_metrics = evaluate_forecast(result)\n",
        "transformer_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "tKLh7-2SFu0K",
        "outputId": "8eeef4b0-1929-400e-b1ec-8cdc33b5d6ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"3aadce78-8808-4a1b-b3e5-f7dffde6cd36\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3aadce78-8808-4a1b-b3e5-f7dffde6cd36\")) {                    Plotly.newPlot(                        \"3aadce78-8808-4a1b-b3e5-f7dffde6cd36\",                        [{\"line\":{\"color\":\"black\"},\"mode\":\"lines\",\"name\":\"Test Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47354.723,47386.867,47480.477,47473.496,47487.812,47481.54,47479.62,47470.945,47472.492,47478.9,47458.195,47448.29,47460.25,47502.414,47531.24,47498.434,47501.688,47516.1,47485.664,47526.723,47534.07,47558.37,47580.25,47563.18,47553.977,47579.523,47578.06,47567.715,47581.76,47578.43,47569.055,47570.44,47555.64,47533.176,47540.406,47546.89,47586.316,47688.715,47680.273,47713.484,47752.676,47771.42,47776.61,47787.48,47793.324,47894.84,47900.336,47876.62,47871.887,47895.88,47923.523,47921.543,47915.22,47883.438,47889.65,47888.965,47884.215,47877.965,47851.34,47849.41,47919.895,48024.168,48001.297,47956.777,47951.668,47942.934,47936.01,47891.285,47869.402,47898.98,47915.797,47921.13,47899.66,47902.406],\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"LSTM Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47454.664,47391.64,47401.434,47466.3,47475.887,47488.35,47488.13,47487.375,47481.816,47481.58,47485.664,47473.06,47463.45,47468.98,47497.453,47522.555,47508.824,47508.074,47517.15,47500.113,47521.977,47531.977,47549.74,47568.06,47563.48,47557.3,47571.125,47574.074,47569.023,47576.188,47576.258,47570.87,47570.27,47561.168,47544.867,47544.594,47548.027,47572.363,47636.94,47651.625,47675.117,47703.562,47722.895,47732.863,47742.414,47749.16,47798.156,47817.734,47815.906,47814.367,47824.484,47839.81,47844.965,47844.855,47832.145,47830.203,47829.02,47826.453,47822.71,47809.793,47804.06,47831.6,47881.805,47891.94,47880.39,47874.836,47869.5,47864.707,47844.83,47827.957,47833.46,47841.72,47846.703,47839.855],\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"GRU Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47445.223,47380.72,47386.25,47456.91,47470.54,47480.875,47478.19,47475.43,47468.668,47468.03,47472.562,47459.51,47449.023,47455.176,47486.2,47513.348,47497.176,47493.87,47503.055,47485.016,47507.594,47519.01,47537.082,47555.293,47548.312,47539.734,47554.004,47557.074,47550.8,47557.977,47557.895,47551.645,47550.87,47541.324,47524.184,47524.816,47529.926,47556.734,47625.832,47637.887,47658.098,47684.64,47701.12,47707.453,47714.41,47719.094,47771.59,47788.035,47779.88,47774.863,47785.285,47801.336,47804.543,47802.117,47786.008,47784.715,47784.316,47781.965,47778.297,47764.082,47759.383,47792.844,47848.61,47853.38,47833.938,47826.047,47820.09,47815.414,47792.977,47776.27,47786.676,47797.902,47803.355,47794.383],\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Transformer Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47483.227,47386.242,47418.324,47508.484,47501.95,47515.332,47509.53,47507.8,47499.7,47501.184,47507.227,47487.766,47478.363,47489.84,47529.258,47555.53,47525.598,47528.598,47541.83,47513.8,47551.527,47558.176,47579.906,47599.184,47584.293,47576.22,47598.746,47597.484,47588.46,47600.8,47597.94,47589.793,47591.055,47578.01,47557.918,47564.414,47570.21,47604.9,47690.04,47683.297,47709.617,47739.652,47753.676,47757.59,47765.64,47770.004,47841.027,47844.855,47828.367,47825.316,47841.832,47860.83,47859.473,47855.164,47833.395,47837.71,47837.285,47834.055,47829.848,47811.87,47810.566,47858.496,47925.254,47911.188,47883.004,47879.652,47873.86,47869.22,47838.938,47824.203,47844.21,47855.656,47859.31,47844.69],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"legend\":{\"x\":0,\"y\":1},\"title\":{\"text\":\"Gold Price Forecasts (Test)\"},\"xaxis\":{\"title\":{\"text\":\"Test Sample Index (time ordered)\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3aadce78-8808-4a1b-b3e5-f7dffde6cd36');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot results of models on testset\n",
        "lstm_test_pred_inv = predict_train_test(lstm_model, trainset, testset, batch_size=128, scaler=scaler)['test_pred_inv']\n",
        "gru_test_pred_inv = predict_train_test(gru_model, trainset, testset, batch_size=128, scaler=scaler)['test_pred_inv']\n",
        "transformer_test_pred_inv = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)['test_pred_inv']\n",
        "test_true_inv = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)['test_true_inv']\n",
        "\n",
        "\n",
        "results = {\n",
        "    \"test_true_inv\": test_true_inv,           # shape [N] or [N, H]\n",
        "    \"preds\": {\n",
        "        \"LSTM\": lstm_test_pred_inv,           # shape [N] or [N, H]\n",
        "        \"GRU\": gru_test_pred_inv,\n",
        "        \"Transformer\": transformer_test_pred_inv\n",
        "    }\n",
        "}\n",
        "\n",
        "plot_test_predictions_plotly(results, horizon_step=0, title=\"Gold Price Forecasts (Test)\", name='compare')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbgSeOvA8Knn"
      },
      "source": [
        "## Grid Search (Params Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s0nKb1M8PX2"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "param_grid = {\n",
        "    'd_model': [64, 256],\n",
        "    'lr': [0.001, 0.01],\n",
        "    'nhead': [8, 16],\n",
        "    'num_encoder_layers': [1, 6],\n",
        "    'dim_feedforward':[64, 256],\n",
        "    'dropout': [0.1, 0.5]\n",
        "\n",
        "}\n",
        "\n",
        "keys = param_grid.keys()\n",
        "\n",
        "combinations = [dict(zip(keys, values)) for values in product(*param_grid.values())]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT2tW6lh-N-q"
      },
      "outputs": [],
      "source": [
        "augmentations = Compose([AddGaussianNoise(), RandomScaling()])\n",
        "\n",
        "trainset, train_loader, testset, test_loader = get_loaders(df, time_bin=60, scaler=scaler, lookback=6,\n",
        "                                        lookforward=1, batch_size=16, train_size=0.9,\n",
        "                                        transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0LNDPU2X-iJJ",
        "outputId": "8ade23da-b59f-45ca-afff-66e07026ea1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 1] Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 2] Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 3] Early stopping at epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 4] Early stopping at epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 5] Early stopping at epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 6] Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 7] Early stopping at epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 8] Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 9] Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 10] Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 11] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 12] Early stopping at epoch 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 13] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 14] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 15] Early stopping at epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 16] Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 17] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 18] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 19] Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 20] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 21] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 22] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 23] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 24] Early stopping at epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 25] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 26] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 27] Early stopping at epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 28] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 29] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 30] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 31] Early stopping at epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 32] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 33] Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 34] Early stopping at epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 35] Early stopping at epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 36] Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 37] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 38] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 39] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 40] Early stopping at epoch 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 41] Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 42] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 43] Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 44] Early stopping at epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 45] Early stopping at epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 46] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 47] Early stopping at epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 48] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 49] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 50] Early stopping at epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 51] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 52] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 53] Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 54] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 55] Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 56] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 57] Early stopping at epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 58] Early stopping at epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 59] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 60] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 61] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 62] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 63] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 64] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   d_model     lr  nhead  num_encoder_layers  dim_feedforward  dropout  \\\n",
              "0      256  0.001      8                   1              256      0.1   \n",
              "1      256  0.001     16                   1               64      0.1   \n",
              "2       64  0.010      8                   1               64      0.1   \n",
              "3      256  0.001      8                   1               64      0.1   \n",
              "4      256  0.001      8                   1              256      0.5   \n",
              "\n",
              "    train_MAE  train_RMSE  train_MAPE  train_sMAPE  train_R2  train_MASE  \\\n",
              "0   59.876881  311.896606    0.134548     0.131579  0.635762    1.277514   \n",
              "1  104.224777  318.076447    0.230257     0.227805  0.621185    2.223706   \n",
              "2   47.454025  307.333282    0.107617     0.104765  0.646342    1.012464   \n",
              "3  106.763321  324.294800    0.235487     0.232300  0.606228    2.277868   \n",
              "4  156.878799  349.178284    0.343728     0.340185  0.543481    3.347116   \n",
              "\n",
              "    test_MAE  test_RMSE  test_MAPE  test_sMAPE   test_R2  test_MASE  \n",
              "0  28.325327  38.131069   0.059378    0.059385  0.960992   0.604340  \n",
              "1  30.906672  38.924900   0.064777    0.064772  0.959350   0.659415  \n",
              "2  30.021906  41.927132   0.062885    0.062913  0.952838   0.640538  \n",
              "3  36.672825  45.775204   0.076973    0.076952  0.943784   0.782440  \n",
              "4  37.275181  46.248051   0.078194    0.078180  0.942616   0.795291  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec1bb963-416d-44e0-a227-27b0d5bcbf58\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>d_model</th>\n",
              "      <th>lr</th>\n",
              "      <th>nhead</th>\n",
              "      <th>num_encoder_layers</th>\n",
              "      <th>dim_feedforward</th>\n",
              "      <th>dropout</th>\n",
              "      <th>train_MAE</th>\n",
              "      <th>train_RMSE</th>\n",
              "      <th>train_MAPE</th>\n",
              "      <th>train_sMAPE</th>\n",
              "      <th>train_R2</th>\n",
              "      <th>train_MASE</th>\n",
              "      <th>test_MAE</th>\n",
              "      <th>test_RMSE</th>\n",
              "      <th>test_MAPE</th>\n",
              "      <th>test_sMAPE</th>\n",
              "      <th>test_R2</th>\n",
              "      <th>test_MASE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>256</td>\n",
              "      <td>0.1</td>\n",
              "      <td>59.876881</td>\n",
              "      <td>311.896606</td>\n",
              "      <td>0.134548</td>\n",
              "      <td>0.131579</td>\n",
              "      <td>0.635762</td>\n",
              "      <td>1.277514</td>\n",
              "      <td>28.325327</td>\n",
              "      <td>38.131069</td>\n",
              "      <td>0.059378</td>\n",
              "      <td>0.059385</td>\n",
              "      <td>0.960992</td>\n",
              "      <td>0.604340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>104.224777</td>\n",
              "      <td>318.076447</td>\n",
              "      <td>0.230257</td>\n",
              "      <td>0.227805</td>\n",
              "      <td>0.621185</td>\n",
              "      <td>2.223706</td>\n",
              "      <td>30.906672</td>\n",
              "      <td>38.924900</td>\n",
              "      <td>0.064777</td>\n",
              "      <td>0.064772</td>\n",
              "      <td>0.959350</td>\n",
              "      <td>0.659415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>0.010</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>47.454025</td>\n",
              "      <td>307.333282</td>\n",
              "      <td>0.107617</td>\n",
              "      <td>0.104765</td>\n",
              "      <td>0.646342</td>\n",
              "      <td>1.012464</td>\n",
              "      <td>30.021906</td>\n",
              "      <td>41.927132</td>\n",
              "      <td>0.062885</td>\n",
              "      <td>0.062913</td>\n",
              "      <td>0.952838</td>\n",
              "      <td>0.640538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>106.763321</td>\n",
              "      <td>324.294800</td>\n",
              "      <td>0.235487</td>\n",
              "      <td>0.232300</td>\n",
              "      <td>0.606228</td>\n",
              "      <td>2.277868</td>\n",
              "      <td>36.672825</td>\n",
              "      <td>45.775204</td>\n",
              "      <td>0.076973</td>\n",
              "      <td>0.076952</td>\n",
              "      <td>0.943784</td>\n",
              "      <td>0.782440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>256</td>\n",
              "      <td>0.5</td>\n",
              "      <td>156.878799</td>\n",
              "      <td>349.178284</td>\n",
              "      <td>0.343728</td>\n",
              "      <td>0.340185</td>\n",
              "      <td>0.543481</td>\n",
              "      <td>3.347116</td>\n",
              "      <td>37.275181</td>\n",
              "      <td>46.248051</td>\n",
              "      <td>0.078194</td>\n",
              "      <td>0.078180</td>\n",
              "      <td>0.942616</td>\n",
              "      <td>0.795291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec1bb963-416d-44e0-a227-27b0d5bcbf58')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec1bb963-416d-44e0-a227-27b0d5bcbf58 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec1bb963-416d-44e0-a227-27b0d5bcbf58');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-de38aaf9-2bb0-4547-9786-ed1dc501e5a1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de38aaf9-2bb0-4547-9786-ed1dc501e5a1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-de38aaf9-2bb0-4547-9786-ed1dc501e5a1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_grid",
              "summary": "{\n  \"name\": \"df_grid\",\n  \"rows\": 64,\n  \"fields\": [\n    {\n      \"column\": \"d_model\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96,\n        \"min\": 64,\n        \"max\": 256,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64,\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004535573676110723,\n        \"min\": 0.001,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nhead\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 8,\n        \"max\": 16,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          16,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_encoder_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dim_feedforward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96,\n        \"min\": 64,\n        \"max\": 256,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64,\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20158105227158793,\n        \"min\": 0.1,\n        \"max\": 0.5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_MAE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          614.7304077148438,\n          451.2472839355469\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_RMSE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          760.3920288085938,\n          598.1897583007812\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.3335003852844238,\n          0.9788501858711243\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_sMAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.3195170164108276,\n          0.9711848497390747\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_R2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          -1.1649088859558105,\n          -0.3398076295852661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_MASE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          13.115689277648926,\n          9.627666473388672\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_MAE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          632.9443969726562,\n          893.9075927734375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_RMSE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          661.4508056640625,\n          914.5366821289062\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.325530767440796,\n          1.8727104663848877\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_sMAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.3351795673370361,\n          1.8912229537963867\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_R2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          -10.738036155700684,\n          -21.43895149230957\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_MASE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          13.504297256469727,\n          19.072124481201172\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "num_epochs = 200\n",
        "results = []\n",
        "\n",
        "for cfg_idx, config in enumerate(combinations, start=1):\n",
        "\n",
        "  model = TransformerTS(\n",
        "      input_size=1,\n",
        "      d_model=config['d_model'],\n",
        "      nhead=config['nhead'],\n",
        "      num_encoder_layers=config['num_encoder_layers'],\n",
        "      dim_feedforward=config['dim_feedforward'],\n",
        "      output_size=1,\n",
        "      dropout=config['dropout'])\n",
        "\n",
        "  model.to(device)\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
        "  early_stopping = EarlyStopping(patience=10, mode='min', verbose=False, save_path= f'/content/checkpoint_{cfg_idx}.pt')\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      train_one_epoch(model, show=False)\n",
        "      stop_criteria = validate_one_epoch(model, show=False, early_stopping=early_stopping)\n",
        "\n",
        "      if stop_criteria:\n",
        "          print(f\"[CFG {cfg_idx}] Early stopping at epoch {epoch}\")\n",
        "          break\n",
        "\n",
        "\n",
        "\n",
        "  model = TransformerTS(\n",
        "      input_size=1,\n",
        "      d_model=config['d_model'],\n",
        "      nhead=config['nhead'],\n",
        "      num_encoder_layers=config['num_encoder_layers'],\n",
        "      dim_feedforward=config['dim_feedforward'],\n",
        "      output_size=1,\n",
        "      dropout=config['dropout'])\n",
        "\n",
        "  model.load_state_dict(torch.load(f'/content/checkpoint_{cfg_idx}.pt'))\n",
        "\n",
        "  result = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "  transformer_metrics = evaluate_forecast(result)\n",
        "\n",
        "\n",
        "  # متریک‌های train\n",
        "  for k, v in transformer_metrics['train'].items():\n",
        "      config[f\"train_{k}\"] = v\n",
        "\n",
        "  # متریک‌های test\n",
        "  for k, v in transformer_metrics['test'].items():\n",
        "      config[f\"test_{k}\"] = v\n",
        "\n",
        "  results.append(config)\n",
        "\n",
        "df_grid = pd.DataFrame(results)\n",
        "\n",
        "if \"test_RMSE\" in df_grid.columns:\n",
        "    df_grid = df_grid.sort_values(by=\"test_RMSE\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "df_grid.to_csv(\"transformer_grid_results.csv\", index=False)\n",
        "df_grid.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GtgFQkzQd8G"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8g_ilnxFzZh"
      },
      "source": [
        "\n",
        "*   Multiple step prediction\n",
        "*   How much Lookback\n",
        "*   Effect of TimeBin\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple step prediction"
      ],
      "metadata": {
        "id": "pR3teEB87jr2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WzAgIG6F33q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855bb28b-7fad-4b56-bccf-82287be7521b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.05561\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.05150\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02522\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02186\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00452\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00149\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00246\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00101\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00127\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00151\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00088\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00143\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00069\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00132\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00076\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00084\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00062\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00310\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00143\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00190\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00155\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00189\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00059\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00395\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00080\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00386\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00116\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00153\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00324\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00269\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00208\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00174\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00135\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00170\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00237\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00048\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00176\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00545\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00066\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00219\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00045\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00037\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00047\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00066\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00207\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00149\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00069\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00076\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00144\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00306\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00113\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00111\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00277\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00042\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00347\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00050\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00410\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00102\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00224\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00155\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00059\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00225\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00164\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00215\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00165\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00036\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00039\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00158\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00130\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00116\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00130\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00088\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00065\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00060\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00057\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00070\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00114\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00052\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00030\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00212\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00028\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00044\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00067\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00074\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00093\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00242\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00039\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00113\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00035\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00076\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00078\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00112\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00092\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00041\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00213\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00286\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00152\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00040\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00124\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00054\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00108\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00378\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00057\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00137\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00043\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00042\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00197\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00038\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00139\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00131\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00126\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00508\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00065\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00066\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00034\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00135\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00242\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00236\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00029\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00058\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00032\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00044\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00037\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00321\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00037\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00030\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00042\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00040\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00095\n",
            "***************************************************\n",
            "\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "early_stopping = EarlyStopping(patience=50, mode='min', verbose=False, save_path='/content/checkpoint_lookforward.pt')\n",
        "\n",
        "\n",
        "trainset, train_loader, testset, test_loader = get_loaders(df, time_bin=60, scaler=scaler, lookback=6,\n",
        "                                        lookforward=10, batch_size=16, train_size=0.9,\n",
        "                                        transform=augmentations)\n",
        "model_lookforward = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size= 10,\n",
        "    dropout=dropout)\n",
        "\n",
        "# model = GRUModel(1, 4, 1, 1)\n",
        "model_lookforward.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model_lookforward.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model_lookforward, show=False)\n",
        "    stop_criteria = validate_one_epoch(model_lookforward, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self, sample_mean=3, lookahead_steps=3):\n",
        "        \"\"\"\n",
        "        Baseline model to predict `lookahead_steps` using the mean of the previous `sample_mean` time steps.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sample_mean = sample_mean\n",
        "        self.lookahead_steps = lookahead_steps\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: input of shape [batch_size, time_steps, features]\n",
        "        :return: predictions of shape [batch_size, lookahead_steps, features]\n",
        "        \"\"\"\n",
        "        # Get the last `sample_mean` number of time steps\n",
        "        last_samples = x[:, -self.sample_mean:, :]  # shape: [batch, sample_mean, features]\n",
        "\n",
        "        # Calculate the mean over the last `sample_mean` steps\n",
        "        mean_value = last_samples.mean(dim=1)  # shape: [batch, features]\n",
        "\n",
        "        # Repeat the mean value to generate predictions for the next `lookahead_steps`\n",
        "        repeated_predictions = mean_value.unsqueeze(1).repeat(1, self.lookahead_steps, 1)\n",
        "\n",
        "        return repeated_predictions\n"
      ],
      "metadata": {
        "id": "opzt7LyEAqY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "baselineModel = BaselineModel(sample_mean=10, lookahead_steps=10)\n",
        "\n",
        "\n",
        "model_lookforward.eval()\n",
        "all_predictions = []\n",
        "all_predictions_baseline = []\n",
        "all_actual_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(test_loader):  # test_loader should provide the test data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        predicted_baseline = baselineModel(inputs)\n",
        "        predicted = model_lookforward(inputs)  # predicted shape (batch_size, 3)\n",
        "        all_predictions_baseline.append(predicted_baseline.cpu().numpy())\n",
        "        all_predictions.append(predicted.cpu().numpy())\n",
        "        all_actual_values.append(labels.cpu().numpy())\n",
        "\n",
        "# Convert the list of predictions and labels to numpy arrays for easier plotting\n",
        "all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "all_actual_values = np.concatenate(all_actual_values, axis=0)\n",
        "all_predictions_baseline = np.concatenate(all_predictions_baseline, axis=0)\n",
        "\n",
        "\n",
        "print(mean_absolute_error(all_actual_values, all_predictions),\n",
        "mean_absolute_error(all_actual_values, all_predictions_baseline.squeeze(-1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc57iHEF7zJq",
        "outputId": "89b1397c-1250-49fc-9a05-e715dc02e5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.024832408875226974 0.013545608147978783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the BaselineModel\n",
        "baseline_model = BaselineModel(sample_mean=3, lookahead_steps=3)\n",
        "\n",
        "result = predict_train_test(baseline_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "\n",
        "baseline_metrics = evaluate_forecast(result)\n",
        "baseline_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KumMSy7s9Ry-",
        "outputId": "473c6ee1-2f33-467d-f155-e041b8b0ac41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'MAE': np.float32(60.92469),\n",
              "  'RMSE': np.float32(357.41925),\n",
              "  'MAPE': np.float32(0.13697843),\n",
              "  'sMAPE': np.float32(0.13486174),\n",
              "  'R2': np.float32(0.50790465),\n",
              "  'MASE': np.float32(1.053577)},\n",
              " 'test': {'MAE': np.float32(29.475332),\n",
              "  'RMSE': np.float32(41.677708),\n",
              "  'MAPE': np.float32(0.06180005),\n",
              "  'sMAPE': np.float32(0.0618294),\n",
              "  'R2': np.float32(0.95263547),\n",
              "  'MASE': np.float32(0.50971997)}}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NO2h3ZL_r4U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}