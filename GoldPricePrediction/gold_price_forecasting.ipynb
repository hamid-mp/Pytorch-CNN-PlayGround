{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook focuses on **gold price forecasting** using a combination of baseline and deep learning models implemented in PyTorch. The project aims to predict future gold prices based on historical market data, enabling informed decision-making for trading and investment strategies.\n",
        "\n",
        "The workflow includes:\n",
        "\n",
        "* **Data Loading and Preprocessing** — Importing gold price data, applying statistical checks, scaling features, and generating technical indicators to enhance predictive power.\n",
        "* **Feature Engineering** — Leveraging rolling statistical tests and custom transformations to capture temporal dynamics in gold price movements.\n",
        "* **Model Development** — Implementing multiple architectures including:\n",
        "\n",
        "  * **Baseline Model** — A simple average-based predictor for performance benchmarking.\n",
        "  * **LSTM and GRU Networks** — Recurrent neural network models designed to capture sequential dependencies in time series data.\n",
        "* **Training and Evaluation** — Applying a structured training/testing regime, integrating techniques such as early stopping, and comparing model performances against the baseline.\n",
        "\n",
        "By establishing a robust comparison between naive prediction methods and advanced neural architectures, this notebook not only measures forecasting accuracy but also highlights the added value of deep learning for financial time series prediction.\n"
      ],
      "metadata": {
        "id": "8HWgI6bWS55c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI2gOzk9lETv",
        "outputId": "a812f64e-299b-45f4-b003-1171430b2364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMuRZfc26cgT",
        "outputId": "d592b4bc-cc85-40d7-8905-f4bf325ffcc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=092b491c08bd2cfde5b534c4a1671522dd8b3e53284c137d957ae834dc109dac\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.5-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from ptflops) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.5-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.5\n"
          ]
        }
      ],
      "source": [
        " ! pip install ta\n",
        " ! pip install ptflops\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tteu6Rhk6HHi"
      },
      "source": [
        "# import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqHG44Kw6Cl2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from ptflops import get_model_complexity_info\n",
        "import math\n",
        "import seaborn as sns\n",
        "import ta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yHH7wOW6L31"
      },
      "source": [
        "# Load Data and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfp_h0Qm6UhS"
      },
      "outputs": [],
      "source": [
        "def rolling_adf(df, col, window_size=30):\n",
        "    \"\"\"\n",
        "    Calculate the Augmented Dickey-Fuller test statistic on a rolling window.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing the column on which to perform the ADF test.\n",
        "    col : str\n",
        "        The name of the column on which to perform the ADF test.\n",
        "    window_size : int\n",
        "        The size of the rolling window.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_copy : pandas.DataFrame\n",
        "        A new DataFrame with an additional column containing the rolling ADF test statistic.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Create an empty series to store rolling ADF test statistic\n",
        "    rolling_adf_stat = pd.Series(dtype='float64', index=df_copy.index)\n",
        "\n",
        "    # Loop through the DataFrame by `window_size` and apply `adfuller`.\n",
        "    for i in range(window_size, len(df)):\n",
        "        window = df_copy[col].iloc[i-window_size:i]\n",
        "        adf_result = adfuller(window)\n",
        "        adf_stat = adf_result[0]\n",
        "        rolling_adf_stat.at[df_copy.index[i]] = adf_stat\n",
        "\n",
        "    # Add the rolling ADF test statistic series to the original DataFrame\n",
        "    # df_copy['rolling_adf_stat'] = rolling_adf_stat\n",
        "\n",
        "    return rolling_adf_stat\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Kaufman’s Adaptive Moving Average (KAMA)\n",
        "\n",
        "def kama(df, col, n):\n",
        "  df_copy = df.copy()\n",
        "  # df_copy[f'kama_{n}'] = ta.momentum.KAMAIndicator(df_copy[col], n).kama()\n",
        "  return ta.momentum.KAMAIndicator(df_copy[col], n).kama()\n",
        "\n",
        "def moving_parkinson_estimator(df, window_size=10):\n",
        "\n",
        "  def parkinson_estimator(df):\n",
        "    N = len(df)\n",
        "    sum_squared = np.sum(np.log(df['High'] / df['Low'])**2)\n",
        "    volatility = math.sqrt((1/(4 * N * math.log(2))) * sum_squared)\n",
        "    return volatility\n",
        "\n",
        "  df_copy = df.copy()\n",
        "\n",
        "  rolling_volatility = pd.Series(dtype='float64')\n",
        "\n",
        "  for i in range(window_size, len(df)):\n",
        "    window = df_copy.loc[df_copy.index[i-window_size] : df_copy.index[i]]\n",
        "    volatility = parkinson_estimator(window)\n",
        "    rolling_volatility.at[df_copy.index[i]] = volatility\n",
        "\n",
        "  # df_copy['rolling_volatility_parkinson'] = rolling_volatility\n",
        "\n",
        "  return rolling_volatility\n",
        "\n",
        "def moving_yang_zhang_estimator(df, window_size=30):\n",
        "    \"\"\"\n",
        "    Calculate Parkinson's volatility estimator based on high and low prices.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame containing 'high' and 'low' columns for each trading period.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    volatility : float\n",
        "        Estimated volatility based on Parkinson's method.\n",
        "    \"\"\"\n",
        "    def yang_zhang_estimator(df):\n",
        "        N = len(window)\n",
        "\n",
        "        term1 = np.log(window['High'] / window['Close']) * np.log(window['High'] / window['Open'])\n",
        "        term2 = np.log(window['Low'] / window['Close']) * np.log(window['Low'] / window['Open'])\n",
        "\n",
        "        sum_squared = np.sum(term1 + term2)\n",
        "        volatility = np.sqrt(sum_squared / N)\n",
        "\n",
        "        return volatility\n",
        "\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    rolling_volatility = pd.Series(dtype='float64')\n",
        "\n",
        "    for i in range(window_size, len(df)):\n",
        "        window = df_copy.loc[df_copy.index[i-window_size]: df_copy.index[i]]\n",
        "        volatility = yang_zhang_estimator(window)\n",
        "        rolling_volatility.at[df_copy.index[i]] = volatility\n",
        "\n",
        "\n",
        "    return rolling_volatility\n",
        "\n",
        "def RSI(df, col, window_size):\n",
        "  delta = df[col].diff(1)\n",
        "  positive = delta.copy()\n",
        "  negative = delta.copy()\n",
        "  positive[positive < 0] = 0\n",
        "  negative[negative > 0] = 0\n",
        "  average_gain = positive.rolling(window=window_size).mean()\n",
        "  average_loss = abs(negative.rolling(window=window_size).mean())\n",
        "  relative_strength = average_gain / average_loss\n",
        "  RSI = 100.0 - (100.0 - (1.0 + relative_strength))\n",
        "\n",
        "  return RSI\n",
        "\n",
        "\n",
        "def create_features(df, window_size=30):\n",
        "  df['moving_parkinson'] = moving_parkinson_estimator(df, window_size=window_size)\n",
        "  df['rolling_adf'] = rolling_adf(df, 'Price', window_size=window_size)\n",
        "  df['moving_yang_zhang'] = moving_yang_zhang_estimator(df, window_size=window_size)\n",
        "  df['kama'] =kama(df, 'Price', window_size)\n",
        "  df['rsi'] = RSI(df, 'Price', window_size)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_MonFxF6tIb"
      },
      "outputs": [],
      "source": [
        "pth = '/content/drive/MyDrive/MilliGold/data.xlsx'\n",
        "\n",
        "df = pd.read_excel(pth, header=1, sheet_name='Data')\n",
        "\n",
        "df['Timestamp'] = pd.to_timedelta(df['Day'], unit='d') + \\\n",
        "                pd.to_timedelta(df['Hour'], unit='h') + \\\n",
        "                pd.to_timedelta(df['Minute'], unit='m') + \\\n",
        "                pd.to_timedelta(df['Second'], unit='s')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers(df):\n",
        "  df1 = deepcopy(df)\n",
        "  z_scores = (df1['Price.'] - df1['Price.'].mean()) / df1['Price.'].std()\n",
        "\n",
        "  # Filter out rows where |z| > 2\n",
        "  df_no_outliers = df1[np.abs(z_scores) <= 2.0]\n",
        "\n",
        "  # Optional: reset index\n",
        "  df_no_outliers = df_no_outliers.reset_index()\n",
        "\n",
        "  return df_no_outliers\n",
        "\n",
        "\n",
        "\n",
        "df_no_outliers = remove_outliers(df)"
      ],
      "metadata": {
        "id": "Dn1IqKe37fHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 1)\n",
        "axs[0].plot(df['Price.'])\n",
        "axs[1].plot(df_no_outliers['Price.'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "bYJITXDTyXHB",
        "outputId": "d40ff4c1-35e6-4439-f095-b5ad53537119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79e24d0d24e0>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcbJJREFUeJzt3XlclNX+B/DPsA07KAqIoOAugguShLlkkqjcurZZRmZmll28iXZxybVNuFqppaVZZveXhtnNuompiFsmbiAKLrgrpoAbDKisc35/EI88zAADDDMwfN6v17xknvOd5zlnEObLec6iEEIIEBEREZkYM2NXgIiIiKghMMkhIiIik8Qkh4iIiEwSkxwiIiIySUxyiIiIyCQxySEiIiKTxCSHiIiITBKTHCIiIjJJFsaugDGp1Wpcu3YNDg4OUCgUxq4OERER6UAIgby8PHh4eMDMrOr+mmad5Fy7dg1eXl7GrgYRERHVQUZGBjw9Passb9ZJjoODA4CyN8nR0dHItSEiIiJdqFQqeHl5SZ/jVWnWSU75LSpHR0cmOURERE1MTUNNOPCYiIiITBKTHCIiIjJJTHKIiIjIJDHJISIiohp5z4yD98w4XMu5b+yq6KxZDzwmIiJqrradyETi+VuY+zdfmCnKBvEKIXDp1j1kqwqw9+wNRIZ0gaW5Gbxnxkmv6x+zU+NcYf5tsPSF3sjMLcDARbsAAIfeGYpW9kqYmRlvHTqFEEIY7epGplKp4OTkhNzcXM6uIiIikyaEgEKhgFot8NCHO3DrbpFBrrs1ciC6uev3M1bXz2/25BAREZmYtD9zEf7VQXw9LhDPrkw0al08W9ga7dpMcoiIiLQQQiDx/C0Ed3RpVFv/CCFw4eZddGhlB4VCgYLiUliYKWBhXjbM9sCFW3jhywMAUOcEJ3XBMPgv2C479u2r/TC4S2uN2Iq3sgCgi5s9rucWIK+gBCtfCoC90nipBm9X8XYVERFVcvHmXQz5aLf0/FJMWLXxS+LPYFnCWZ1iU6/mwq+tIxQKBS7fuovBi3ejh4cjvh73ENydrDHmywNIvHCr3m2oyUPeLbBxUn+9nCuvoBhKC3NYWRhmPpOun99McpjkEBFRJZV7JxpCmH8bxKVeb/DrADUnXk0Nx+QQEVGD0ZYE/Dp5gNRD0ZS8uvYwslQFOHFNZdDrNnSCY2qJTV0wySEiIg2594rR670HYzJOvTccX+69gK9+v4C8whKtr3li+T6NY8te6I0nenpg+n+P481HO6Jja3utry0pVUtjSvTt893nsGhrOgDgtQE++OXYNdzIK9Tptc8EeOK/yVcbpF7V+b8J/RDcwQXXcgqgKijGldv3sONkFqYN62LUgbxNDW9X8XYVEZHkZn4hAj/YYZBrBfm0xMGLt2XHatv7UD4tGgA2HL6CGf9N1Vv9KtZHCIEStYBlDYnYtZz76B+zE7NHdsfEQR1QWFKKgiI1nGwtpboWlajRZc5vAIAOre2w8+1H9Vrn5oBjcnTAJIeIqExdx6BsnzoIXdwcZK9/pb831u6/VOe6PNLJBX+ca/iBt9pcignDvaISfHfgMiYO7NDkbr01F0xydMAkhxqTguJSdJu7FQBwfMEwOFqX/eWXX1gCe6UFf9lSgxFCwGfWFq1lp98fjo+3p2P17xc1ys58MKLa2TSBH8TjZn7VC85FDOmIFbvO177CdfBYN1csfMofeQXF6NjaHmZmZYviGXM1Xqo7Jjk6YJJDxpaSkYNRK/7QOT51wTA4WFs2YI3ImNRqgdv3iuBiZ9VgSa2uPTab/zkAfm2dNI7fLyqFtaWZ3uvXefYWFJfq5+PoUkwYSkrVGL/2MN4e1hW9PJ34R4KJYZKjAyY5ZGi594vR693tNQfq4IleHvj12DVsnBSMh7xbQggBIYCCklL4ztsGAHhjcAdEDOkERyZGjZZaLdDhHe29KACw5pVAvLr2CAAgcdZjsFda4LfUTHRys4dvG0dYW5rL4iuv71Jb5xeOhLkRezc+2HwSX+0r6zX6OeIRfHfgMmaN6AYXe2WVr0m9misNeuaMouaBSY4OmOTopqC4FDfzC5vkiP6KgxKNzRDrblRn9sjuGNffG7fvFsHdybre5xNC4KWvD+Ih75YY398HY1YfwOuDOmBUn7Z6qG3Tt+1EJpKv3MGEAT6AAPotTDB2lWr03YQgDOjcytjVIKoRkxwdNNckp7CkFN/8cQmvDfCBWkAa5a/t/vqt/EL0/WumxZh+Xoh+uqfG+UrVAh3f2aLX1TP1YdZPqfj+0JUqy5c+3xuRG1KqPceATq3w3WtBWsuu3LoHc3MF2jrbQK0WUPy1i682v6T8iSmxVV8rrGcbfPxcL42/ysuVlKrRafZv1dZVHy7FhKGguFRWj99Sr6OzmwNcHZVw+GtsUOiSvUjPyqvXtc59OKLKKcNCCHy59wIGdm4NXw/j/WzmFRRDLQArczOs+eMiurk7YGDn1lWOQ9mdno1/fn8UMU/3RMT6ZAPXVnfLXuiN3PvFaGFrhSd6eRi7OkS1xiRHB80xyblXVCLdyqjKlrcGYsH/TsBWaY7d6TdkZekfDMe1nAJ8vuscNiZpXztixvBusFOa46k+bRts/IgQAqeu56FDazutiUF1AynronICmF9YAr/51b+PAODTyg4Xb97VOH7qveGwsTJHqVrU+tZAQXEpFm1Nx8vB7ZFXUCJbm6SVvRI388vW/4h52h8zf9LvdNqGEObfBh+P7iUNuq4see7jaGlnpfP5ikvVNU7z1UW2qsCgvS87pg3GjbxCjFldtueQu6M1MlUFAIA9UY/iic/2QVVQtj7N+teCcPn2Pcyq4vtbPuOJyFQxydFBc0py9P2hr4uJA30wO8y3Qc5d8dbPLxGP4O+1GLxrbIYeM3D8ag6eXN7w709LOyvcvlv1TJr6mvZ4F7w1tHO1M2LO38jH0I/3AAAOzw5BawelbH0TtVrgTHYeXOyUeOjDB2vBLHqmJ57p64mb+YVwdVDi1PU8jPz0d723IcinJTa8EVxl+YlruQj7tCxpHdK1Nb4Z36/Gc/aYtxV3i0pxMXpko7k1S9TQmOTowNSTnFK1wL+3nkZAuxaY9F2SQa7Zt30LJF2+U23MypcCMNyvjU7n6/XuduTeL8bF6JEAym4H1Wdsy+5/PYpHKw3K1PbhcCu/EJEbUvD72Zt1vpY2VhZmOPPBCL2eUx/qsiFgVbNvdGHs8UmG0tvLGT9HPKJz/KnrKoxYVpZchXR3xVfjHmqoqhE1aUxydGCoJCfnXhF6vxcPAHjv7z0w75cTeH1QB7wzsjvuFZUg514xPJxtpPjyD4Dd/3oUPxzJwOe7z6OlnRWS5z4OoGycwKTvkjDSvw3Cg9prXE/XWylA2eybqGFdtY73KB+fkXH7HpYlnMXm49r3WTm/cCTMKoxH0fUDrGNrO5SoBfZEDZGOxR2/joj1yVg/MQgvrj6o03lqEvfWAPTwePBh/FPyVew8nY2PqhkDUy7j9j0MXLRLp+tcignDpqNXMXXDMfw4KRiB3i3rVe/m4Js/LuLdX09Kz/dGDYFXSxuD9zpW5+txgRja3U12TAiBdQevYM7Paejm7oDN/xyAG/mFaGlnBUszM7z76wlcuHkX/zdB+3iuqqRn5iF06V4AwDBfN3z5cqDe2kFkSpjk6KChkhxVQTF6LtDPNOGaONtaInnO48grKIGDtQXO3cjHsCV7q32NttslSZfvIEtVgJH+bVBUooa5mULrWJGLN++iXUtbdPxryqu2c1X8a1RXLe2sEObfBv934HKtXlfR6EBPfDDKHxuTMjB7Uxp2TBuMjq3t9NKFfyOvELZW5rC1ModCoWhUs7ZMkVotUKxWo+sc7eN06iO4gwvSruVi46RgdHZ1wP3iUtzKL4TqfglUBcU4djVH2ufI2dYSKfOG6b0OVTmXnYeQT8p+fof3cMfKsX0Ndm2ipoRJjg4aKslpjF3x04d3xUsPtzf4einlYyQWPdMTzwV66vUvdK6H0TyVlKrxyL93IktViHMfjsBvaZl4yLsl3J2scfnWXQgBWJgrUFIq4N3KztjVrZWKY4rCerbBihcDjFwjosZJ189v7kKuZ4bIGaNCu2LxtvQa4/ZEPYr2Lsb9Jd+xtb0sGSn/WtdEsOI04/LX+LZxxJYpA/VcU2oqLMzNcPCdEOl5xSnQxv7/Xl9mFXoHzdhTSFRvTHL0TC2Afz/jj+NXc7HuoHyNlsD2LfDjm2XryAR+sEOa6lu5R+LO3SL0eT9eep7+wXBcunkPT33+B47OexxKC3OMDvSSzQ6pbNEzPRv1L/xLMWGynXgrW/NKIB7rJh8HcTF6JPIKS7h6L5msineImeIQ1R9vVxlg4HF5D0TFJKcmpWoBtRDVrvdxv6gUC/53Ar28nDGmnxeEQJPebC4ztwC/n72B5wK9jF0VIqOoONB9VG8PLH2hj5FrRNQ48XZVI1SbbNLcTAHzGv6Ws7Eyx7+ffbACcVPv3XZ3smaCQ81axZ9h3q4iqr96LQsaExMDhUKByMhI6dijjz4KhUIhe0yaNEn2uitXriAsLAy2trZwdXVFVFQUSkpKZDG7d+9GQEAAlEolOnXqhLVr12pcf8WKFfD29oa1tTWCgoJw6NCh+jSHiMiozHm/ikiv6pzkHD58GKtWrULPnpp7GU2cOBHXr1+XHosWLZLKSktLERYWhqKiIuzfvx/ffvst1q5di3nz5kkxFy9eRFhYGIYMGYKUlBRERkbitddew7ZtD9Z+2bBhA6ZNm4b58+cjOTkZvXr1QmhoKLKzs+vapAbH31lEVB0OPCbSrzolOfn5+QgPD8fq1avRokULjXJbW1u4u7tLj4r3y7Zv346TJ0/iu+++Q+/evTFixAi8//77WLFiBYqKypaEX7lyJXx8fPDxxx+je/fumDx5Mp599lksWbJEOs8nn3yCiRMnYvz48fD19cXKlStha2uLNWvW1KVJRERGJ79dZbx6EJmKOiU5ERERCAsLQ0hIiNbydevWoVWrVvDz88OsWbNw7949qSwxMRH+/v5wc3swcyY0NBQqlQonTpyQYiqfOzQ0FImJiQCAoqIiJCUlyWLMzMwQEhIixWhTWFgIlUolexARNRYVe29qu3ErEWmq9cDj2NhYJCcn4/Dhw1rLX3zxRbRv3x4eHh44fvw4ZsyYgfT0dPz0008AgMzMTFmCA0B6npmZWW2MSqXC/fv3cefOHZSWlmqNOX36dJV1j46Oxrvvvlu7ButRs53GRkQ6sbN68Cv50a6uRqwJkWmoVZKTkZGBKVOmID4+HtbW1lpjXn/9delrf39/tGnTBkOHDsX58+fRsWPH+tW2nmbNmoVp06ZJz1UqFby8OJuHiBoHGytzdHK1x7nsfIT2cDd2dYiavFolOUlJScjOzkZAwIOlxktLS7F3714sX74chYWFMDeXb3gYFFS2Qd25c+fQsWNHuLu7a8yCysrKAgC4u7tL/5Yfqxjj6OgIGxsbmJubw9zcXGtM+Tm0USqVUCqVtWkyEZFB7Zg22NhVIDIZtRqTM3ToUKSmpiIlJUV6BAYGIjw8HCkpKRoJDgCkpKQAANq0aQMACA4ORmpqqmwWVHx8PBwdHeHr6yvFJCQkyM4THx+P4OBgAICVlRX69u0ri1Gr1UhISJBiGiPeYSciIjKcWvXkODg4wM/PT3bMzs4OLi4u8PPzw/nz57F+/XqMHDkSLi4uOH78OKZOnYpBgwZJU82HDRsGX19fjB07FosWLUJmZibmzJmDiIgIqZdl0qRJWL58OaZPn45XX30VO3fuxA8//IC4uAf7HU2bNg3jxo1DYGAg+vXrh6VLl+Lu3bsYP358fd+TBsMxOURERIaj1xWPrayssGPHDinh8PLywjPPPIM5c+ZIMebm5ti8eTPefPNNBAcHw87ODuPGjcN7770nxfj4+CAuLg5Tp07FsmXL4Onpia+++gqhoaFSzPPPP48bN25g3rx5yMzMRO/evbF161aNwchERETUPHHvKgPuXdW3fQv8V8e9q4iIiEg7XT+/67WtAxEREVFjxSTHgDjwmIiIyHCY5BhQs70vSEREZARMcoiIiMgkMckhIiIik8Qkh4iIiEwSkxwiIiIySUxyiIiIyCQxySEiIiKTxCSHiIiITBKTHCIiIjJJTHIMqBlvE0ZERGRwTHKIiIjIJDHJMSCFgrtXERERGQqTHCIiIjJJTHKIiIjIJDHJMSAOPCYiIjIcJjlERERkkpjkEBERkUlikmNAnF1FRERkOExyDIhjcoiIiAyHSQ4RERGZJCY5REREZJKY5BAREZFJYpJDREREJolJDhEREZkkJjlERERkkpjkEBERkUlikkNEREQmiUmOAXEpQCIiIsNhkkNEREQmiUmOAXHnKiIiIsNhkkNEREQmiUkOERERmSQmOQbEgcdERESGwySHiIiITFK9kpyYmBgoFApERkZKxwoKChAREQEXFxfY29vjmWeeQVZWlux1V65cQVhYGGxtbeHq6oqoqCiUlJTIYnbv3o2AgAAolUp06tQJa9eu1bj+ihUr4O3tDWtrawQFBeHQoUP1aU6D48BjIiIiw6lzknP48GGsWrUKPXv2lB2fOnUqfv31V2zcuBF79uzBtWvX8PTTT0vlpaWlCAsLQ1FREfbv349vv/0Wa9euxbx586SYixcvIiwsDEOGDEFKSgoiIyPx2muvYdu2bVLMhg0bMG3aNMyfPx/Jycno1asXQkNDkZ2dXdcmERERkSkRdZCXlyc6d+4s4uPjxeDBg8WUKVOEEELk5OQIS0tLsXHjRin21KlTAoBITEwUQgixZcsWYWZmJjIzM6WYL774Qjg6OorCwkIhhBDTp08XPXr0kF3z+eefF6GhodLzfv36iYiICOl5aWmp8PDwENHR0Tq3Izc3VwAQubm5uje+DtrP2Czaz9gsnlqxr0GvQ0RE1Bzo+vldp56ciIgIhIWFISQkRHY8KSkJxcXFsuPdunVDu3btkJiYCABITEyEv78/3NzcpJjQ0FCoVCqcOHFCiql87tDQUOkcRUVFSEpKksWYmZkhJCREitGmsLAQKpVK9jAkDjwmIiIyHIvaviA2NhbJyck4fPiwRllmZiasrKzg7OwsO+7m5obMzEwppmKCU15eXlZdjEqlwv3793Hnzh2UlpZqjTl9+nSVdY+Ojsa7776rW0OJiIioSatVT05GRgamTJmCdevWwdrauqHq1GBmzZqF3Nxc6ZGRkWHsKhEREVEDqVWSk5SUhOzsbAQEBMDCwgIWFhbYs2cPPv30U1hYWMDNzQ1FRUXIycmRvS4rKwvu7u4AAHd3d43ZVuXPa4pxdHSEjY0NWrVqBXNzc60x5efQRqlUwtHRUfYwJM6uIiIiMpxaJTlDhw5FamoqUlJSpEdgYCDCw8Olry0tLZGQkCC9Jj09HVeuXEFwcDAAIDg4GKmpqbJZUPHx8XB0dISvr68UU/Ec5THl57CyskLfvn1lMWq1GgkJCVJMY8QxOURERIZTqzE5Dg4O8PPzkx2zs7ODi4uLdHzChAmYNm0aWrZsCUdHR/zzn/9EcHAwHn74YQDAsGHD4Ovri7Fjx2LRokXIzMzEnDlzEBERAaVSCQCYNGkSli9fjunTp+PVV1/Fzp078cMPPyAuLk667rRp0zBu3DgEBgaiX79+WLp0Ke7evYvx48fX6w0hIiIi01Drgcc1WbJkCczMzPDMM8+gsLAQoaGh+Pzzz6Vyc3NzbN68GW+++SaCg4NhZ2eHcePG4b333pNifHx8EBcXh6lTp2LZsmXw9PTEV199hdDQUCnm+eefx40bNzBv3jxkZmaid+/e2Lp1q8ZgZCIiImqeFEKIZnsXRaVSwcnJCbm5uQ06Psd7ZlkPVJ92ztj0j0ca7DpERETNga6f39y7ioiIiEwSkxwiIiIySUxyiIiIyCQxySEiIiKTxCSHiIiITBKTHANqvvPYiIiIDI9JDhEREZkkJjkGpODmVURERAbDJIeIiIhMEpMcIiIiMklMcgyIA4+JiIgMh0kOERERmSQmOURERGSSmOQYEGdXERERGQ6THCIiIjJJTHIMiAOPiYiIDIdJDhEREZkkJjlERERkkpjkGBAHHhMRERkOkxwD4pgcIiIiw2GSQ0RERCaJSQ4RERGZJCY5REREZJKY5BAREZFJYpJDREREJolJDhEREZkkJjlERERkkpjkEBERkUlikmNAXAuQiIjIcJjkEBERkUlikmNA3LqKiIjIcJjkEBERkUlikkNEREQmiUmOAXHgMRERkeEwySEiIiKTxCSHiIiITFKtkpwvvvgCPXv2hKOjIxwdHREcHIzffvtNKn/00UehUChkj0mTJsnOceXKFYSFhcHW1haurq6IiopCSUmJLGb37t0ICAiAUqlEp06dsHbtWo26rFixAt7e3rC2tkZQUBAOHTpUm6YYBWdXERERGU6tkhxPT0/ExMQgKSkJR44cwWOPPYa///3vOHHihBQzceJEXL9+XXosWrRIKistLUVYWBiKioqwf/9+fPvtt1i7di3mzZsnxVy8eBFhYWEYMmQIUlJSEBkZiddeew3btm2TYjZs2IBp06Zh/vz5SE5ORq9evRAaGors7Oz6vBcNjmNyiIiIDEchhKjXZ2/Lli2xePFiTJgwAY8++ih69+6NpUuXao397bff8Le//Q3Xrl2Dm5sbAGDlypWYMWMGbty4ASsrK8yYMQNxcXFIS0uTXvfCCy8gJycHW7duBQAEBQXhoYcewvLlywEAarUaXl5e+Oc//4mZM2fqXHeVSgUnJyfk5ubC0dGxju9AzbxnxgEAenk545eIRxrsOkRERM2Brp/fdR6TU1paitjYWNy9exfBwcHS8XXr1qFVq1bw8/PDrFmzcO/ePaksMTER/v7+UoIDAKGhoVCpVFJvUGJiIkJCQmTXCg0NRWJiIgCgqKgISUlJshgzMzOEhIRIMVUpLCyESqWSPYiIiMg0WdT2BampqQgODkZBQQHs7e2xadMm+Pr6AgBefPFFtG/fHh4eHjh+/DhmzJiB9PR0/PTTTwCAzMxMWYIDQHqemZlZbYxKpcL9+/dx584dlJaWao05ffp0tXWPjo7Gu+++W9smExERURNU6ySna9euSElJQW5uLn788UeMGzcOe/bsga+vL15//XUpzt/fH23atMHQoUNx/vx5dOzYUa8Vr4tZs2Zh2rRp0nOVSgUvLy8j1oiIiIgaSq2THCsrK3Tq1AkA0LdvXxw+fBjLli3DqlWrNGKDgoIAAOfOnUPHjh3h7u6uMQsqKysLAODu7i79W36sYoyjoyNsbGxgbm4Oc3NzrTHl56iKUqmEUqmsRWuJiIioqar3OjlqtRqFhYVay1JSUgAAbdq0AQAEBwcjNTVVNgsqPj4ejo6O0i2v4OBgJCQkyM4THx8vjfuxsrJC3759ZTFqtRoJCQmysUFERETUvNWqJ2fWrFkYMWIE2rVrh7y8PKxfvx67d+/Gtm3bcP78eaxfvx4jR46Ei4sLjh8/jqlTp2LQoEHo2bMnAGDYsGHw9fXF2LFjsWjRImRmZmLOnDmIiIiQelgmTZqE5cuXY/r06Xj11Vexc+dO/PDDD4iLi5PqMW3aNIwbNw6BgYHo168fli5dirt372L8+PF6fGuIiIioKatVkpOdnY2XX34Z169fh5OTE3r27Ilt27bh8ccfR0ZGBnbs2CElHF5eXnjmmWcwZ84c6fXm5ubYvHkz3nzzTQQHB8POzg7jxo3De++9J8X4+PggLi4OU6dOxbJly+Dp6YmvvvoKoaGhUszzzz+PGzduYN68ecjMzETv3r2xdetWjcHIRERE1HzVe52cpozr5BARETU9Db5ODhEREVFjxiSHiIiITBKTHCIiIjJJTHKIiIjIJDHJMaTmO8abiIjI4JjkEBERkUlikmNICoWxa0BERNRsMMkhIiIik8Qkh4iIiEwSkxxD4sBjIiIig2GSQ0RERCaJSQ4RERGZJCY5hsTZVURERAbDJMeQOCaHiIjIYJjkEBERkUlikkNEREQmiUkOERERmSQmOURERGSSmOQQERGRSWKSQ0RERCaJSQ4RERGZJCY5REREZJKY5BiQgiseExERGQyTHAN4spcHzBTASw+3N3ZViIiImg2FEM13rwGVSgUnJyfk5ubC0dGxwa4jhMDdolLYKy0a7BpERETNha6f3+zJMQCFQsEEh4iIyMCY5BAREZFJYpJDREREJolJDhEREZkkJjlERERkkpr1aNjyiWUqlcrINSEiIiJdlX9u1zRBvFknOXl5eQAALy8vI9eEiIiIaisvLw9OTk5VljfrdXLUajWuXbsGBwcHva5GrFKp4OXlhYyMjAZdf6cxas5tB5p3+9n25tl2oHm3n203TtuFEMjLy4OHhwfMzKoeedOse3LMzMzg6enZYOd3dHRsdv/pyzXntgPNu/1se/NsO9C828+2G77t1fXglOPAYyIiIjJJTHKIiIjIJDHJaQBKpRLz58+HUqk0dlUMrjm3HWje7Wfbm2fbgebdfra9cbe9WQ88JiIiItPFnhwiIiIySUxyiIiIyCQxySEiIiKTxCSHiIiITBKTnAawYsUKeHt7w9raGkFBQTh06JCxq1St6OhoPPTQQ3BwcICrqytGjRqF9PR0WUxBQQEiIiLg4uICe3t7PPPMM8jKypLFXLlyBWFhYbC1tYWrqyuioqJQUlIii9m9ezcCAgKgVCrRqVMnrF27VqM+xnz/YmJioFAoEBkZKR0z5bb/+eefeOmll+Di4gIbGxv4+/vjyJEjUrkQAvPmzUObNm1gY2ODkJAQnD17VnaO27dvIzw8HI6OjnB2dsaECROQn58vizl+/DgGDhwIa2treHl5YdGiRRp12bhxI7p16wZra2v4+/tjy5YtDdNoAKWlpZg7dy58fHxgY2ODjh074v3335ftg2NKbd+7dy+eeOIJeHh4QKFQ4Oeff5aVN6a26lIXfbW9uLgYM2bMgL+/P+zs7ODh4YGXX34Z165dM4m219T+yiZNmgSFQoGlS5fKjjfl9kOQXsXGxgorKyuxZs0aceLECTFx4kTh7OwssrKyjF21KoWGhopvvvlGpKWliZSUFDFy5EjRrl07kZ+fL8VMmjRJeHl5iYSEBHHkyBHx8MMPi/79+0vlJSUlws/PT4SEhIijR4+KLVu2iFatWolZs2ZJMRcuXBC2trZi2rRp4uTJk+Kzzz4T5ubmYuvWrVKMMd+/Q4cOCW9vb9GzZ08xZcoUk2/77du3Rfv27cUrr7wiDh48KC5cuCC2bdsmzp07J8XExMQIJycn8fPPP4tjx46JJ598Uvj4+Ij79+9LMcOHDxe9evUSBw4cEL///rvo1KmTGDNmjFSem5sr3NzcRHh4uEhLSxPff/+9sLGxEatWrZJi/vjjD2Fubi4WLVokTp48KebMmSMsLS1Fampqg7T9ww8/FC4uLmLz5s3i4sWLYuPGjcLe3l4sW7bMJNu+ZcsWMXv2bPHTTz8JAGLTpk2y8sbUVl3qoq+25+TkiJCQELFhwwZx+vRpkZiYKPr16yf69u0rO0dTbXtN7a/op59+Er169RIeHh5iyZIlJtN+Jjl61q9fPxERESE9Ly0tFR4eHiI6OtqItaqd7OxsAUDs2bNHCFH2i8DS0lJs3LhRijl16pQAIBITE4UQZT9IZmZmIjMzU4r54osvhKOjoygsLBRCCDF9+nTRo0cP2bWef/55ERoaKj031vuXl5cnOnfuLOLj48XgwYOlJMeU2z5jxgwxYMCAKsvVarVwd3cXixcvlo7l5OQIpVIpvv/+eyGEECdPnhQAxOHDh6WY3377TSgUCvHnn38KIYT4/PPPRYsWLaT3ovzaXbt2lZ6PHj1ahIWFya4fFBQk3njjjfo1sgphYWHi1VdflR17+umnRXh4uBDCtNte+YOuMbVVl7ros+3aHDp0SAAQly9fFkKYTtuFqLr9V69eFW3bthVpaWmiffv2siSnqbeft6v0qKioCElJSQgJCZGOmZmZISQkBImJiUasWe3k5uYCAFq2bAkASEpKQnFxsaxd3bp1Q7t27aR2JSYmwt/fH25ublJMaGgoVCoVTpw4IcVUPEd5TPk5jPn+RUREICwsTKN+ptz2//3vfwgMDMRzzz0HV1dX9OnTB6tXr5bKL168iMzMTFmdnJycEBQUJGu7s7MzAgMDpZiQkBCYmZnh4MGDUsygQYNgZWUla3t6ejru3LkjxVT3/uhb//79kZCQgDNnzgAAjh07hn379mHEiBEATLvtlTWmtupSl4aWm5sLhUIBZ2dnqc6m3Ha1Wo2xY8ciKioKPXr00Chv6u1nkqNHN2/eRGlpqezDDgDc3NyQmZlppFrVjlqtRmRkJB555BH4+fkBADIzM2FlZSX90Jer2K7MzEyt7S4vqy5GpVLh/v37Rnv/YmNjkZycjOjoaI0yU277hQsX8MUXX6Bz587Ytm0b3nzzTbz11lv49ttvZXWvrk6ZmZlwdXWVlVtYWKBly5Z6eX8aqu0zZ87ECy+8gG7dusHS0hJ9+vRBZGQkwsPDZfUyxbZX1pjaqktdGlJBQQFmzJiBMWPGSBtOmnrb//3vf8PCwgJvvfWW1vKm3v5mvQs5aYqIiEBaWhr27dtn7KoYREZGBqZMmYL4+HhYW1sbuzoGpVarERgYiIULFwIA+vTpg7S0NKxcuRLjxo0zcu0a1g8//IB169Zh/fr16NGjB1JSUhAZGQkPDw+TbztpV1xcjNGjR0MIgS+++MLY1TGIpKQkLFu2DMnJyVAoFMauToNgT44etWrVCubm5hozb7KysuDu7m6kWulu8uTJ2Lx5M3bt2gVPT0/puLu7O4qKipCTkyOLr9gud3d3re0uL6suxtHRETY2NkZ5/5KSkpCdnY2AgABYWFjAwsICe/bswaeffgoLCwu4ubmZbNvbtGkDX19f2bHu3bvjypUrsrpXVyd3d3dkZ2fLyktKSnD79m29vD8N1faoqCipN8ff3x9jx47F1KlTpd48U257ZY2prbrUpSGUJziXL19GfHy81ItTXidTbfvvv/+O7OxstGvXTvr9d/nyZbz99tvw9vaW6tWU288kR4+srKzQt29fJCQkSMfUajUSEhIQHBxsxJpVTwiByZMnY9OmTdi5cyd8fHxk5X379oWlpaWsXenp6bhy5YrUruDgYKSmpsp+GMp/WZR/kAYHB8vOUR5Tfg5jvH9Dhw5FamoqUlJSpEdgYCDCw8Olr0217Y888ojGUgFnzpxB+/btAQA+Pj5wd3eX1UmlUuHgwYOytufk5CApKUmK2blzJ9RqNYKCgqSYvXv3ori4WIqJj49H165d0aJFCymmuvdH3+7duwczM/mvP3Nzc6jVagCm3fbKGlNbdamLvpUnOGfPnsWOHTvg4uIiKzflto8dOxbHjx+X/f7z8PBAVFQUtm3bZhrtr/OQZdIqNjZWKJVKsXbtWnHy5Enx+uuvC2dnZ9nMm8bmzTffFE5OTmL37t3i+vXr0uPevXtSzKRJk0S7du3Ezp07xZEjR0RwcLAIDg6WysunUQ8bNkykpKSIrVu3itatW2udRh0VFSVOnTolVqxYoXUatbHfv4qzq4Qw3bYfOnRIWFhYiA8//FCcPXtWrFu3Ttja2orvvvtOiomJiRHOzs7il19+EcePHxd///vftU4t7tOnjzh48KDYt2+f6Ny5s2x6aU5OjnBzcxNjx44VaWlpIjY2Vtja2mpML7WwsBAfffSROHXqlJg/f36DTiEfN26caNu2rTSF/KeffhKtWrUS06dPN8m25+XliaNHj4qjR48KAOKTTz4RR48elWYQNaa26lIXfbW9qKhIPPnkk8LT01OkpKTIfv9VnCnUVNteU/u1qTy7qqm3n0lOA/jss89Eu3bthJWVlejXr584cOCAsatULQBaH998840Uc//+ffGPf/xDtGjRQtja2oqnnnpKXL9+XXaeS5cuiREjRggbGxvRqlUr8fbbb4vi4mJZzK5du0Tv3r2FlZWV6NChg+wa5Yz9/lVOcky57b/++qvw8/MTSqVSdOvWTXz55ZeycrVaLebOnSvc3NyEUqkUQ4cOFenp6bKYW7duiTFjxgh7e3vh6Ogoxo8fL/Ly8mQxx44dEwMGDBBKpVK0bdtWxMTEaNTlhx9+EF26dBFWVlaiR48eIi4uTv8N/otKpRJTpkwR7dq1E9bW1qJDhw5i9uzZsg82U2r7rl27tP6Mjxs3rtG1VZe66KvtFy9erPL3365du5p822tqvzbakpym3H6FEBWW+CQiIiIyERyTQ0RERCaJSQ4RERGZJCY5REREZJKY5BAREZFJYpJDREREJolJDhEREZkkJjlERERkkpjkEBERkUlikkNEREQmiUkOERERmSQLY1fAmNRqNa5duwYHBwcoFApjV4eIiIh0IIRAXl4ePDw8YGZWdX9Ns05yrl27Bi8vL2NXg4iIiOogIyMDnp6eVZY36yTHwcEBQNmb5OjoaOTaEBERkS5UKhW8vLykz/GqNOskp/wWlaOjI5McIiKiJqamoSYceExEREQmiUkOERERmSQmOURERGSSmOQQERFRjUrVAgXFpcauRq0wySEiIqIadXxnC7rN3Yq8gmJjV0VnTHKIiIioWlmqAunrgxduG7EmtcMkh4iIiKp1LCNH+rqaBYYbnXpVNSYmBgqFApGRkdKxzMxMjB07Fu7u7rCzs0NAQAD++9//yl53+/ZthIeHw9HREc7OzpgwYQLy8/NlMcePH8fAgQNhbW0NLy8vLFq0SOP6GzduRLdu3WBtbQ1/f39s2bKlPs0hIiIiLQ5efNB7c+p6nhFrUjt1TnIOHz6MVatWoWfPnrLjL7/8MtLT0/G///0PqampePrppzF69GgcPXpUigkPD8eJEycQHx+PzZs3Y+/evXj99delcpVKhWHDhqF9+/ZISkrC4sWLsWDBAnz55ZdSzP79+zFmzBhMmDABR48exahRozBq1CikpaXVtUlERESkRXGpWvp68bZ03L5bZMTa1IKog7y8PNG5c2cRHx8vBg8eLKZMmSKV2dnZif/85z+y+JYtW4rVq1cLIYQ4efKkACAOHz4slf/2229CoVCIP//8UwghxOeffy5atGghCgsLpZgZM2aIrl27Ss9Hjx4twsLCZNcJCgoSb7zxhs7tyM3NFQBEbm6uzq8hIiJqboZ9ske0n7FZemxNu27U+uj6+V2nnpyIiAiEhYUhJCREo6x///7YsGEDbt++DbVajdjYWBQUFODRRx8FACQmJsLZ2RmBgYHSa0JCQmBmZoaDBw9KMYMGDYKVlZUUExoaivT0dNy5c0eKqXz90NBQJCYmVlnvwsJCqFQq2YOIiIiql54lv0V1I6/QSDWpnVonObGxsUhOTkZ0dLTW8h9++AHFxcVwcXGBUqnEG2+8gU2bNqFTp04AysbsuLq6yl5jYWGBli1bIjMzU4pxc3OTxZQ/rymmvFyb6OhoODk5SQ/uQE5ERFQ9IYTGsTk/p6FUrXm8salVkpORkYEpU6Zg3bp1sLa21hozd+5c5OTkYMeOHThy5AimTZuG0aNHIzU1VS8Vro9Zs2YhNzdXemRkZBi7SkRERI3a1Tv3tR7v+M4WeM+Mw/7zN2WJkBBCa2JkDLXahTwpKQnZ2dkICAiQjpWWlmLv3r1Yvnw50tPTsXz5cqSlpaFHjx4AgF69euH333/HihUrsHLlSri7uyM7O1t23pKSEty+fRvu7u4AAHd3d2RlZcliyp/XFFNero1SqYRSqaxNk4mIiEzSkUu3cftuEYb1qPpzEwDUNSQsL64uG2pyYeFIZOUVIDh6JwDgYvTIGncJb2i1SnKGDh2q0SMzfvx4dOvWDTNmzMC9e/cAAGaVJtGbm5tDrS4bmR0cHIycnBwkJSWhb9++AICdO3dCrVYjKChIipk9ezaKi4thaWkJAIiPj0fXrl3RokULKSYhIUE2fT0+Ph7BwcG1aRIREVGzU1SixrMry8aw+rV1xOZ/DpTKvGfGSV+/9VgnlOh4W2rT0T/x9sZj0vM3v0vGyrF99VTjulGIevYpPfroo+jduzeWLl2K4uJi+Pr6ok2bNvjoo4/g4uKCn3/+GVFRUdi8eTNGjhwJABgxYgSysrKwcuVKFBcXY/z48QgMDMT69esBALm5uejatSuGDRuGGTNmIC0tDa+++iqWLFkiTTXfv38/Bg8ejJiYGISFhSE2NhYLFy5EcnIy/Pz8dKq7SqWCk5MTcnNz4ejoWJ+3gYiIqMn4JeVPTIlNkZ6/0t8bC57sIUtw9KGbuwPi3hoIczP99ujo+vmt13ULLS0tsWXLFrRu3RpPPPEEevbsif/85z/49ttvpQQHANatW4du3bph6NChGDlyJAYMGCBbA8fJyQnbt2/HxYsX0bdvX7z99tuYN2+ebC2d/v37Y/369fjyyy/Rq1cv/Pjjj/j55591TnCIiIiaq6NXcmTP1+6/hJSMHK2xFXVxs0dYzzY6X+d0Zh4u3MivObCB1LsnpyljTw4RETVHr317GDtOZdccWMmlmDDpa7VaIHJDCv537Fq1rzk8OwStHfQ7HtYoPTlERETU+NUlwanMzEyBT8f0qTGulb1VjTENhUkOERERySx8yl8v5wkPamfUGVa1ml1FREREpu/FoHawMFNg+n+P1+p1vbycsfaVh9DCzni9NxWxJ4eIiKgZ2z/zMdnzQ+8MBQCMfki+K8Ch2UNrPNe34xtPggOwJ4eIiMjkFZeq8b+Ua+jWxgFhn+6Tjnd1c4CHsw0OzBqK41dzql0Y0NVB+04HFTnbNp4EB2CSQ0REZPI6z/5N6/GHfMoW2HV3soa7k2aCk/ZuKHanZ2NIV1eNsnLDfN2w/WRWleXGxNtVREREJuzolTtVls1/oke1r7VXWuBvPT1gp6y6T+SVR7zrWrUGx54cIiIiE/bU5/urLLM0r39fR/+OrfDl2L7o6Gpf73PpG5McIiIiE6NWC1y8dRdv/3Cs5mA9qGmTT2NhkkNERGRier+3HaqCkmpjwoPaGag2xsMkh4iIyMRUl+CkLhgGOysLmOl508zGiEkOERGRCblbqD3BiQrtiuF+7nCwtjRwjYyHSQ4REZEJqWrDzIghnQxcE+PjFHIiIiIT4u1ip3HsnZHdjFAT42OSQ0REZEKOXc3RODZxYAfDV6QRYJJDRERkQmJ+O61xzJg7gRsTkxwiIiIySUxyiIiItBi35hC8Z8YhJSPH2FWR+XLveXjPjMOirWU9Nmey8lBQXAoAEEJoxK98qa9B69eYcHYVERGRFnvO3AAAjFrxBy7FhBm5Ng8s3FKW3Hy++zzaOFlj7i8npLLhlVYePvvhCL1s3dBUNd+WExER6VFmboHWnpS6SDx/Czn3igAAhSWluH237OvK56+Y4ADA1hOZsufNOcEBmOQQERHV26o95/FwdAK+2HO+3udavfcCxqw+gN7vxQMAus7ZioD34xF76Ap8Zm3R+TxH5oTUuy5NXb2SnJiYGCgUCkRGRgIALl26BIVCofWxceNG6XXaymNjY2Xn3r17NwICAqBUKtGpUyesXbtW4/orVqyAt7c3rK2tERQUhEOHDtWnOURERHUS/deMpkVb02v92pJSNe781VMDAB9uOSV9ffXOPenrmT+l1uq8js1oZeOq1DnJOXz4MFatWoWePXtKx7y8vHD9+nXZ491334W9vT1GjBghe/0333wjixs1apRUdvHiRYSFhWHIkCFISUlBZGQkXnvtNWzbtk2K2bBhA6ZNm4b58+cjOTkZvXr1QmhoKLKzs+vaJCIiIgBliYehdJr9G/q8H49LN+9qlA349646n9eiGexNVZM6JTn5+fkIDw/H6tWr0aJFC+m4ubk53N3dZY9NmzZh9OjRsLe3l53D2dlZFmdtbS2VrVy5Ej4+Pvj444/RvXt3TJ48Gc8++yyWLFkixXzyySeYOHEixo8fD19fX6xcuRK2trZYs2ZNXZpEREQk6TT7N4Ncp+ucB9f5aHu6XpOr5rABZ03qlOREREQgLCwMISHV3+9LSkpCSkoKJkyYoPUcrVq1Qr9+/bBmzRrZYKrExESNc4eGhiIxMREAUFRUhKSkJFmMmZkZQkJCpBgiIiJ9ybh9r+YgHaRk5OClrw7i4l+9NoUlD5Kazcev1zm56uXlLHt+YNbQOtfRlNR6CnlsbCySk5Nx+PDhGmO//vprdO/eHf3795cdf++99/DYY4/B1tYW27dvxz/+8Q/k5+fjrbfeAgBkZmbCzc1N9ho3NzeoVCrcv38fd+7cQWlpqdaY06c1V3osV1hYiMLCQum5SqWqsQ1ERERX79yHV0tbrWW+87bKngshqlxheNSKPwAAQz7ajQ+f8qtzfYb3cEff9i1QWFKKzm4OCO3hjozb9/CfxEt4dYAP3J2saz5JM1CrJCcjIwNTpkxBfHy87PaSNvfv38f69esxd+5cjbKKx/r06YO7d+9i8eLFUpLTUKKjo/Huu+826DWIiMj0/CfxEoI7umgtu1dUKnteqhYwU9R8u2j2prQ61SX9g+FQWphrHPdqaYvZYb51OqepqtXtqqSkJGRnZyMgIAAWFhawsLDAnj178Omnn8LCwgKlpQ++0T/++CPu3buHl19+ucbzBgUF4erVq1Ivi7u7O7KysmQxWVlZcHR0hI2NDVq1agVzc3OtMe7u8oWQKpo1axZyc3OlR0ZGRm2aT0REzdRvaZm4XymZqUqn2b+hwztb8Me5mwDK1s9ZvfcC1Gr9rKGjLcEh7WqV5AwdOhSpqalISUmRHoGBgQgPD0dKSgrMzR+88V9//TWefPJJtG7dusbzpqSkoEWLFlAqlQCA4OBgJCQkyGLi4+MRHBwMALCyskLfvn1lMWq1GgkJCVKMNkqlEo6OjrIHERGRLrpXui116eZdacE+bcK/Ooib+YV4ODoBH245hQ7v6L7GDQBcignD6feH46d/9K85mLSq1e0qBwcH+PnJ7yHa2dnBxcVFdvzcuXPYu3cvtmzR/Ib++uuvyMrKwsMPPwxra2vEx8dj4cKF+Ne//iXFTJo0CcuXL8f06dPx6quvYufOnfjhhx8QFxcnxUybNg3jxo1DYGAg+vXrh6VLl+Lu3bsYP358bZpERESkM++ZcTUHVRD4wY56Xc/a0hwB7Vo0qm0lmpIG2btqzZo18PT0xLBhwzTKLC0tsWLFCkydOhVCCHTq1EmaDl7Ox8cHcXFxmDp1KpYtWwZPT0989dVXCA0NlWKef/553LhxA/PmzUNmZiZ69+6NrVu3agxGJiIiqqtFz/TE9P8eN/h1fdvwToM+KIS+NtpoglQqFZycnJCbm8tbV0REBADIzitAvw/LhkMkznoMwdE7G/yarg5KHJodgh+OZOCr3y9gW+SgKmdoke6f39y7ioiI6syQKwMbwr2iEqzYeU563tLOCrGvP9yg1xzV2wN7pw8BAIwO9ML2qYOZ4OgJkxwiIqqTL/eeR6fZv8F7ZhyyVQXGro5e+M7bhm8TL0vPlRbmCPJpqRH3z8c6ycbJDO7SGqffH16na84O84W1JWdMNYQGGZNDRESm7dLNu1i45cHiq/0WJpjs4FiFQoGVLwXg+0MZWDW2rywhuRQThsKSUq3TutM/GI6uc8pmZK1+ORDOtpbwb+uEbnPls7QcbfhR3FD4zhIRUa2UlKrx6Ee7NY6rCoqb7M7XhSWlUkKizXC/Nhju10ZrWVXr1igtzHVK/JrvyNiGx9tVRERUK4/8W/tA3Ge/2G/gmtRfcakaPRdsqzbBaWhW5vwobih8Z4mIqFayVIVaj5/JyjdwTervkZidUBWU6O18S57vBQCYGtKlypjRgZ7S16E93LhbeAPi7SoiImq2svO0J2x19VQfTzzVx7PamIVP+aONkw2GdndFT09nvV6f5NiTQ0REeuM9Mw7xJ7NqDmwEio00/d3C3AxTH+/CBMcAmOQQEVGt1DSGZOJ/jsB7Zhz2nLlhoBrVzXcHLmsce7RrzfstUtPBJIeIiHTmN38binTsARm35lAD16Z+/pt8VePY9NBuRqgJNRQmOUREpNXxqznwnhmHl746KB3LL5QP0l36fG88HdC2ynNUjm9M0v5UaRxTWprhk9G90KGVHc5+OMIItSJ9YpJDREQaSkrVeHL5HwCAfeduVhk3qk/batfG8Zu/Te91q62xXx+E98w4FBSXAihLvLKqWKG5Y2t7PB3giZ3/ehSWnNrd5HF2FRERaeg0+zfZ8zNZeRi2ZK/s2Pt/7wEAmDmiG9buvwQAuBg9EkIAHd7ZIsXN+TkV3x24gqjQrogY0qlhK16JWi3w+9myJK3ySsNk+pimEhGRjNCyBG/lBAd4sNKvtaU5zn04AhejR0KhUGis+/LdgSsAgMXb0rVeL/deMY5euaP1uvU1++dUvZ+Tmg725BARkcz5G7ot6rfu0BWMfsgLQNm06Lrq9d52AEB7F1vsiRpS5/MAZT03CgWkXby/P5Sh0+u6uNlj+YsB9bo2NT5McoiISCbkE81eG208W9jU+tzeM+MAAMEdXPD96w/Lyi7fulfr81VUqhbo+NdtsgsLR+q8kvCo3h5Y+kKfel2bGicmOUREJPmjmkHGlU0c2KHO10m8cEtKePTl/c0npa8PXLiF4I4uWuPmP+GLccHe2HP2Bnp5OqOlnZVe60GNB5McIiKShFeYLg4AR+aEIPCDHVpjbSy1775tLOWDnwHgxUrteDqgLT4Z3Vt2bEhXVwPUioyJA4+JiKhKreyV6F9Fj4iFedW3gwZ1ebBycFRoV73Xq7Z+Sv7T2FUgI2BPDhERaXUpJgwAsH5i2diZyreXvFrYVvnab155SBof849HO2qdOq7v21VElbEnh4iI6sTKouqPEHMzBS7FhOFSTJg006myCwtHop9Pyzpfv7CkFCV/bTFxLCOn2tiR/u51vg41XUxyiIio1l4Obl/vc5iZKfDDG8HY/a9Ha/3aohI1er27HY9+tBsA8PcVf1Qb/9kYTg9vjuqV5MTExEChUCAyMhIAcOnSJSgUCq2PjRs3Sq+7cuUKwsLCYGtrC1dXV0RFRaGkRL6/ye7duxEQEAClUolOnTph7dq1GtdfsWIFvL29YW1tjaCgIBw61Lg3gyMiaqy8Z8bVePvI2+XB7al3n+yht2ubV5jq7T0zDpdv3dWIyS8swenMB3tNbUm9joJiNa7eua9R75kjNDfZNNdxOjmZljonOYcPH8aqVavQs2dP6ZiXlxeuX78ue7z77ruwt7fHiBFlG52VlpYiLCwMRUVF2L9/P7799lusXbsW8+bNk85z8eJFhIWFYciQIUhJSUFkZCRee+01bNv2YA+UDRs2YNq0aZg/fz6Sk5PRq1cvhIaGIjs7u65NIiKiavwSMQBzwrrj0DtDq7wFVReVE5DBi3dLX9/ML4T3zDj4zd+G4Ut/R9DCHVCrBSI3pFR5vspT288vHKm3ulLTUqckJz8/H+Hh4Vi9ejVatGghHTc3N4e7u7vssWnTJowePRr29vYAgO3bt+PkyZP47rvv0Lt3b4wYMQLvv/8+VqxYgaKiIgDAypUr4ePjg48//hjdu3fH5MmT8eyzz2LJkiXStT755BNMnDgR48ePh6+vL1auXAlbW1usWbOmPu8HEVGzo63nRBsnW0u8NrADXB2t9Xr94r/G1VSU9mcuxn59UGP6epaqULYvljbmZgqE9nADAPTv6MJenGasTklOREQEwsLCEBISUm1cUlISUlJSMGHCBOlYYmIi/P394ebmJh0LDQ2FSqXCiRMnpJjK5w4NDUViYiIAoKioCElJSbIYMzMzhISESDHaFBYWQqVSyR5ERM1demaexrH/vhlssOubaekV+ttn+6SNNeti0bO9EP20Pz4P51ic5qzWSU5sbCySk5MRHR1dY+zXX3+N7t27o3///tKxzMxMWYIDQHqemZlZbYxKpcL9+/dx8+ZNlJaWao0pP4c20dHRcHJykh5eXl41toGIyNTdulukcayHh5PBru9sa1nn127+5wCtx51sLDGmXzs423I14+asVklORkYGpkyZgnXr1sHauvruyvv372P9+vWyXhxjmzVrFnJzc6VHRoZuG7cREZmyO/c0kxxrA65m7GBd+yRnx7RBuBQThm7uDg1QIzIVtVoMMCkpCdnZ2QgIeND9V1pair1792L58uUoLCyEuXnZD8aPP/6Ie/fu4eWXX5adw93dXWMWVFZWllRW/m/5sYoxjo6OsLGxgbm5OczNzbXGlJ9DG6VSCaVSWZsmExGZvNx7xcauAg69MxT9FiZUG3Ns3jA4Ver1qbz7+ZLne+m9btR01aonZ+jQoUhNTUVKSor0CAwMRHh4OFJSUqQEByi7VfXkk0+idevWsnMEBwcjNTVVNgsqPj4ejo6O8PX1lWISEuT/2ePj4xEcXHaP2MrKCn379pXFqNVqJCQkSDFERKSbyj05rg6G/2OwpsHMl2LCNBIcbb7ed1FfVSITUKueHAcHB/j5+cmO2dnZwcXFRXb83Llz2Lt3L7Zs0RwBP2zYMPj6+mLs2LFYtGgRMjMzMWfOHEREREi9LJMmTcLy5csxffp0vPrqq9i5cyd++OEHxMU9WAth2rRpGDduHAIDA9GvXz8sXboUd+/exfjx42v1BhARNXe37z7oyTm/cGSTno3U3sXO2FWgRqRB9q5as2YNPD09MWzYMI0yc3NzbN68GW+++SaCg4NhZ2eHcePG4b333pNifHx8EBcXh6lTp2LZsmXw9PTEV199hdDQUCnm+eefx40bNzBv3jxkZmaid+/e2Lp1q8ZgZKKmQgiBG/mFaG2v1OsaJEQ1qdiTY8wE57m+ntiYdFXj+OJne2qJ1i5qmPE3A6XGQyGEEMauhLGoVCo4OTkhNzcXjo6Oxq4ONXOPf7IHZ7PzATzYGJHIEB77eDcu3ChbK8eY//f2n7+JF1cflB17a2hnTBnaudrkq+KKx/zZaR50/fzm3lVEjUR5ggMAb/9wzIg1oeYmpxEMPAaA/h1bYfvUQdLz7yc+jGmPd6mxd6m8p6eq6eTUfDXI7Soiqp//Jl/F9OFdEfTXbJNN/+iPPu1a1PAqorq5rWWdHGPp4uaA/5vQDxdu3EVwRxedXvNcoBeeC+S6Z6SJPTlEjVRQhem0T32+Hwu3nDJibchU1bQppzEM7Nwa4/p7G7saZAKY5BA1AvvP17x8/Zd7LxigJmTq/pt0FVvTylaG1+X/HVFTxttVREZ2JitPY7BlVYQQnHlFdZaZW4C3Nz4Y72Wv5EcAmTb25BAZ0YUb+Ri2ZK/O8UVadmsm0lXGnXuy5/mFJbLnfdo5G7A2RA2PSQ6RkQgh8NjHe6qNSV0gX2uqpLTZrvhAenD51r0qy47MCcGmfzxiwNoQNTwmOURGcj23oNryXycPgIO1JX6fPkQ6xiSH6sOvbdXribSy575+ZHp4Q5bIgIQQ+P3sTRSWqFFQXKo1xsXOCo90agV/TycAgGcLG6ms13vbudiZiUrJyMG8X9Jw/Gou3B2t8eXLfdHT01mv17iVr32q+N97e+j1OkSNBZMcIgPampaJN9clVxuTNPdx2fPKA40n/ucIVr8cqPe6kXGNWvGH9HWmqgBPLv8DyXMfR0s7qzqdr1QtIISQ7dId/pX2Ae7LXuhTp2sQNXa8XUVkQPP/d6La8o2Tgms8R/zJLH1VhxqJqtaq2Xz8Wp3Od+FGPjq+swWdZv+G+0VlPYZV7eBTcYVhIlPDJIfIgLLzCqssezqgLR7ybqnTeSasPVxtefRvp/Dx9vRa1Y0an5PXVBBCIOD9eMz66bhOr8nOK5ANaO8+bysAwGfWFq3xXdwc6l9RokaKt6uIGomZI7rpHJtwOrvKspv5hVi1p2zhwJeDvdHagQNKG7NSddWDyWMPZyD2cAYA4PtDGYh+uubduPt9mKBxTFtPEcd2UXPAnhyiRmBv1BC4OlhXWb5/5mMaxx5emIBPtPTWVJyB9e6v1d8eI+P7KfmqzrH/Tao+tqpbUpVxI0tqLpjkEDUC7Vxsqy33cLbROJapKsCnO8+h54JtsuOFJQ9mbW0+fh2PfbRbL3WkhhH1o263oQDg7Y3H8FnCWXjPjMPvZ29olP9YQxIEAKN6e8CvrVOt6kjUVDHJITKQL/eer9frE2dp9uYAgKqgBNdz7+PF1QfgPTMOgxfvlpVfuHlX57/wyfiOzAmptvzj+DMAgLFfH4L3zDjZUgSHLt6WxU4Y4KPx+qWcSUXNCJMcIgNZuOV0vV7fxskGJ94N1VoWHL0T+8/fqvK1f68wPZmM4z+Jl+A9Mw670x+Mp9KWfNZ2Ub5uc7ei5K/tPjZW6MlxtLbA3L/5ysbenH5/eG2rTdSkceAxkZF9+JSfzrF2ddxQ8fjV3Dq9jvRn3i9l46Ne+eawlHgUFGvfi6yFrSXu3CvW+dxBCxPg6yFfzbjiekscZEzNFZMcIiN4pb83FjzZw9jVICMpKlHDysIMt+7KlxTo99cSAuteexgjP/1d5/PduluE38/elB2zNGdHPRGTHCIDqHxbYtZI3aeLV/afV/vh8KXbeHtYV42pwd+Mfwjjv6l+DR0yvi5zftM4lvZuKOz/6qmr3CuT/sFwdJ2zVXq+f+Zj8HC2qXIRwfMLR+qxtkRNF5McIgP45K/BogCw/MU+UFqY1/lcg7q0xqAurQE8uA3x3YHLEEJgSFdX2bE5P6dJrytVC5ibKTRPWEdCCPR+Lx6B7Vvg61ceQqlawEyhuQ1FcxX2V0/Mr5MHwEyH993OSv5/YsWLAYhYX7YFiNLCHP+b/AjiT2Zh2uNdanyP9fl9JmrK6tWfGRMTA4VCgcjISNnxxMREPPbYY7Czs4OjoyMGDRqE+/fvS+Xe3t5QKBSyR0xMjOwcx48fx8CBA2FtbQ0vLy8sWrRI4/obN25Et27dYG1tDX9/f2zZon1FT6o7IQQWbzuNVI7pqJfPdp6Tvm6I3Z5ferg9xgZ7y44drDTTpuM7W7DrdDa++eOiXq7pM2sLcu8XI+F0NrxnxqHjO1vgM2sLZ3IBOJedjxPXVDhxTYUO72zB6FWJNb6mcuIS1rMNzn44Ahejy3pleno64+1hXWtMcOK5TQORpM5JzuHDh7Fq1Sr07ClfgTMxMRHDhw/HsGHDcOjQIRw+fBiTJ0+GmZn8Uu+99x6uX78uPf75z39KZSqVCsOGDUP79u2RlJSExYsXY8GCBfjyyy+lmP3792PMmDGYMGECjh49ilGjRmHUqFFIS0sD6c/7m09hxa7zeGL5PmNXpda2ncjE9B+PVbnbt7H09nI2yHXat9Rce2f82sN499eTWLztNIpL1bhztwj5hSV6ve7k9Uf1er6mKOSTPbLnlad2V/bBKO2Dzy3NzWrdM9aZ2zQQSep0uyo/Px/h4eFYvXo1PvjgA1nZ1KlT8dZbb2HmzJnSsa5du2qcw8HBAe7u7lrPv27dOhQVFWHNmjWwsrJCjx49kJKSgk8++QSvv/46AGDZsmUYPnw4oqKiAADvv/8+4uPjsXz5cqxcubIuzWqWsvMKMGdTGt4f5Qc3R80Vd9fo6a9+Y3jj/5IAAI7WlpjzN1+dXnMrvxBLd5zFUwFtEdCuRYPUS2lhmAGhbw/rguW7zmktW7HrPFbserBuTyt7KxyZ87jWWG2yVAVVlsWlXscK3avZrFUch6MPu//1qN7ORWQK6vTbNiIiAmFhYQgJkS9alZ2djYMHD8LV1RX9+/eHm5sbBg8ejH37NHsBYmJi4OLigj59+mDx4sUoKXnw12RiYiIGDRoEKysr6VhoaCjS09Nx584dKaby9UNDQ5GYWHO3MD3Q78MEbD+ZhaCFmvvdmIqv9lWdqN0tLJHtHdT3gx34vwOX8fTn+xvstouhxqwoFAp8ER6gU+zN/KIqB7FqY8r/X/TBXcsfDOUuRo/EpZgwXIoJ02uC8/feHvBuZae38xGZglonObGxsUhOTkZ0dLRG2YULZZsCLliwABMnTsTWrVsREBCAoUOH4uzZs1LcW2+9hdjYWOzatQtvvPEGFi5ciOnTp0vlmZmZcHNzk527/HlmZma1MeXl2hQWFkKlUske1DyUJzIlpWrsO3sTdwtLMGrFH+gxfxs6vqN9LJc+15YZ2s0VANDBwB9Cj3Z1Nej1yk3dkKJxTFVQjMnrk5Fx+57hK2Rg4UHtqizTZ5JbcYDxJ6N76+28RKaiVn9GZGRkYMqUKYiPj4e1teZfKmp12cJWb7zxBsaPHw8A6NOnDxISErBmzRopMZo2bZr0mp49e8LKygpvvPEGoqOjoVQ23I7J0dHRePfddxvs/NR4PbtyP74cG4iHPtyhtVxbL8bfV/yBi9Ej9fKhZP3XzJlx/b3rfa7asLGq+yyuqtzML6wxZtPRP7Hp6J9ayzYfv47fpw+Bl5YxQ03dpwlnceDCLZzNzjfI9da/FoTnvzwAgDOqiLSpVU9OUlISsrOzERAQAAsLC1hYWGDPnj349NNPYWFhIfWs+PrKxz90794dV65cqfK8QUFBKCkpwaVLlwAA7u7uyMrKksWUPy8fx1NVTFXjfABg1qxZyM3NlR4ZGRm6NdzEJF2+jb1nNDf3q0nF2zoV/WvjMXjPjMOKKsZ/NAZHr+RUmeBUx2eW9l6e+0WlWLPvIq7c0q1XovzWV2P5HCq/XaLLSrjFpfJVeY9cuiN7HhnSGclzH8fKl3S7NQYAAxftqrY8r6AYL64+gPUHq/690Rh9En8G+8/fwo28mhNBfejWxrHmIKJmrFZJztChQ5GamoqUlBTpERgYiPDwcKSkpKBDhw7w8PBAenq67HVnzpxB+/btqzxvSkoKzMzM4Opa1rUeHByMvXv3orj4wbLm8fHx6Nq1K1q0aCHFJCTIxwXEx8cjODi4yusolUo4OjrKHs2NEALPfJGIl9ccwvXc+7IyVUH1y8hfuKH9r9PynY8Xb0vXWm6KPtqejvc2n9SYRVOVvzo5jbKGzFuPdaq2PKCds/S1Wi1ku5hHbTyGzrN/Q78Pd+BMVh4AIPH8g5V1I4Z0RGRIF7S0s0Joj6r/wNBm9d4LVZaNXnUA+8/fwjubUmt1Tn3LuH0PdwtL4D0zTnqcr+LnwBicbCxxeHYIUhcMM3ZViBqlWiU5Dg4O8PPzkz3s7Ozg4uICPz8/KBQKREVF4dNPP8WPP/6Ic+fOYe7cuTh9+jQmTJgAoGzA8NKlS3Hs2DFcuHAB69atw9SpU/HSSy9JCcyLL74IKysrTJgwASdOnMCGDRuwbNky2W2uKVOmYOvWrfj4449x+vRpLFiwAEeOHMHkyZP1+PaYnpPXH4xDCo7eKSt7cfWBal/7762aG0yWlGrfe8fUlW+GWVSqRvKVO/CeGYf4k1lVxqulnhzDJznThnXFpZgwxE8dhO5tHPHHTPlu5vbWltLXHd7Zgq5ztiIlIwfAgw0fs/MKMWzJXry/+SS+TbwsxY/0byN9rVAoMKZf1WNRKvtwy6kqB3efum788XLpmXkYuGgXeszfJjs+9GPdElttdkwbXN9qaWjtoIRDhe8hET2g9xWPIyMjUVBQgKlTp+L27dvo1asX4uPj0bFjRwBlvSmxsbFYsGABCgsL4ePjg6lTp8oSGCcnJ2zfvh0RERHo27cvWrVqhXnz5knTxwGgf//+WL9+PebMmYN33nkHnTt3xs8//ww/P903O2yOdp3OrrIsv6D69VJ2nJK/VgiBTrM1l6dvilIXDJM+KCqPzzl5TaWxzH6p+kFy9/Tn+wEAE/9zBACwNXIgurnL49WN4HZVZzcH/DZloMbxohLNdYRGrfgD7bSMmfm60ky1Hh5OsufRT/tj7t+648ilO7C1Moe1pTn82pbFCCFwNjsfw5bsleKnbkjB0hf6AHjwvtfmtldDCl26t+YgHXw9LhATvi37v9HJ1V4v5yQi3dQ7ydm9e7fGsZkzZ8rWyakoICAABw5U32MAlA1I/v336jeoe+655/Dcc8/pVE8q89H2M1WWXbpV1jVfvtN15bEYla3cU/XthsbA2dYSOTrs5Lz5nwNkfwk7WFsgr0LCd/DiLS1JTtXTy4cv/V1jrEt5uC7L+xtaemae1uNX6jgLytbKQtp2oiKFQoEulRaq+znlGmaO6I6E0w96wSZ9l1yn6+pbWM82iDt+XWtZ9JZTeCqgrSyZ1bZ20Jyw7hja3Y27gBMZCbepJZmKi8fdrbQSbi/PB3+1n8nK03r7qroPf0PTJcEBIPU0lDv0jnz9pXd/PanxmvM37lZ7ztAle7HxyIOB7ca8XVUTzxaGneUU5NNS9vzh6ATM3tT4ViofrCVRK7dq7wUMX/o7jl65g6NX7mDFrnM4euWORtwHcacasopEVAMmOSST9mfVa8NkqcpmjOxKz5bdcqio4ztbpAGaTZWNlTmS5+q++q826Vl5iPrxOICywby708tmsxVquTVkbB8916tOr+vmXrftA76f+LDGsfGPeNfpXA0pM7fqVZ3LPfX5fjz1+X4s3pbeaHqgiOgB7kJOMr+ffTBzpnKvTKaqAKVqgfHfHNbpXFmqAq1bRehDdl4B0jPzMKBTqxpnLP1v8iN4cvkfeG2AD747eBkFxTUPljbXU49L5WRv9qY0hAdVPdPQGLq4VT9O5G8922D5iwG4V1QCWysLlKoFikvVsLas2xo82m7ZVdeblHT5Dvq2b5gtNqpTced4APhm/EM6/98nosaBSU4zouvaHdX1wkz/q3fC2AYt2oWCYjVWvtQXw/20T112UFogr7AE9koLaUyEo42lxoeXNk628tkqqoJiOJroDBaFQgGvljbIuF22pMD2qYPw7f5LWHfwCg6+M1RKVG2tyn5dmJspYG6m30UG7xdVPej9mS/2N4oxLQM6tar1a7hAH5Fx8XZVM6JtDE1t/Tf5apVlZz4YgTMfjKj3NXRR3huzNU1zYOilm3cRveUU8v4aU1Sxp8emUu/D5n8OqPIaFbdgWPDLiXrVt9z/Jj+il/Po2/bIwXilvze+n/gwurg54MOn/HEpJqzBeuLSPxgue17dgPjaOHlNhcGLd+GXFO2rLddG+VYcowM9cTF6JCzNa//r0q4BVpwmIt0xyWkmsvMKpEX7KvPVw6qpowM9YWVhBisLM+mv1wba31Lm55RrGseeXL4PqyosNFdxG4KKt0rWvxakMei4omV/TW0GgJ+O/on5v6TBe2Yc5v1St0Gyr/T3Rk9P5zq9tqHZWJljwZM9ENzRxSDXU1qYY9EzPXWOLyjWbSzTlNijuHzrHqbEptSxZg842ZT13HVytZcS5dkju6OFre49enW9pUdE+sHbVc3EvJ+r7ok4WcXCa4ue7anT7anJQzphcoVVdY3dQa+qtN6Pi92D3ezNK1Sufw23H9q2sJE9L18E7z8VFsOrTmO4xdKYLdmhe+9Nt7lbcWHhyBqn4OcXVr/WU20cu5oDQJ6oTBzUARMHddB5YP3oQC+91YeIao89Oc1E+ZL8tfFkLw/Zkv9V+VdoV6P+xXrwwi14z4xDspYpvADgU+G2U23GSCgtavfjMbDzg6TJGANlm5qqeo16emrvXatp2xEAyL2v27IBuihfJmBfhcH45Tyc5LfxXunvjd+nD8HF6JHoX6Fdg7tWPQ2diBoek5xm4sLN6td10cba0hw//aPxjSG5XyS/dVG+C3P5ysOVVRyTU5u9o8oXRdTVt+P7AQCe6OWB/77Zv1avbY7+reV21Zyw7lWuJVSiwxpM9yr83yjfmqK+tC2KuOPtwVj6fG/p+fwnfOHV0hYKhQJfhPcFADhaW+Ah75YaryUiw+HtqmZu4VP+NW6CqOttq8oEGmZQTlyq9lVogepnhgGARS1nuxxfMAw9F2zXKdbMTMFbVLVgaW6GU+8NR+79Yrg6KHH1zn20c7HF+kPadx7/eHs6op/WfRzPqBV/VPn9uHO3CH3ejwcAhHR3w7+f8YeLvVIqr7inlrbxQLZWFhjVpy1G9WmrUeZka8n/B0SNBHtymrkXg2reUPG5vp5Vlv37GX+NY+V/iFe3YWV9/GvjsTq/tr2LXc1BFZjqtPHGwsbKHO5O1jAzU6CdS9laOReqWE36+0MZWo/Xxi8pf8J7ZpyU4ADAjlNZ6PvBDmSpCrByz3n8X+Il+MzaIpXPGN6t3tclIuNgT04zdXTu43Cw1u3br1Ao8GjX1tKqvRX19tIce1JcWvZX8LxfTqCXpzP82jo1mvVCgju64P1RfujYunbJTk34Qag/Ae2ckXwlR2vZR9vScetuERY+5adx6/H23SLZc28XW1y5dQ+DFu8CAPw+fUi1s66CFiZoPT6wmu0diKhxY5LTDG2LHIQWFWYcaVN54bO3H++qNcmpvO5MZX9f8Yfs+cn3QqVF5QyhjZPmOi9jH9bvisMDOrXCpMEd9HrO5mxwF9cqk5zyvdXGPtxe2jT11HUVRizT3Mx3cJfWUoIDAAMX7dKI0UUjyc+JqA6Y5Jiw/MISnLymQit7eULTtdKeQ22crHH9r316Yl9/GN/8cRELnuwhi/Frq30tnfJbDLrynbfNoOMVdr79aL3P8UgnF/xx7hYA4GL0SCgUCly8eVc2a4v0Z/JjnfDFnnPVbr8RueEotk8dDABaExzgwZT/+jJkUk5E+sWfXhP27Bf7cTqz5qnjLWytpCTn4Q4ueLiD5tRehUKBhLcHo6hEXeWHiq50TRBK1QLLd57DQ94tpDVt/pN4qdrXPB3QFj8ll612u2/GENjoYcXZ1S8HYufpbAzq0lq6RcIEp+GYmylw+v0R1Q4iP5OVX+/rHJ4dgtYOZYONhRDYfjILb/xfUr3PS0SNB5McE6ZLggMAT/Vpi5PXVTVu1NixdfXlutpwOAMzR9Q8hmXQol34M6dsP6Xy3p951Wyv8MEoP7z0cHt8OKpsMLQ+Ehyg7C/5v/X00Mu5SH+8Z8Zh5UsBtX7dr5MHwL/SWjwKhQKhPdxxKSYMiedvYczqA/qqJhEZEZOcZubke6Eax14d4IMu7g7obaAtB4pLa94FHICU4OgqzL8NAP0lN9T4TfouuVbxnV3tNRKcyvrosAAmETUNTHKakaf6tNU6vsDcTIHBBpxBcvlW7RcmpObn9PvD0W3uVoQHtcO6g9rXztHVxeiRAHRbDLK2K10TUePFn2YDydNhSXp9uXLrntbxDJuO1n9nZgCIftpf9m9l5z4cgf+82g/+VWx+ueNUdq2vuefMDY31caJCu8qe6zolnpoGa0tzXIoJw4dP+eOnf9R9BenDs0OgUCh0Xu26Ytyh2UPrfF0iMj5+KhjAlNij+CXlGj4PD8DIv26pVKWwpBQL405hbLA3OrSyw4GLt9CjjROcKux8nJKRg52nsvCPIZ207hlVcdpsQxjTrx3G9Kt6EUELczMM6tIaPTwc0feDHbU69+70bBQUq/G4r5vs+Lg1hzRi/do64VJMGC7dvAuFouy6ZJoC2rXAm492xBe7z9fqdXWdyccVi4lMAz8VDOCXlGsAgH+sq3n8QOiSvfg28TJCPtmDH5Ou4sXVBzHqc/laM6NW/IFPd57D1/su1qoe44L1uz5MTZQ1rKFzI68Q6gr7ERWVqPHKN4cx6bskjF97uMbzB/mU7Qvk3cqu1isZU9MzY3g36baTNpMGdzRgbYioKWBPjoF9En8GKRk5+M+r/bSWX7r1YDPAH5OvAiibcl1SqtboqUjXcfYUYJy/TKu7OVB+O22Yrxu+fDkQAHCvqEQq33tGc+HBit5+vItRdz4n41AoFHi4Q0scuHAbAHB+4UhpNe3zN/Kxcs95eDhZI9C7JR7ppH2XcyJqPurVkxMTEwOFQoHIyEjZ8cTERDz22GOws7ODo6MjBg0ahPv3H8yUuX37NsLDw+Ho6AhnZ2dMmDAB+fnydS+OHz+OgQMHwtraGl5eXli0aJHG9Tdu3Ihu3brB2toa/v7+2LJli0ZMY/NpwlnsPXMDv6TUPD7m0MXb0tcV99oppxYNswGmvlTcxVvbrC4A2F5hf6uiEt1mXQHAZ3+tfEvNz9BuD25lVtwupGNrexyYNRQ7//UoPh3TB88/VPO+bERk2uqc5Bw+fBirVq1Cz57yXYETExMxfPhwDBs2DIcOHcLhw4cxefJkmJk9uFR4eDhOnDiB+Ph4bN68GXv37sXrr78ulatUKgwbNgzt27dHUlISFi9ejAULFuDLL7+UYvbv348xY8ZgwoQJOHr0KEaNGoVRo0YhLS2trk0yqP+r5WqseQUlGsd0TXHWvxZUq2vp06WYMFyKCdNp1djqVritrDYJEZmWiYM64OEOLTHvb74aZe5O1uzhIyJJnZKc/Px8hIeHY/Xq1WjRQr5B49SpU/HWW29h5syZ6NGjB7p27YrRo0dDqSxbWfTUqVPYunUrvvrqKwQFBWHAgAH47LPPEBsbi2vXysaurFu3DkVFRVizZg169OiBF154AW+99RY++eQT6TrLli3D8OHDERUVhe7du+P9999HQEAAli9fXtf3wqCOXL6D23eL4D0zDv/8/mjdTqIly8m9J5/FFTGko7RasLGtGtu32vLCklKtxycP6dQQ1aEmLPb1YLw6wMfY1SCiRq5OSU5ERATCwsIQEhIiO56dnY2DBw/C1dUV/fv3h5ubGwYPHox9+/ZJMYmJiXB2dkZgYKB0LCQkBGZmZjh48KAUM2jQIFhZPdhzKTQ0FOnp6bhz544UU/n6oaGhSExMrLLehYWFUKlUskdDK6lm4buAv25B/XrsGi7cqHmZ+s3Hr8mex6Ve14g5ef1Bm9o62zSqwZihPdyrLa+qJ+dfoV2h4+xfIiIiSa2TnNjYWCQnJyM6Olqj7MKFCwCABQsWYOLEidi6dSsCAgIwdOhQnD17FgCQmZkJV1dX2essLCzQsmVLZGZmSjFubvIpxOXPa4opL9cmOjoaTk5O0sPLy6s2Ta+Ts9m67bHz2Md7kF+oeUuqosnrj1a7nw9Q1l1fbt+MIXCwtqwm2vC2Rg6ssqygip4cAKg8/OjLGnqFiIiIapXkZGRkYMqUKVi3bh2sra01ytXqsr/E33jjDYwfPx59+vTBkiVL0LVrV6xZs0Y/Na6HWbNmITc3V3pkZGQ0+DWnbkjROdZv/rZan3/5zrOy5+U9Ry1sLXVe/MyQurlr380cAAqKNZOcJc/30hpbeR0dIiKiymqV5CQlJSE7OxsBAQGwsLCAhYUF9uzZg08//RQWFhZSz4qvr3xAYPfu3XHlStmy7O7u7sjOlq94W1JSgtu3b8Pd3V2KycrKksWUP68pprxcG6VSCUdHR9mjId25W6TzJpl19dH2M7LnxaVlXR537hluheX6Kv1rrZxCLberRvVuq3Hs2b6ejTKBIyKixqVWSc7QoUORmpqKlJQU6REYGIjw8HCkpKSgQ4cO8PDwQHp6uux1Z86cQfv2ZQvRBQcHIycnB0lJSVL5zp07oVarERQUJMXs3bsXxcUPPqjj4+PRtWtXaaBzcHAwEhISZNeJj49HcHBwbZrUoH44ov+eou1TB1VbvuDXqnfpbiyWvdBb9rzjO1vgPTMOG5M03y9tycyPSVcbqmpERGRCarUYoIODA/z8/GTH7Ozs4OLiIh2PiorC/Pnz0atXL/Tu3RvffvstTp8+jR9//BFAWa/O8OHDMXHiRKxcuRLFxcWYPHkyXnjhBXh4eAAAXnzxRbz77ruYMGECZsyYgbS0NCxbtgxLliyRrjtlyhQMHjwYH3/8McLCwhAbG4sjR47Ippkby87TWXh17ZEGObeFWfU9GBXX1mms+nfUPttr24ksrccrS3h7sD6rQ0REJkrvKx5HRkaioKAAU6dOxe3bt9GrVy/Ex8ejY8cHs3zWrVuHyZMnY+jQoTAzM8MzzzyDTz/9VCp3cnLC9u3bERERgb59+6JVq1aYN2+ebC2d/v37Y/369ZgzZw7eeecddO7cGT///LNGEmYM2hKcpDkhtd7HSRsLM83ON++ZcXi2r2eT6eHQdSPNz8MDtB7v0IpbOBARUc0UQjTyZXMbkEqlgpOTE3Jzc/U6PkfbDKiK2ypULv9glB/m/KzbIob7Zz6G/jE7dYptrJsMCiHgM6vm1akr1n/Uij+QkpGjcZyIiJofXT+/uUFnI/DSw+1xKSYMEwb4oEPr6nspLMyb/oDbugwafv6hhp/uT0REpoVJTgNo62xTp9fN/Zsvdr79aLUxtlYWGNOv6e/JkzQnpOagCsb0a4en+7TFomd61hxMREQEJjkNonxKtL4cnv0gIbBXWiD6aX+9nt8YXOyVtX7NJ8/3xmj26BARkY70PvCYgOJKWzk819ezzueaNLgjWjsoaz0OJWJI49nOoSrP9fXExioGS3/0nPZFAImIiHTFJKcB/DHzMRSWqGFjaY4b+YXV3r6Kr2Hdm5kjutX6+nFvDUAPD6dav87Q5j/Zo8ok59l6JIZEREQAk5wGYW1pDmtLcwDax+ckzQlBwulshPm3gZ1S/9+CppDgAGW33rSpabdyIiIiXTDJMQIXeyVGBzbM2JK4twY0yHkbyoWFI1FQUopVey5gWULZPlyeLeo2cJuIiKgiDjxuxIZVswllL0/tvTVNpRennJmZArZWFuji5iAd8+Fif0REpAfsyWmEdr49GP87dg3jH/GpMubHN/uj8+zfpOfpHwyH0sLcENVrEOlZDzYytbXif0siIqo/9uQ0Qh1a2yMypAucbCyrjLE0f/Ct+2b8Q006wQGAKUM7AwDcHGs/tZyIiEgb/snchC16tidu5RdhSFdXY1el3szNFNyugYiI9IpJThPWUIOXiYiITAFvVxEREZFJYpJDREREJolJDhEREZkkJjlERERkkpr1wGMhynYLV6lURq4JERER6ar8c7v8c7wqzTrJycsrW4DOy4uzlIiIiJqavLw8ODlVvdK/QtSUBpkwtVqNa9euwcHBAQqFQm/nValU8PLyQkZGBhwdHfV23qaiObefbW+ebQead/ubc9uB5t1+Y7VdCIG8vDx4eHjAzKzqkTfNuifHzMwMnp6eDXZ+R0fHZvcfvqLm3H62vXm2HWje7W/ObQead/uN0fbqenDKceAxERERmSQmOURERGSSmOQ0AKVSifnz50OpbJ6bTTbn9rPtzbPtQPNuf3NuO9C829/Y296sBx4TERGR6WJPDhEREZkkJjlERERkkpjkEBERkUlikkNEREQmiUlOA1ixYgW8vb1hbW2NoKAgHDp0yNhVqlZ0dDQeeughODg4wNXVFaNGjUJ6erospqCgABEREXBxcYG9vT2eeeYZZGVlyWKuXLmCsLAw2NrawtXVFVFRUSgpKZHF7N69GwEBAVAqlejUqRPWrl2rUR9jvn8xMTFQKBSIjIyUjply2//880+89NJLcHFxgY2NDfz9/XHkyBGpXAiBefPmoU2bNrCxsUFISAjOnj0rO8ft27cRHh4OR0dHODs7Y8KECcjPz5fFHD9+HAMHDoS1tTW8vLywaNEijbps3LgR3bp1g7W1Nfz9/bFly5aGafRfSktLMXfuXPj4+MDGxgYdO3bE+++/L9sLx1Tav3fvXjzxxBPw8PCAQqHAzz//LCtvTO3UpS76bH9xcTFmzJgBf39/2NnZwcPDAy+//DKuXbtmEu2v6Xtf0aRJk6BQKLB06VKTaHv5SUmPYmNjhZWVlVizZo04ceKEmDhxonB2dhZZWVnGrlqVQkNDxTfffCPS0tJESkqKGDlypGjXrp3Iz8+XYiZNmiS8vLxEQkKCOHLkiHj44YdF//79pfKSkhLh5+cnQkJCxNGjR8WWLVtEq1atxKxZs6SYCxcuCFtbWzFt2jRx8uRJ8dlnnwlzc3OxdetWKcaY79+hQ4eEt7e36Nmzp5gyZYrJt/327duiffv24pVXXhEHDx4UFy5cENu2bRPnzp2TYmJiYoSTk5P4+eefxbFjx8STTz4pfHx8xP3796WY4cOHi169eokDBw6I33//XXTq1EmMGTNGKs/NzRVubm4iPDxcpKWlie+//17Y2NiIVatWSTF//PGHMDc3F4sWLRInT54Uc+bMEZaWliI1NbVB2i6EEB9++KFwcXERmzdvFhcvXhQbN24U9vb2YtmyZSbX/i1btojZs2eLn376SQAQmzZtkpU3pnbqUhd9tj8nJ0eEhISIDRs2iNOnT4vExETRr18/0bdvX9k5mmr7a/rel/vpp59Er169hIeHh1iyZIlJtF0IIZjk6Fm/fv1ERESE9Ly0tFR4eHiI6OhoI9aqdrKzswUAsWfPHiFE2S8BS0tLsXHjRinm1KlTAoBITEwUQpT9IJmZmYnMzEwp5osvvhCOjo6isLBQCCHE9OnTRY8ePWTXev7550VoaKj03FjvX15enujcubOIj48XgwcPlpIcU277jBkzxIABA6osV6vVwt3dXSxevFg6lpOTI5RKpfj++++FEEKcPHlSABCHDx+WYn777TehUCjEn3/+KYQQ4vPPPxctWrSQ3ovya3ft2lV6Pnr0aBEWFia7flBQkHjjjTfq18hqhIWFiVdffVV27Omnnxbh4eFCCNNtf+UPusbUTl3qUl/VfdCXO3TokAAgLl++LIQwnfZX1farV6+Ktm3birS0NNG+fXtZktPU287bVXpUVFSEpKQkhISESMfMzMwQEhKCxMREI9asdnJzcwEALVu2BAAkJSWhuLhY1q5u3bqhXbt2UrsSExPh7+8PNzc3KSY0NBQqlQonTpyQYiqeozym/BzGfP8iIiIQFhamUT9Tbvv//vc/BAYG4rnnnoOrqyv69OmD1atXS+UXL15EZmamrE5OTk4ICgqStd3Z2RmBgYFSTEhICMzMzHDw4EEpZtCgQbCyspK1PT09HXfu3JFiqnt/GkL//v2RkJCAM2fOAACOHTuGffv2YcSIEQBMv/3lGlM7damLIeTm5kKhUMDZ2Vmqt6m2X61WY+zYsYiKikKPHj00ypt625nk6NHNmzdRWloq+7ADADc3N2RmZhqpVrWjVqsRGRmJRx55BH5+fgCAzMxMWFlZST/w5Sq2KzMzU2u7y8uqi1GpVLh//77R3r/Y2FgkJycjOjpao8yU237hwgV88cUX6Ny5M7Zt24Y333wTb731Fr799ltZ3aurU2ZmJlxdXWXlFhYWaNmypV7en4b8vs+cORMvvPACunXrBktLS/Tp0weRkZEIDw+X1c1U21+uMbVTl7o0tIKCAsyYMQNjxoyRNpw05fb/+9//hoWFBd566y2t5U297c16F3LSFBERgbS0NOzbt8/YVTGIjIwMTJkyBfHx8bC2tjZ2dQxKrVYjMDAQCxcuBAD06dMHaWlpWLlyJcaNG2fk2jW8H374AevWrcP69evRo0cPpKSkIDIyEh4eHs2i/aSpuLgYo0ePhhACX3zxhbGr0+CSkpKwbNkyJCcnQ6FQGLs6DYI9OXrUqlUrmJuba8y8ycrKgru7u5FqpbvJkydj8+bN2LVrFzw9PaXj7u7uKCoqQk5Ojiy+Yrvc3d21tru8rLoYR0dH2NjYGOX9S0pKQnZ2NgICAmBhYQELCwvs2bMHn376KSwsLODm5maybW/Tpg18fX1lx7p3744rV67I6l5dndzd3ZGdnS0rLykpwe3bt/Xy/jTkz01UVJTUm+Pv74+xY8di6tSpUo+eqbe/XGNqpy51aSjlCc7ly5cRHx8v9eKU18sU2//7778jOzsb7dq1k37/Xb58GW+//Ta8vb2lOjXltjPJ0SMrKyv07dsXCQkJ0jG1Wo2EhAQEBwcbsWbVE0Jg8uTJ2LRpE3bu3AkfHx9Zed++fWFpaSlrV3p6Oq5cuSK1Kzg4GKmpqbIfhvJfFOUfpMHBwbJzlMeUn8MY79/QoUORmpqKlJQU6REYGIjw8HDpa1Nt+yOPPKKxVMCZM2fQvn17AICPjw/c3d1ldVKpVDh48KCs7Tk5OUhKSpJidu7cCbVajaCgIClm7969KC4ulmLi4+PRtWtXtGjRQoqp7v1pCPfu3YOZmfxXoLm5OdRqNQDTb3+5xtROXerSEMoTnLNnz2LHjh1wcXGRlZtq+8eOHYvjx4/Lfv95eHggKioK27ZtM42213nIMmkVGxsrlEqlWLt2rTh58qR4/fXXhbOzs2zmTWPz5ptvCicnJ7F7925x/fp16XHv3j0pZtKkSaJdu3Zi586d4siRIyI4OFgEBwdL5eXTqIcNGyZSUlLE1q1bRevWrbVOo46KihKnTp0SK1as0DqN2tjvX8XZVUKYbtsPHTokLCwsxIcffijOnj0r1q1bJ2xtbcV3330nxcTExAhnZ2fxyy+/iOPHj4u///3vWqcW9+nTRxw8eFDs27dPdO7cWTa9NCcnR7i5uYmxY8eKtLQ0ERsbK2xtbTWml1pYWIiPPvpInDp1SsyfP7/Bp5CPGzdOtG3bVppC/tNPP4lWrVqJ6dOnm1z78/LyxNGjR8XRo0cFAPHJJ5+Io0ePSrOHGlM7damLPttfVFQknnzySeHp6SlSUlJkvwMrzhZqqu2v6XtfWeXZVU257UJwCnmD+Oyzz0S7du2ElZWV6Nevnzhw4ICxq1QtAFof33zzjRRz//598Y9//EO0aNFC2Nraiqeeekpcv35ddp5Lly6JESNGCBsbG9GqVSvx9ttvi+LiYlnMrl27RO/evYWVlZXo0KGD7BrljP3+VU5yTLntv/76q/Dz8xNKpVJ069ZNfPnll7JytVot5s6dK9zc3IRSqRRDhw4V6enpsphbt26JMWPGCHt7e+Ho6CjGjx8v8vLyZDHHjh0TAwYMEEqlUrRt21bExMRo1OWHH34QXbp0EVZWVqJHjx4iLi5O/w2uQKVSiSlTpoh27doJa2tr0aFDBzF79mzZB5uptH/Xrl1af8bHjRvX6NqpS1302f6LFy9W+Ttw165dTb79NX3vK9OW5DTVtgshhEKICst7EhEREZkIjskhIiIik8Qkh4iIiEwSkxwiIiIySUxyiIiIyCQxySEiIiKTxCSHiIiITBKTHCIiIjJJTHKIiIjIJDHJISIiIpPEJIeIiIhMEpMcIiIiMklMcoiIiMgk/T8wth+5BzWvqQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8wfPXVX6J_z"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsjTYESI04kF"
      },
      "outputs": [],
      "source": [
        "class AddGaussianNoise:\n",
        "    def __init__(self):\n",
        "        self.noise_std = random.uniform(0, 0.005)\n",
        "    def __call__(self, x):\n",
        "        noise = torch.randn_like(x) * self.noise_std * x\n",
        "        return x + noise\n",
        "\n",
        "class RandomScaling:\n",
        "    def __init__(self, scale_range=(0.995, 1.005)):\n",
        "        self.scale_range = scale_range\n",
        "    def __call__(self, x):\n",
        "        scale = np.random.uniform(*self.scale_range)\n",
        "        return x * scale\n",
        "\n",
        "class Compose:\n",
        "    \"\"\"Chain multiple augmentations.\"\"\"\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "    def __call__(self, x):\n",
        "        for t in self.transforms:\n",
        "            x = t(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9AWmTzW6Lae"
      },
      "outputs": [],
      "source": [
        "class MilliGoldDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataset, lookback, transform=None):\n",
        "\n",
        "    self.transform = transform\n",
        "    self.dataset = dataset\n",
        "    self.lookback = lookback\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    data = self.transform(self.dataset[idx]) if self.transform else self.dataset[idx]\n",
        "\n",
        "    x = data[:self.lookback].unsqueeze(-1)\n",
        "    y = data[self.lookback:]\n",
        "\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSar_zTr8X0y"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "def get_loaders(dataframe, time_bin, scaler, lookback, lookforward,batch_size: int, train_size:float,  transform=None):\n",
        "\n",
        "    dataframe['TimeBin'] = df['Timestamp'].dt.floor(f'{time_bin}min')\n",
        "    grouped_df = dataframe.groupby('TimeBin').agg(\n",
        "                Price = ('Price.', 'mean'))\n",
        "    lookforward -= 1\n",
        "\n",
        "    grouped_df['ScaledPrice'] = scaler.fit_transform(grouped_df[['Price']])\n",
        "\n",
        "    for i in range((lookback+lookforward), 0, -1):\n",
        "\n",
        "      grouped_df[f'price_lag_{i}'] = grouped_df['ScaledPrice'].shift(i)\n",
        "\n",
        "    grouped_df.dropna(inplace=True)\n",
        "\n",
        "    lag_cols = [f'price_lag_{i}' for i in range(lookback+lookforward, 0, -1)]\n",
        "    lag_cols.append('ScaledPrice')\n",
        "\n",
        "    data = torch.tensor(grouped_df[lag_cols].values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    index = int(len(grouped_df) * train_size)\n",
        "\n",
        "    train_raw = data[:index, :]\n",
        "    test_raw = data[index:, :]\n",
        "\n",
        "    trainset = MilliGoldDataset(train_raw, lookback, transform)\n",
        "    testset = MilliGoldDataset(test_raw, lookback, None)\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return trainset, train_loader, testset, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKjGBmkY7-m7"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "  def __init__(self, sample_mean=3):\n",
        "\n",
        "    super().__init__()\n",
        "    self.sample_mean = sample_mean\n",
        "\n",
        "  def forward(self, x):\n",
        "    last_samples = x[:, -self.sample_mean:, :]   # shape: [batch, 2, 1]\n",
        "    return last_samples.mean(dim=1)"
      ],
      "metadata": {
        "id": "OO4lvKmeFkDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vp1OQNz6iIw"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_stacked_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_stacked_layers = num_stacked_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvlZWAeQ0d3e"
      },
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_stacked_layers, output_size, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_stacked_layers\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_stacked_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_stacked_layers > 1 else 0.0\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, C]\n",
        "        B = x.size(0)\n",
        "        h0 = torch.zeros(self.num_layers, B, self.hidden_size, device=x.device)\n",
        "        out, _ = self.gru(x, h0)     # out: [B, T, H]\n",
        "        out = out[:, -1, :]          # last timestep\n",
        "        out = self.fc(out)           # [B, output_size]\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lqj166D06lE"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)                  # [T, D]\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)    # [T, 1]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)  # not a parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, D]\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:T].unsqueeze(0)   # broadcast to [B, T, D]\n",
        "\n",
        "class TransformerTS(nn.Module):\n",
        "    \"\"\"\n",
        "    Projects input_size -> d_model, adds positional encoding,\n",
        "    passes through TransformerEncoder, takes last timestep, then a head.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 num_encoder_layers,\n",
        "                 dim_feedforward,\n",
        "                 output_size,\n",
        "                 dropout=0.1,\n",
        "                 layer_norm_eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_encoder_layers,\n",
        "            norm=nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        # x: [B, T, C]\n",
        "        x = self.input_proj(x)            # [B, T, D]\n",
        "        x = self.pos_enc(x)               # [B, T, D]\n",
        "        # src_key_padding_mask: [B, T] with True for PAD positions (optional)\n",
        "        h = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # [B, T, D]\n",
        "        h_last = h[:, -1, :]              # last timestep representation\n",
        "        out = self.head(h_last)           # [B, output_size]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51tGBmNs8CG7"
      },
      "source": [
        "# training & test regime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzKs_3miAgc_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0, mode='min', verbose=False, save_path='checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        patience : int         # how many epochs to wait without improvement\n",
        "        min_delta : float      # minimum change to count as an improvement\n",
        "        mode : 'max' | 'min'   # 'max' for accuracy, 'min' for loss\n",
        "        verbose : bool         # print messages when improvement happens\n",
        "        save_path : str        # path to save the best model\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.verbose = verbose\n",
        "        self.save_path = save_path\n",
        "\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_best = np.inf if mode == 'min' else -np.inf\n",
        "\n",
        "    def __call__(self, val_metric, model):\n",
        "        score = val_metric\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            if self.best_score is None:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "            elif score > self.best_score - self.min_delta:\n",
        "                self.counter += 1\n",
        "                if self.verbose:\n",
        "                    print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "                if self.counter >= self.patience:\n",
        "                    self.early_stop = True\n",
        "            else:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "                self.counter = 0\n",
        "        else:  # mode == 'max'\n",
        "            if self.best_score is None:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "            elif score < self.best_score + self.min_delta:\n",
        "                self.counter += 1\n",
        "                if self.verbose:\n",
        "                    print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "                if self.counter >= self.patience:\n",
        "                    self.early_stop = True\n",
        "            else:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(model)\n",
        "                self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        \"\"\"Saves model when validation metric improves.\"\"\"\n",
        "        torch.save(model.state_dict(), self.save_path)\n",
        "        if self.verbose:\n",
        "            print(f\"Model improved. Saving model to {self.save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PjYtPD18BgK"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, show=True):\n",
        "\n",
        "    model.train(True)\n",
        "\n",
        "    if show:\n",
        "      print(f'Epoch: {epoch + 1}')\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_index, batch in enumerate(train_loader):\n",
        "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "        output = model(x_batch)\n",
        "        loss = loss_function(output, y_batch)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_index % 100 == 99:  # print every 100 batches\n",
        "            avg_loss_across_batches = running_loss / 100\n",
        "            if show:\n",
        "              print('Batch {0}, Loss: {1:.3f}'.format(batch_index+1,\n",
        "                                                    avg_loss_across_batches))\n",
        "            running_loss = 0.0\n",
        "    if show:\n",
        "      print()\n",
        "\n",
        "def validate_one_epoch(model,early_stopping, show=True):\n",
        "    model.train(False)\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_index, batch in enumerate(test_loader):\n",
        "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(x_batch)\n",
        "            loss = loss_function(output, y_batch)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    avg_loss_across_batches = running_loss / len(test_loader)\n",
        "    early_stopping(avg_loss_across_batches, model)\n",
        "\n",
        "\n",
        "    if show:\n",
        "      print('Val Loss: {0:.5f}'.format(avg_loss_across_batches))\n",
        "      print('***************************************************')\n",
        "      print()\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "      return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWLksV-tKKRC"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7OXYXinKPqH"
      },
      "outputs": [],
      "source": [
        "def _mae(y, yhat):\n",
        "    return np.mean(np.abs(y - yhat))\n",
        "\n",
        "def _rmse(y, yhat):\n",
        "    return np.sqrt(np.mean((y - yhat) ** 2))\n",
        "\n",
        "def _mape(y, yhat, eps=1e-8):\n",
        "    denom = np.maximum(np.abs(y), eps)\n",
        "    return 100.0 * np.mean(np.abs((y - yhat) / denom))\n",
        "\n",
        "def _smape(y, yhat, eps=1e-8):\n",
        "    denom = np.maximum((np.abs(y) + np.abs(yhat)) / 2.0, eps)\n",
        "    return 100.0 * np.mean(np.abs(y - yhat) / denom)\n",
        "\n",
        "def _r2(y, yhat):\n",
        "    ss_res = np.sum((y - yhat) ** 2)\n",
        "    ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
        "    return 1.0 - ss_res / ss_tot if ss_tot > 0 else np.nan\n",
        "\n",
        "def _mase(y_insample, y_insample_naive, y, yhat, eps=1e-12):\n",
        "    \"\"\"\n",
        "    y_insample: series used to compute naive scale (e.g., train_true[1:])\n",
        "    y_insample_naive: naive preds for insample (e.g., train_true[:-1])\n",
        "    MASE = MAE(model)/MAE(naive) using in-sample naive MAE as the scale.\n",
        "    \"\"\"\n",
        "    scale = _mae(y_insample, y_insample_naive)\n",
        "    return _mae(y, yhat) / (scale + eps)\n",
        "\n",
        "def evaluate_forecast(out_dict, horizon_step=0):\n",
        "\n",
        "    def select_step(arr):\n",
        "      if arr.ndim == 2:\n",
        "          return arr[:, horizon_step]\n",
        "      return arr\n",
        "\n",
        "    train_pred = select_step(out_dict[\"train_pred_inv\"])\n",
        "    train_true = select_step(out_dict[\"train_true_inv\"])\n",
        "    test_pred  = select_step(out_dict[\"test_pred_inv\"])\n",
        "    test_true  = select_step(out_dict[\"test_true_inv\"])\n",
        "    # ensure 1D & drop NaNs if any\n",
        "    def _clean(a):\n",
        "        a = np.asarray(a).reshape(-1)\n",
        "        return a[~np.isnan(a)]\n",
        "    train_true = _clean(train_true); train_pred = _clean(train_pred)\n",
        "    test_true  = _clean(test_true);  test_pred  = _clean(test_pred)\n",
        "\n",
        "    # In-sample naive (one-step) for MASE scale: y_{t-1}\n",
        "    # Align lengths: compare train_true[1:] vs naive = train_true[:-1]\n",
        "    if len(train_true) >= 2:\n",
        "        insample_y = train_true[1:]\n",
        "        insample_naive = train_true[:-1]\n",
        "        mase_train = _mase(insample_y, insample_naive, train_true, train_pred)\n",
        "        mase_test  = _mase(insample_y, insample_naive, test_true,  test_pred)\n",
        "    else:\n",
        "        mase_train = np.nan\n",
        "        mase_test  = np.nan\n",
        "\n",
        "    metrics_train = {\n",
        "        \"MAE\":  _mae(train_true, train_pred),\n",
        "        \"RMSE\": _rmse(train_true, train_pred),\n",
        "        \"MAPE\": _mape(train_true, train_pred),\n",
        "        \"sMAPE\": _smape(train_true, train_pred),\n",
        "        \"R2\":   _r2(train_true, train_pred),\n",
        "        \"MASE\": mase_train,\n",
        "    }\n",
        "    metrics_test = {\n",
        "        \"MAE\":  _mae(test_true, test_pred),\n",
        "        \"RMSE\": _rmse(test_true, test_pred),\n",
        "        \"MAPE\": _mape(test_true, test_pred),\n",
        "        \"sMAPE\": _smape(test_true, test_pred),\n",
        "        \"R2\":   _r2(test_true, test_pred),\n",
        "        \"MASE\": mase_test,\n",
        "    }\n",
        "    return {\"train\": metrics_train, \"test\": metrics_test}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jht49w2-Wdf_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def _predict_on_loader(model, loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    preds, trues = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)                 # [B, lookback, 1]\n",
        "        y = y.to(device)                 # [B, horizon]\n",
        "        out = model(x)                   # expect [B, horizon] (or [B, horizon, 1] -> squeeze)\n",
        "        if out.dim() == 3 and out.size(-1) == 1:\n",
        "            out = out.squeeze(-1)\n",
        "        preds.append(out.detach().cpu())\n",
        "        trues.append(y.detach().cpu())\n",
        "\n",
        "    return torch.cat(preds, dim=0), torch.cat(trues, dim=0)  # [N, horizon], [N, horizon]\n",
        "\n",
        "def _inverse_scale_2d(arr_2d, scaler):\n",
        "    \"\"\"\n",
        "    arr_2d: numpy array [N, H] in scaler space.\n",
        "    scaler: fitted MinMaxScaler on price column.\n",
        "    \"\"\"\n",
        "    flat = arr_2d.reshape(-1, 1)\n",
        "    inv = scaler.inverse_transform(flat)\n",
        "    return inv.reshape(arr_2d.shape)\n",
        "\n",
        "def predict_train_test(\n",
        "    model,\n",
        "    trainset,\n",
        "    testset,\n",
        "    batch_size=256,\n",
        "    device=None,\n",
        "    scaler=None,          # pass if you want inverse-scaled outputs\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dict with predictions and targets for train and test.\n",
        "    Shapes are [N, horizon] for both preds and targets.\n",
        "    If `scaler` is provided, adds *_inv with inverse-scaled prices.\n",
        "    \"\"\"\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n",
        "    test_loader  = DataLoader(testset,  batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n",
        "\n",
        "    train_pred, train_true = _predict_on_loader(model, train_loader, device)\n",
        "    test_pred,  test_true  = _predict_on_loader(model, test_loader,  device)\n",
        "\n",
        "    out = {\n",
        "        \"train_pred\": train_pred.numpy(),   # scaled\n",
        "        \"train_true\": train_true.numpy(),\n",
        "        \"test_pred\":  test_pred.numpy(),\n",
        "        \"test_true\":  test_true.numpy(),\n",
        "    }\n",
        "\n",
        "    if scaler is not None:\n",
        "        out[\"train_pred_inv\"] = _inverse_scale_2d(out[\"train_pred\"], scaler)\n",
        "        out[\"train_true_inv\"] = _inverse_scale_2d(out[\"train_true\"], scaler)\n",
        "        out[\"test_pred_inv\"]  = _inverse_scale_2d(out[\"test_pred\"],  scaler)\n",
        "        out[\"test_true_inv\"]  = _inverse_scale_2d(out[\"test_true\"],  scaler)\n",
        "\n",
        "\n",
        "    return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdnp5LgEIC2J"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_predictions_plotly(out_dict, horizon_step=0, title=\"Prediction vs Actual (Train/Test)\"):\n",
        "    \"\"\"\n",
        "    out_dict: dictionary with keys:\n",
        "      train_pred_inv, train_true_inv, test_pred_inv, test_true_inv\n",
        "      Each should be numpy arrays [N, H] or [N] if single step\n",
        "    horizon_step: which forecast step to plot (0 = next step)\n",
        "    \"\"\"\n",
        "\n",
        "    # pick correct horizon step\n",
        "    def select_step(arr):\n",
        "        if arr.ndim == 2:\n",
        "            return arr[:, horizon_step]\n",
        "        return arr\n",
        "\n",
        "    train_pred = select_step(out_dict[\"train_pred_inv\"])\n",
        "    train_true = select_step(out_dict[\"train_true_inv\"])\n",
        "    test_pred  = select_step(out_dict[\"test_pred_inv\"])\n",
        "    test_true  = select_step(out_dict[\"test_true_inv\"])\n",
        "\n",
        "    n_train = len(train_pred)\n",
        "    n_test  = len(test_pred)\n",
        "\n",
        "    # x axes\n",
        "    x_train = list(range(n_train))\n",
        "    x_test  = list(range(n_train, n_train + n_test))\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Train actual\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_train, y=train_true,\n",
        "        mode='lines', name='Train Actual',\n",
        "        line=dict(color='blue')\n",
        "    ))\n",
        "\n",
        "    # Train prediction\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_train, y=train_pred,\n",
        "        mode='lines', name='Train Pred',\n",
        "        line=dict(color='blue', dash='dash')\n",
        "    ))\n",
        "\n",
        "    # Test actual\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_test, y=test_true,\n",
        "        mode='lines', name='Test Actual',\n",
        "        line=dict(color='orange')\n",
        "    ))\n",
        "\n",
        "    # Test prediction\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x_test, y=test_pred,\n",
        "        mode='lines', name='Test Pred',\n",
        "        line=dict(color='orange', dash='dash')\n",
        "    ))\n",
        "\n",
        "    # vertical line for train/test split\n",
        "    fig.add_vline(\n",
        "        x=n_train - 0.5,\n",
        "        line_width=2, line_dash=\"dot\", line_color=\"black\"\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"Sample Index (time ordered)\",\n",
        "        yaxis_title=\"Price\",\n",
        "        legend=dict(x=0, y=1),\n",
        "        hovermode=\"x unified\",\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9OWAs-oL0kk"
      },
      "outputs": [],
      "source": [
        "def plot_test_predictions_plotly(\n",
        "    results,\n",
        "    horizon_step: int = 0,\n",
        "    title: str = \"Prediction vs Actual (Test Only)\"\n",
        "):\n",
        "    \"\"\"\n",
        "    results: dict with structure\n",
        "        {\n",
        "            \"test_true_inv\": np.ndarray [N] or [N, H],\n",
        "            \"preds\": {\n",
        "                \"LSTM\": np.ndarray [N] or [N, H],\n",
        "                \"GRU\":  np.ndarray [N] or [N, H],\n",
        "                \"Transformer\": np.ndarray [N] or [N, H],\n",
        "                ...\n",
        "            }\n",
        "        }\n",
        "    horizon_step: which forecast step to plot (0 = next step)\n",
        "    \"\"\"\n",
        "\n",
        "    def select_step(arr):\n",
        "        # Accept shape [N] or [N, H]\n",
        "        if arr.ndim == 2:\n",
        "            if horizon_step < 0 or horizon_step >= arr.shape[1]:\n",
        "                raise ValueError(f\"horizon_step {horizon_step} out of range for array with shape {arr.shape}.\")\n",
        "            return arr[:, horizon_step]\n",
        "        return arr\n",
        "\n",
        "    # Extract and validate y_true\n",
        "    if \"test_true_inv\" not in results or \"preds\" not in results:\n",
        "        raise KeyError('Expected keys: \"test_true_inv\" and \"preds\" in results.')\n",
        "\n",
        "    y_true_full = np.asarray(results[\"test_true_inv\"])\n",
        "    y_true = select_step(y_true_full)\n",
        "    N = len(y_true)\n",
        "\n",
        "    # Prepare x axis (test indices only)\n",
        "    x = np.arange(N)\n",
        "\n",
        "    # Build figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Actual test series (solid line)\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=x, y=y_true,\n",
        "        mode=\"lines\",\n",
        "        name=\"Test Actual\",\n",
        "        line=dict(color=\"black\")\n",
        "    ))\n",
        "\n",
        "    # Add one dashed line per model\n",
        "    for model_name, y_pred_full in results[\"preds\"].items():\n",
        "        y_pred = select_step(np.asarray(y_pred_full))\n",
        "\n",
        "        if len(y_pred) != N:\n",
        "            raise ValueError(\n",
        "                f'Length mismatch for \"{model_name}\": got {len(y_pred)} vs y_true length {N}.'\n",
        "            )\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=x, y=y_pred,\n",
        "            mode=\"lines\",\n",
        "            name=f\"{model_name} Pred\",\n",
        "            line=dict(dash=\"dash\")  # color auto-cycles\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title=\"Test Sample Index (time ordered)\",\n",
        "        yaxis_title=\"Price\",\n",
        "        legend=dict(x=0, y=1),\n",
        "        hovermode=\"x unified\",\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLAQbHGyHIQ"
      },
      "source": [
        "# Concat all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvb1VaCsyQw6"
      },
      "outputs": [],
      "source": [
        "augmentations = Compose([AddGaussianNoise()])#, RandomScaling()])\n",
        "\n",
        "trainset, train_loader, testset, test_loader = get_loaders(df_no_outliers, time_bin=60, scaler=scaler, lookback=12,\n",
        "                                        lookforward=1, batch_size=8, train_size=0.9,\n",
        "                                        transform=augmentations)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 200\n",
        "input_size=1\n",
        "d_model=128\n",
        "nhead=8\n",
        "num_encoder_layers=1\n",
        "dim_feedforward=64\n",
        "output_size=1\n",
        "dropout=0.1"
      ],
      "metadata": {
        "id": "QqKKnKl56CJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0yyg3gsySEq",
        "outputId": "d62f65cf-343d-496d-f54c-dad52378175d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.01321\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03695\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02243\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00769\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01171\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00683\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01516\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01725\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02708\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02325\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.08964\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00838\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.04578\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00812\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02599\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02259\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02097\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03563\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01732\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03313\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01054\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02452\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00667\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01999\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.05102\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02287\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00487\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00705\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01295\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02679\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00936\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00773\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01194\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00767\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01069\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02133\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01872\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00720\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02019\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00611\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01382\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00944\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00489\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00943\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01069\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02356\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00455\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02721\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01678\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00678\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00382\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02248\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01649\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00429\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00681\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00702\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01206\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00304\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01186\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01739\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00290\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01178\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02459\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00944\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01175\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01974\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02108\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01495\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00561\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01675\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00910\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00506\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01252\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00431\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01209\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00794\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01558\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00617\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00240\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00644\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01107\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00754\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01398\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01376\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00696\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00934\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01138\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01914\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00509\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00335\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00587\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01541\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00918\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01632\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01066\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00660\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01769\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02171\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01515\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00303\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00404\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03092\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00676\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00299\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00364\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01491\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01149\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01115\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00446\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01577\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01010\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00724\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01886\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00926\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00173\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01362\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00464\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01427\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00832\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00677\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00703\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01946\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01493\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00382\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02493\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00432\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01054\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00208\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01303\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02029\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00660\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03148\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00208\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01276\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00481\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01307\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00942\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00474\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01151\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00401\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00824\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00799\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00322\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00213\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00353\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01419\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01714\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00922\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00202\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01950\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00921\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00595\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00913\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00815\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01684\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00277\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00663\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00749\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01081\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00446\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01783\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00692\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00695\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00450\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00646\n",
            "***************************************************\n",
            "\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "early_stopping = EarlyStopping(patience=50, mode='min', verbose=False, save_path='/content/checkpoint_transformer.pt')\n",
        "\n",
        "\n",
        "model = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size=output_size,\n",
        "    dropout=dropout)\n",
        "\n",
        "# model = GRUModel(1, 4, 1, 1)\n",
        "model.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, show=False)\n",
        "    stop_criteria = validate_one_epoch(model, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtHiKv5A-kfM"
      },
      "source": [
        "# Models Comparison"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "output_size = 1\n",
        "num_hidden_layers = 4\n",
        "num_stacked_layers = 1\n",
        "patience = 30\n",
        "num_epochs = 5000\n",
        "learning_rate = 0.0005"
      ],
      "metadata": {
        "id": "f4H-tZb0wdrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB3twgcd-nal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e6c4c9be-f77c-4d56-db23-f40713e3c252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.13064\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.21848\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.33407\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.48683\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.64623\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.76588\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.83553\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.85679\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.84050\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.79269\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.71879\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.64383\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.55957\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.47157\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.38270\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.30166\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.22551\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.16634\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.12861\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.10750\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.09882\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.09419\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.09297\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.09142\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.08902\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.08640\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.08515\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.08227\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.08057\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.07708\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.07545\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.07248\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.06959\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.06702\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.06400\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.06227\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.06110\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.05743\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.05608\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.05472\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.05259\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.05128\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.05008\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.04761\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.04592\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.04338\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.04327\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.04126\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.04045\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03988\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.04012\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03776\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03486\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03496\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03409\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03289\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03200\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02971\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03110\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02941\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.03005\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02908\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02791\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02653\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02604\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02596\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02483\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02379\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02369\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02459\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02260\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02183\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02235\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02155\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02073\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02101\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02042\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01934\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01934\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02024\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01881\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01897\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01703\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01704\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01775\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01670\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01640\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01648\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01603\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01577\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01647\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01647\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01429\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01534\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01425\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01433\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01441\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01471\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01379\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01399\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01281\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01432\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01355\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01238\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01231\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01291\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01252\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01189\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01176\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01196\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01174\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01151\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01181\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01028\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01059\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01110\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01144\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01020\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01022\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01140\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01041\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00996\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.01041\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00972\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00920\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00944\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00890\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00970\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00894\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00920\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00975\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00827\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00872\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00966\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00823\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00780\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00983\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00789\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00781\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00843\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00800\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00630\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00832\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00739\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00778\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00700\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00840\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00644\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00686\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00644\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00636\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00675\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00688\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00708\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00570\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00737\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00666\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00619\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00545\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00575\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00559\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00584\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00567\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00558\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00653\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00684\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00649\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00510\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00631\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00571\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00520\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00622\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00515\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00476\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00535\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00559\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00624\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00593\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00541\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00547\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00533\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00578\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00414\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00457\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00530\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00482\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00494\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00392\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00565\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00499\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00503\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00539\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00557\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00491\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00460\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00589\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00514\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00485\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00480\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00499\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00480\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00525\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00548\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00474\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00456\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00444\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00523\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00489\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00419\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00498\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00468\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00443\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00473\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00517\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00446\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00485\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00534\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00535\n",
            "***************************************************\n",
            "\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "# train LSTM\n",
        "lstm_model = LSTM(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "\n",
        "lstm_model.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
        "early_stopping = EarlyStopping(patience=patience, mode='min', verbose=False, save_path='/content/checkpoint_lstm.pt')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(lstm_model, show=False)\n",
        "\n",
        "    stop_criteria = validate_one_epoch(lstm_model, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnpSfTEE_XHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "463e3884-3593-476d-c3f8-27e9bc57fb2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "\n",
            "Val Loss: 0.55201\n",
            "***************************************************\n",
            "\n",
            "Epoch: 2\n",
            "\n",
            "Val Loss: 0.68076\n",
            "***************************************************\n",
            "\n",
            "Epoch: 3\n",
            "\n",
            "Val Loss: 0.79559\n",
            "***************************************************\n",
            "\n",
            "Epoch: 4\n",
            "\n",
            "Val Loss: 0.87098\n",
            "***************************************************\n",
            "\n",
            "Epoch: 5\n",
            "\n",
            "Val Loss: 0.89383\n",
            "***************************************************\n",
            "\n",
            "Epoch: 6\n",
            "\n",
            "Val Loss: 0.87246\n",
            "***************************************************\n",
            "\n",
            "Epoch: 7\n",
            "\n",
            "Val Loss: 0.80961\n",
            "***************************************************\n",
            "\n",
            "Epoch: 8\n",
            "\n",
            "Val Loss: 0.72727\n",
            "***************************************************\n",
            "\n",
            "Epoch: 9\n",
            "\n",
            "Val Loss: 0.64655\n",
            "***************************************************\n",
            "\n",
            "Epoch: 10\n",
            "\n",
            "Val Loss: 0.54535\n",
            "***************************************************\n",
            "\n",
            "Epoch: 11\n",
            "\n",
            "Val Loss: 0.44582\n",
            "***************************************************\n",
            "\n",
            "Epoch: 12\n",
            "\n",
            "Val Loss: 0.33905\n",
            "***************************************************\n",
            "\n",
            "Epoch: 13\n",
            "\n",
            "Val Loss: 0.24248\n",
            "***************************************************\n",
            "\n",
            "Epoch: 14\n",
            "\n",
            "Val Loss: 0.16587\n",
            "***************************************************\n",
            "\n",
            "Epoch: 15\n",
            "\n",
            "Val Loss: 0.10926\n",
            "***************************************************\n",
            "\n",
            "Epoch: 16\n",
            "\n",
            "Val Loss: 0.07588\n",
            "***************************************************\n",
            "\n",
            "Epoch: 17\n",
            "\n",
            "Val Loss: 0.05972\n",
            "***************************************************\n",
            "\n",
            "Epoch: 18\n",
            "\n",
            "Val Loss: 0.05165\n",
            "***************************************************\n",
            "\n",
            "Epoch: 19\n",
            "\n",
            "Val Loss: 0.04799\n",
            "***************************************************\n",
            "\n",
            "Epoch: 20\n",
            "\n",
            "Val Loss: 0.04565\n",
            "***************************************************\n",
            "\n",
            "Epoch: 21\n",
            "\n",
            "Val Loss: 0.04484\n",
            "***************************************************\n",
            "\n",
            "Epoch: 22\n",
            "\n",
            "Val Loss: 0.04309\n",
            "***************************************************\n",
            "\n",
            "Epoch: 23\n",
            "\n",
            "Val Loss: 0.04131\n",
            "***************************************************\n",
            "\n",
            "Epoch: 24\n",
            "\n",
            "Val Loss: 0.04015\n",
            "***************************************************\n",
            "\n",
            "Epoch: 25\n",
            "\n",
            "Val Loss: 0.03864\n",
            "***************************************************\n",
            "\n",
            "Epoch: 26\n",
            "\n",
            "Val Loss: 0.03752\n",
            "***************************************************\n",
            "\n",
            "Epoch: 27\n",
            "\n",
            "Val Loss: 0.03639\n",
            "***************************************************\n",
            "\n",
            "Epoch: 28\n",
            "\n",
            "Val Loss: 0.03576\n",
            "***************************************************\n",
            "\n",
            "Epoch: 29\n",
            "\n",
            "Val Loss: 0.03437\n",
            "***************************************************\n",
            "\n",
            "Epoch: 30\n",
            "\n",
            "Val Loss: 0.03352\n",
            "***************************************************\n",
            "\n",
            "Epoch: 31\n",
            "\n",
            "Val Loss: 0.03203\n",
            "***************************************************\n",
            "\n",
            "Epoch: 32\n",
            "\n",
            "Val Loss: 0.03160\n",
            "***************************************************\n",
            "\n",
            "Epoch: 33\n",
            "\n",
            "Val Loss: 0.03070\n",
            "***************************************************\n",
            "\n",
            "Epoch: 34\n",
            "\n",
            "Val Loss: 0.02996\n",
            "***************************************************\n",
            "\n",
            "Epoch: 35\n",
            "\n",
            "Val Loss: 0.02906\n",
            "***************************************************\n",
            "\n",
            "Epoch: 36\n",
            "\n",
            "Val Loss: 0.02699\n",
            "***************************************************\n",
            "\n",
            "Epoch: 37\n",
            "\n",
            "Val Loss: 0.02696\n",
            "***************************************************\n",
            "\n",
            "Epoch: 38\n",
            "\n",
            "Val Loss: 0.02693\n",
            "***************************************************\n",
            "\n",
            "Epoch: 39\n",
            "\n",
            "Val Loss: 0.02583\n",
            "***************************************************\n",
            "\n",
            "Epoch: 40\n",
            "\n",
            "Val Loss: 0.02501\n",
            "***************************************************\n",
            "\n",
            "Epoch: 41\n",
            "\n",
            "Val Loss: 0.02482\n",
            "***************************************************\n",
            "\n",
            "Epoch: 42\n",
            "\n",
            "Val Loss: 0.02406\n",
            "***************************************************\n",
            "\n",
            "Epoch: 43\n",
            "\n",
            "Val Loss: 0.02308\n",
            "***************************************************\n",
            "\n",
            "Epoch: 44\n",
            "\n",
            "Val Loss: 0.02201\n",
            "***************************************************\n",
            "\n",
            "Epoch: 45\n",
            "\n",
            "Val Loss: 0.02175\n",
            "***************************************************\n",
            "\n",
            "Epoch: 46\n",
            "\n",
            "Val Loss: 0.02008\n",
            "***************************************************\n",
            "\n",
            "Epoch: 47\n",
            "\n",
            "Val Loss: 0.02096\n",
            "***************************************************\n",
            "\n",
            "Epoch: 48\n",
            "\n",
            "Val Loss: 0.02001\n",
            "***************************************************\n",
            "\n",
            "Epoch: 49\n",
            "\n",
            "Val Loss: 0.01931\n",
            "***************************************************\n",
            "\n",
            "Epoch: 50\n",
            "\n",
            "Val Loss: 0.01886\n",
            "***************************************************\n",
            "\n",
            "Epoch: 51\n",
            "\n",
            "Val Loss: 0.01847\n",
            "***************************************************\n",
            "\n",
            "Epoch: 52\n",
            "\n",
            "Val Loss: 0.01820\n",
            "***************************************************\n",
            "\n",
            "Epoch: 53\n",
            "\n",
            "Val Loss: 0.01599\n",
            "***************************************************\n",
            "\n",
            "Epoch: 54\n",
            "\n",
            "Val Loss: 0.01674\n",
            "***************************************************\n",
            "\n",
            "Epoch: 55\n",
            "\n",
            "Val Loss: 0.01548\n",
            "***************************************************\n",
            "\n",
            "Epoch: 56\n",
            "\n",
            "Val Loss: 0.01598\n",
            "***************************************************\n",
            "\n",
            "Epoch: 57\n",
            "\n",
            "Val Loss: 0.01449\n",
            "***************************************************\n",
            "\n",
            "Epoch: 58\n",
            "\n",
            "Val Loss: 0.01462\n",
            "***************************************************\n",
            "\n",
            "Epoch: 59\n",
            "\n",
            "Val Loss: 0.01418\n",
            "***************************************************\n",
            "\n",
            "Epoch: 60\n",
            "\n",
            "Val Loss: 0.01320\n",
            "***************************************************\n",
            "\n",
            "Epoch: 61\n",
            "\n",
            "Val Loss: 0.01381\n",
            "***************************************************\n",
            "\n",
            "Epoch: 62\n",
            "\n",
            "Val Loss: 0.01337\n",
            "***************************************************\n",
            "\n",
            "Epoch: 63\n",
            "\n",
            "Val Loss: 0.01276\n",
            "***************************************************\n",
            "\n",
            "Epoch: 64\n",
            "\n",
            "Val Loss: 0.01322\n",
            "***************************************************\n",
            "\n",
            "Epoch: 65\n",
            "\n",
            "Val Loss: 0.01234\n",
            "***************************************************\n",
            "\n",
            "Epoch: 66\n",
            "\n",
            "Val Loss: 0.01274\n",
            "***************************************************\n",
            "\n",
            "Epoch: 67\n",
            "\n",
            "Val Loss: 0.01224\n",
            "***************************************************\n",
            "\n",
            "Epoch: 68\n",
            "\n",
            "Val Loss: 0.01183\n",
            "***************************************************\n",
            "\n",
            "Epoch: 69\n",
            "\n",
            "Val Loss: 0.01059\n",
            "***************************************************\n",
            "\n",
            "Epoch: 70\n",
            "\n",
            "Val Loss: 0.01028\n",
            "***************************************************\n",
            "\n",
            "Epoch: 71\n",
            "\n",
            "Val Loss: 0.01001\n",
            "***************************************************\n",
            "\n",
            "Epoch: 72\n",
            "\n",
            "Val Loss: 0.01108\n",
            "***************************************************\n",
            "\n",
            "Epoch: 73\n",
            "\n",
            "Val Loss: 0.00963\n",
            "***************************************************\n",
            "\n",
            "Epoch: 74\n",
            "\n",
            "Val Loss: 0.00923\n",
            "***************************************************\n",
            "\n",
            "Epoch: 75\n",
            "\n",
            "Val Loss: 0.01053\n",
            "***************************************************\n",
            "\n",
            "Epoch: 76\n",
            "\n",
            "Val Loss: 0.00943\n",
            "***************************************************\n",
            "\n",
            "Epoch: 77\n",
            "\n",
            "Val Loss: 0.00907\n",
            "***************************************************\n",
            "\n",
            "Epoch: 78\n",
            "\n",
            "Val Loss: 0.00880\n",
            "***************************************************\n",
            "\n",
            "Epoch: 79\n",
            "\n",
            "Val Loss: 0.00912\n",
            "***************************************************\n",
            "\n",
            "Epoch: 80\n",
            "\n",
            "Val Loss: 0.00907\n",
            "***************************************************\n",
            "\n",
            "Epoch: 81\n",
            "\n",
            "Val Loss: 0.00958\n",
            "***************************************************\n",
            "\n",
            "Epoch: 82\n",
            "\n",
            "Val Loss: 0.00774\n",
            "***************************************************\n",
            "\n",
            "Epoch: 83\n",
            "\n",
            "Val Loss: 0.00809\n",
            "***************************************************\n",
            "\n",
            "Epoch: 84\n",
            "\n",
            "Val Loss: 0.00840\n",
            "***************************************************\n",
            "\n",
            "Epoch: 85\n",
            "\n",
            "Val Loss: 0.00872\n",
            "***************************************************\n",
            "\n",
            "Epoch: 86\n",
            "\n",
            "Val Loss: 0.00734\n",
            "***************************************************\n",
            "\n",
            "Epoch: 87\n",
            "\n",
            "Val Loss: 0.00709\n",
            "***************************************************\n",
            "\n",
            "Epoch: 88\n",
            "\n",
            "Val Loss: 0.00736\n",
            "***************************************************\n",
            "\n",
            "Epoch: 89\n",
            "\n",
            "Val Loss: 0.00735\n",
            "***************************************************\n",
            "\n",
            "Epoch: 90\n",
            "\n",
            "Val Loss: 0.00704\n",
            "***************************************************\n",
            "\n",
            "Epoch: 91\n",
            "\n",
            "Val Loss: 0.00657\n",
            "***************************************************\n",
            "\n",
            "Epoch: 92\n",
            "\n",
            "Val Loss: 0.00701\n",
            "***************************************************\n",
            "\n",
            "Epoch: 93\n",
            "\n",
            "Val Loss: 0.00692\n",
            "***************************************************\n",
            "\n",
            "Epoch: 94\n",
            "\n",
            "Val Loss: 0.00708\n",
            "***************************************************\n",
            "\n",
            "Epoch: 95\n",
            "\n",
            "Val Loss: 0.00669\n",
            "***************************************************\n",
            "\n",
            "Epoch: 96\n",
            "\n",
            "Val Loss: 0.00709\n",
            "***************************************************\n",
            "\n",
            "Epoch: 97\n",
            "\n",
            "Val Loss: 0.00665\n",
            "***************************************************\n",
            "\n",
            "Epoch: 98\n",
            "\n",
            "Val Loss: 0.00632\n",
            "***************************************************\n",
            "\n",
            "Epoch: 99\n",
            "\n",
            "Val Loss: 0.00648\n",
            "***************************************************\n",
            "\n",
            "Epoch: 100\n",
            "\n",
            "Val Loss: 0.00606\n",
            "***************************************************\n",
            "\n",
            "Epoch: 101\n",
            "\n",
            "Val Loss: 0.00615\n",
            "***************************************************\n",
            "\n",
            "Epoch: 102\n",
            "\n",
            "Val Loss: 0.00656\n",
            "***************************************************\n",
            "\n",
            "Epoch: 103\n",
            "\n",
            "Val Loss: 0.00527\n",
            "***************************************************\n",
            "\n",
            "Epoch: 104\n",
            "\n",
            "Val Loss: 0.00606\n",
            "***************************************************\n",
            "\n",
            "Epoch: 105\n",
            "\n",
            "Val Loss: 0.00562\n",
            "***************************************************\n",
            "\n",
            "Epoch: 106\n",
            "\n",
            "Val Loss: 0.00593\n",
            "***************************************************\n",
            "\n",
            "Epoch: 107\n",
            "\n",
            "Val Loss: 0.00584\n",
            "***************************************************\n",
            "\n",
            "Epoch: 108\n",
            "\n",
            "Val Loss: 0.00480\n",
            "***************************************************\n",
            "\n",
            "Epoch: 109\n",
            "\n",
            "Val Loss: 0.00540\n",
            "***************************************************\n",
            "\n",
            "Epoch: 110\n",
            "\n",
            "Val Loss: 0.00513\n",
            "***************************************************\n",
            "\n",
            "Epoch: 111\n",
            "\n",
            "Val Loss: 0.00499\n",
            "***************************************************\n",
            "\n",
            "Epoch: 112\n",
            "\n",
            "Val Loss: 0.00498\n",
            "***************************************************\n",
            "\n",
            "Epoch: 113\n",
            "\n",
            "Val Loss: 0.00518\n",
            "***************************************************\n",
            "\n",
            "Epoch: 114\n",
            "\n",
            "Val Loss: 0.00499\n",
            "***************************************************\n",
            "\n",
            "Epoch: 115\n",
            "\n",
            "Val Loss: 0.00509\n",
            "***************************************************\n",
            "\n",
            "Epoch: 116\n",
            "\n",
            "Val Loss: 0.00489\n",
            "***************************************************\n",
            "\n",
            "Epoch: 117\n",
            "\n",
            "Val Loss: 0.00443\n",
            "***************************************************\n",
            "\n",
            "Epoch: 118\n",
            "\n",
            "Val Loss: 0.00462\n",
            "***************************************************\n",
            "\n",
            "Epoch: 119\n",
            "\n",
            "Val Loss: 0.00427\n",
            "***************************************************\n",
            "\n",
            "Epoch: 120\n",
            "\n",
            "Val Loss: 0.00436\n",
            "***************************************************\n",
            "\n",
            "Epoch: 121\n",
            "\n",
            "Val Loss: 0.00416\n",
            "***************************************************\n",
            "\n",
            "Epoch: 122\n",
            "\n",
            "Val Loss: 0.00471\n",
            "***************************************************\n",
            "\n",
            "Epoch: 123\n",
            "\n",
            "Val Loss: 0.00475\n",
            "***************************************************\n",
            "\n",
            "Epoch: 124\n",
            "\n",
            "Val Loss: 0.00598\n",
            "***************************************************\n",
            "\n",
            "Epoch: 125\n",
            "\n",
            "Val Loss: 0.00483\n",
            "***************************************************\n",
            "\n",
            "Epoch: 126\n",
            "\n",
            "Val Loss: 0.00442\n",
            "***************************************************\n",
            "\n",
            "Epoch: 127\n",
            "\n",
            "Val Loss: 0.00419\n",
            "***************************************************\n",
            "\n",
            "Epoch: 128\n",
            "\n",
            "Val Loss: 0.00441\n",
            "***************************************************\n",
            "\n",
            "Epoch: 129\n",
            "\n",
            "Val Loss: 0.00345\n",
            "***************************************************\n",
            "\n",
            "Epoch: 130\n",
            "\n",
            "Val Loss: 0.00448\n",
            "***************************************************\n",
            "\n",
            "Epoch: 131\n",
            "\n",
            "Val Loss: 0.00486\n",
            "***************************************************\n",
            "\n",
            "Epoch: 132\n",
            "\n",
            "Val Loss: 0.00411\n",
            "***************************************************\n",
            "\n",
            "Epoch: 133\n",
            "\n",
            "Val Loss: 0.00416\n",
            "***************************************************\n",
            "\n",
            "Epoch: 134\n",
            "\n",
            "Val Loss: 0.00434\n",
            "***************************************************\n",
            "\n",
            "Epoch: 135\n",
            "\n",
            "Val Loss: 0.00388\n",
            "***************************************************\n",
            "\n",
            "Epoch: 136\n",
            "\n",
            "Val Loss: 0.00472\n",
            "***************************************************\n",
            "\n",
            "Epoch: 137\n",
            "\n",
            "Val Loss: 0.00396\n",
            "***************************************************\n",
            "\n",
            "Epoch: 138\n",
            "\n",
            "Val Loss: 0.00409\n",
            "***************************************************\n",
            "\n",
            "Epoch: 139\n",
            "\n",
            "Val Loss: 0.00422\n",
            "***************************************************\n",
            "\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "gru_model = GRUModel(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "gru_model.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(gru_model.parameters(), lr=learning_rate)\n",
        "early_stopping = EarlyStopping(patience=10, mode='min', verbose=False, save_path='/content/checkpoint_gru.pt')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(gru_model)\n",
        "    stop_criteria = validate_one_epoch(gru_model, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ik57HvnA52C"
      },
      "outputs": [],
      "source": [
        "lstm_model = LSTM(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "lstm_model.load_state_dict(torch.load('/content/checkpoint_lstm.pt'))\n",
        "\n",
        "result = predict_train_test(lstm_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "lstm_metrics = evaluate_forecast(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-X2adOZAltq"
      },
      "outputs": [],
      "source": [
        "gru_model = GRUModel(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "gru_model.load_state_dict(torch.load('/content/checkpoint_gru.pt'))\n",
        "result = predict_train_test(gru_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "gru_metrics = evaluate_forecast(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size=output_size,\n",
        "    dropout=dropout)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/checkpoint_transformer.pt'))\n",
        "result = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "transformer_metrics = evaluate_forecast(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsPk1D-E2lvT",
        "outputId": "348cc0e2-a235-410b-b561-b404cb61a95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnpl-CmQHqeK"
      },
      "outputs": [],
      "source": [
        "def get_params_flops(model, input_size):\n",
        "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    with torch.cuda.device(0):\n",
        "        macs, _ = get_model_complexity_info(model, input_size, as_strings=False, print_per_layer_stat=False)\n",
        "    flops = macs * 2  # MACs to FLOPs\n",
        "    return params, flops\n",
        "\n",
        "def human_readable(num):\n",
        "    for unit in [\"\", \"K\", \"M\", \"B\"]:\n",
        "        if abs(num) < 1000:\n",
        "            return f\"{num:.2f} {unit}\"\n",
        "        num /= 1000\n",
        "    return f\"{num:.2f} T\"  # trillion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36r5KiGNHw3F"
      },
      "outputs": [],
      "source": [
        "model_info = {}\n",
        "for name, model in {\n",
        "    \"LSTM\": lstm_model,\n",
        "    \"GRU\": gru_model,\n",
        "    \"Transformer\": model\n",
        "}.items():\n",
        "    params, flops = get_params_flops(model, (30, 1))  # (seq_len, input_size)\n",
        "    model_info[name] = {\"Params\": human_readable(params), \"FLOPs\": human_readable(flops)}\n",
        "\n",
        "\n",
        "df_info = pd.DataFrame(model_info).T\n",
        "df_info = df_info.reset_index().rename(columns={'index':'Model'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "GRas0whxAm0f",
        "outputId": "bff36494-7286-4fd3-efd4-283f8cf6c5e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           MAE       RMSE      MAPE     sMAPE        R2  \\\n",
              "Model       Dataset                                                       \n",
              "LSTM        train    26.101883  38.711823  0.056125  0.056121  0.990649   \n",
              "            test     53.730415  68.963326  0.112373  0.112465  0.869965   \n",
              "GRU         train    30.151537  41.266483  0.064878  0.064862  0.989364   \n",
              "            test     50.774494  64.858582  0.106207  0.106293  0.884984   \n",
              "Transformer train    28.704010  40.617840  0.061778  0.061762  0.989705   \n",
              "            test     37.992504  47.026203  0.079600  0.079615  0.939535   \n",
              "\n",
              "                         MASE    Params   FLOPs  \n",
              "Model       Dataset                              \n",
              "LSTM        train    1.139834   117.00   9.13 K  \n",
              "            test     2.346334   117.00   9.13 K  \n",
              "GRU         train    1.319468    89.00   6.73 K  \n",
              "            test     2.221954    89.00   6.73 K  \n",
              "Transformer train    1.253302  100.29 K  5.51 M  \n",
              "            test     1.658865  100.29 K  5.51 M  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-77be541b-3c3a-48ca-985d-a2771f26ed31\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "      <th>sMAPE</th>\n",
              "      <th>R2</th>\n",
              "      <th>MASE</th>\n",
              "      <th>Params</th>\n",
              "      <th>FLOPs</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">LSTM</th>\n",
              "      <th>train</th>\n",
              "      <td>26.101883</td>\n",
              "      <td>38.711823</td>\n",
              "      <td>0.056125</td>\n",
              "      <td>0.056121</td>\n",
              "      <td>0.990649</td>\n",
              "      <td>1.139834</td>\n",
              "      <td>117.00</td>\n",
              "      <td>9.13 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>53.730415</td>\n",
              "      <td>68.963326</td>\n",
              "      <td>0.112373</td>\n",
              "      <td>0.112465</td>\n",
              "      <td>0.869965</td>\n",
              "      <td>2.346334</td>\n",
              "      <td>117.00</td>\n",
              "      <td>9.13 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">GRU</th>\n",
              "      <th>train</th>\n",
              "      <td>30.151537</td>\n",
              "      <td>41.266483</td>\n",
              "      <td>0.064878</td>\n",
              "      <td>0.064862</td>\n",
              "      <td>0.989364</td>\n",
              "      <td>1.319468</td>\n",
              "      <td>89.00</td>\n",
              "      <td>6.73 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>50.774494</td>\n",
              "      <td>64.858582</td>\n",
              "      <td>0.106207</td>\n",
              "      <td>0.106293</td>\n",
              "      <td>0.884984</td>\n",
              "      <td>2.221954</td>\n",
              "      <td>89.00</td>\n",
              "      <td>6.73 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Transformer</th>\n",
              "      <th>train</th>\n",
              "      <td>28.704010</td>\n",
              "      <td>40.617840</td>\n",
              "      <td>0.061778</td>\n",
              "      <td>0.061762</td>\n",
              "      <td>0.989705</td>\n",
              "      <td>1.253302</td>\n",
              "      <td>100.29 K</td>\n",
              "      <td>5.51 M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>37.992504</td>\n",
              "      <td>47.026203</td>\n",
              "      <td>0.079600</td>\n",
              "      <td>0.079615</td>\n",
              "      <td>0.939535</td>\n",
              "      <td>1.658865</td>\n",
              "      <td>100.29 K</td>\n",
              "      <td>5.51 M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77be541b-3c3a-48ca-985d-a2771f26ed31')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-77be541b-3c3a-48ca-985d-a2771f26ed31 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-77be541b-3c3a-48ca-985d-a2771f26ed31');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5d323777-9f08-4add-a2d0-c26b1619eb31\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5d323777-9f08-4add-a2d0-c26b1619eb31')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5d323777-9f08-4add-a2d0-c26b1619eb31 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"comparison\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          26.101882934570312,\n          53.73041534423828,\n          37.99250411987305\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          38.711822509765625,\n          68.96332550048828,\n          47.02620315551758\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.056125443428754807,\n          0.11237319558858871,\n          0.07959964126348495\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sMAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.05612064152956009,\n          0.1124652773141861,\n          0.07961548864841461\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.9906490445137024,\n          0.8699650764465332,\n          0.9395350813865662\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MASE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1.1398338079452515,\n          2.346334218978882,\n          1.6588653326034546\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Params\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"117.00 \",\n          \"89.00 \",\n          \"100.29 K\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FLOPs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"9.13 K\",\n          \"6.73 K\",\n          \"5.51 M\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "results = {\n",
        "    \"LSTM\": lstm_metrics,\n",
        "    \"GRU\": gru_metrics,\n",
        "    \"Transformer\": transformer_metrics\n",
        "}\n",
        "\n",
        "\n",
        "comparison = pd.concat({model: pd.DataFrame(metrics).T for model, metrics in results.items()})\n",
        "\n",
        "comparison = comparison.reset_index().rename(columns={'level_0':'Model', 'level_1':'Dataset'})\n",
        "\n",
        "comparison = comparison.merge(df_info, on=\"Model\")\n",
        "\n",
        "comparison.set_index(['Model',\t'Dataset'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = BaselineModel(sample_mean=6)\n",
        "result = predict_train_test(baseline_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "\n",
        "plot_predictions_plotly(result, horizon_step=0, title=\"Gold Price Forecast\")\n",
        "\n",
        "\n",
        "transformer_metrics = evaluate_forecast(result)\n",
        "transformer_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "WBOdtFHOIK1Y",
        "outputId": "48b47045-8901-4eb0-fe32-a45ce4bb1b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"350a093f-0d65-44da-af20-a3fa1cbe3c5a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"350a093f-0d65-44da-af20-a3fa1cbe3c5a\")) {                    Plotly.newPlot(                        \"350a093f-0d65-44da-af20-a3fa1cbe3c5a\",                        [{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Train Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[45983.566,45962.715,46025.7,45970.652,45889.727,45871.36,45900.91,45971.86,45982.8,45970.49,45953.812,45929.254,45936.348,45934.82,45946.656,45934.445,45937.414,45932.785,45918.668,45907.094,45920.188,45929.207,45951.957,46004.21,45958.613,45825.51,45881.668,45893.996,45965.07,46006.652,46003.45,45963.78,45905.555,45952.42,45979.28,45979.285,45979.266,45980.832,45979.99,45980.156,45976.824,45981.207,45980.31,45968.84,45985.324,45989.11,45929.137,45798.33,45796.51,45870.32,45842.52,45815.75,45859.504,45878.95,45880.605,45877.734,45994.176,46077.406,46068.258,46103.566,46099.91,46100.133,46098.016,46098.34,46100.355,46101.348,46100.844,46098.527,46153.816,46193.78,46215.223,46241.414,46335.812,46375.246,46323.32,46359.723,46413.19,46437.6,46309.387,46303.438,46308.9,46247.57,46279.848,46299.63,46330.652,46329.63,46330.477,46329.51,46331.242,46329.547,46330.137,46329.344,46329.082,46325.484,46425.44,46461.28,46314.957,46261.145,46286.96,46276.984,46273.94,46212.918,46283.81,46352.21,46372.94,46362.79,46438.387,46366.492,46359.96,46359.676,46359.2,46359.95,46360.223,46360.016,46360.367,46360.543,46359.492,46235.082,46178.96,46149.395,46117.996,45976.562,45896.746,45875.582,45865.48,45831.92,45938.418,45943.43,45992.37,46059.438,46110.383,46083.938,46050.035,46050.43,46049.02,46048.855,46049.582,46050.324,46051.51,46049.17,45942.926,45983.15,46024.42,46129.832,46217.793,46278.742,46221.62,46164.36,46187.066,46164.26,46241.055,46230.16,46225.535,46225.95,46228.617,46202.293,46199.066,46212.83,46216.105,46220.246,46224.758,46225.72,46219.375,46254.594,46271.48,46353.44,46471.27,46423.22,46426.062,46433.215,46445.89,46445.074,46377.938,46357.254,46385.445,46427.688,46471.188,46459.43,46458.727,46460.207,46467.277,46463.83,46462.0,46462.13,46465.777,46460.33,46459.695,46459.887,46465.098,46465.387,46471.98,46490.23,46482.773,46434.35,46440.082,46416.996,46372.074,46322.723,46295.016,46283.97,46232.07,46233.234,46236.96,46289.168,46286.27,46281.152,46275.445,46258.902,46241.125,46234.61,46230.508,46223.168,46229.445,46250.47,46353.477,46286.906,46254.47,46343.2,46390.54,46370.47,46381.098,46309.31,46314.98,46337.305,46350.95,46330.71,46329.574,46338.914,46340.26,46340.863,46343.824,46340.004,46339.496,46340.195,46339.664,46340.64,46342.582,46343.53,46507.4,46606.203,46671.086,46690.43,46664.79,46720.29,46746.89,46778.105,46768.902,46825.508,46868.11,46837.723,46805.51,46832.297,46839.918,46840.906,46833.24,46829.71,46835.91,46830.695,46843.203,46847.312,46849.984,46838.54,46828.53,46812.152,46725.82,46717.82,46684.996,46679.996,46629.008,46648.227,46640.76,46644.766,46619.875,46619.02,46616.004,46619.027,46630.137,46625.27,46625.953,46629.09,46632.777,46630.152,46636.63,46624.773,46623.176,46615.188,46643.195,46651.848,46636.848,46590.73,46576.234,46506.715,46521.152,46493.2,46495.51,46527.84,46517.273,46510.75,46512.32,46509.387,46509.23,46509.27,46511.742,46519.08,46517.44,46515.113,46518.508,46512.703,46508.91,46500.957,46497.05,46415.88,46377.9,46389.55,46407.74,46401.98,46375.844,46383.004,46387.594,46401.02,46422.375,46416.203,46416.793,46401.004,46399.812,46399.863,46401.43,46404.01,46399.832,46399.293,46400.117,46397.086,46401.52,46413.082,46465.133,46503.926,46536.484,46508.133,46520.38,46532.71,46528.582,46492.605,46469.914,46464.633,46458.684,46463.703,46458.633,46460.98,46462.16,46463.035,46460.082,46460.188,46463.938,46463.285,46459.18,46460.766,46461.48,46460.95,46461.184,46463.664,46465.47,46457.85,46451.406,46429.383,46409.195,46369.29,46379.707,46309.85,46303.766,46299.633,46292.445,46367.188,46403.15,46390.945,46380.055,46380.797,46378.523,46368.367,46367.11,46360.246,46333.2,46328.72,46272.586,46409.406,46289.953,46246.324,46226.438,46176.285,46149.117,46173.348,46111.37,46094.918,46130.44,46113.254,46107.258,46133.793,46187.65,46182.773,46167.25,46159.703,46161.363,46158.75,46159.32,46153.105,46142.426,46200.56,46249.266,46196.06,46149.594,46159.95,46163.176,46196.37,46181.695,46168.023,46180.65,46208.97,46209.8,46226.086,46221.074,46217.914,46210.35,46211.426,46208.953,46210.227,46209.79,46210.793,46209.71,46208.56,46202.47,46199.13,46207.824,46163.69,46263.766,46396.42,46398.23,46419.395,46420.715,46379.613,46363.59,46375.633,46354.555,46327.836,46307.82,46354.72,46353.57,46355.625,46347.047,46347.36,46345.492,46342.523,46340.938,46340.21,46322.367,46329.414,46390.43,46373.58,46324.066,46461.79,46494.96,46503.918,46505.914,46448.79,46505.836,46587.695,46589.93,46559.027,46558.777,46574.11,46579.547,46575.477,46573.395,46572.848,46571.84,46568.152,46570.34,46577.082,46620.56,46660.906,46669.105,46700.11,46710.24,46720.69,46718.03,46651.887,46614.688,46622.746,46638.805,46674.688,46634.45,46611.285,46615.777,46638.555,46645.14,46636.035,46639.84,46640.547,46640.65,46640.594,46632.98,46629.633,46632.01,46622.676,46618.504,46530.754,46542.508,46562.273,46592.387,46599.887,46623.0,46656.008,46675.43,46691.06,46705.676,46682.45,46678.863,46670.223,46668.0,46671.027,46681.113,46682.32,46684.965,46698.094,46691.133,46693.64,46697.6,46703.39,46734.453,46782.227,46791.73,46738.24,46767.91,46784.406,46830.812,46828.81,46937.805,47000.92,46985.21,47111.613,47146.586,47184.63,47201.973,47170.03,47162.066,47148.06,47139.176,47140.06,47149.324,47141.09,47156.242,47173.797,47152.168,47142.918,47272.957,47300.69,47344.883,47308.027,47249.164,47252.312,47198.395,47151.01,47168.023,47142.242,47130.496,47133.676,47131.61,47129.496,47130.14,47129.953,47129.605,47128.664,47129.535,47121.562,47114.95,47115.746,47136.895,47027.45,46990.223,46938.44,46955.945,46984.73,46965.723,46996.996,47002.1,47018.867,47009.652,47008.156,47006.414,47009.266,47009.965,47021.266,47029.547,47029.598,47030.83,47034.484,47036.96,47038.23,47038.598,47023.918,46931.684,46986.004,47086.05,47108.02,47100.875,47081.875,47081.316,47087.992,47103.496,47182.96,47176.75,47178.117,47172.395,47187.652,47190.035,47189.74,47185.406,47187.652,47184.504,47182.53,47180.53,47180.7,47182.48,47187.426,47216.31,47268.754,47249.1,47317.504,47340.16,47299.367,47439.113,47539.918,47472.445,47434.848,47396.297,47412.258,47402.68,47415.727,47419.39,47420.418,47420.227,47419.742,47420.277,47412.184,47409.516,47410.02,47400.594,47439.863,47493.402,47470.555,47396.65,47423.445,47414.81,47410.35,47398.64,47429.39,47414.36,47411.145,47416.223,47454.117],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Train Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[45921.473,45940.645,45956.363,45982.832,45990.9,45979.38,45949.51,45936.0,45937.605,45932.16,45931.086,45942.09,45952.516,45958.316,45952.35,45945.992,45939.754,45936.82,45937.03,45934.59,45930.21,45925.945,45924.723,45926.48,45938.48,45946.047,45932.223,45925.61,45919.27,45922.004,45921.363,45929.215,45952.594,45955.816,45965.35,45968.29,45963.29,45959.445,45962.055,45974.973,45979.977,45979.875,45979.203,45979.766,45978.07,45979.324,45981.04,45972.777,45943.01,45911.453,45894.97,45871.42,45841.51,45829.562,45843.08,45858.78,45860.758,45885.348,45928.28,45963.047,46000.2,46036.184,46074.016,46091.04,46095.297,46100.23,46100.043,46099.95,46100.42,46109.375,46125.15,46144.234,46167.594,46206.99,46252.203,46280.492,46307.875,46341.133,46373.875,46369.832,46357.285,46355.15,46336.77,46314.508,46290.96,46294.527,46299.38,46302.61,46316.203,46324.96,46330.227,46330.258,46329.816,46329.71,46329.63,46344.984,46367.086,46364.6,46353.04,46346.242,46337.754,46312.22,46271.984,46266.688,46281.727,46295.547,46310.188,46337.5,46363.375,46375.58,46377.074,46374.395,46374.4,46361.133,46359.67,46359.867,46359.73,46360.332,46339.395,46308.7,46273.77,46233.727,46168.832,46092.1,46031.246,45980.113,45927.793,45896.684,45891.047,45907.69,45938.523,45979.652,46022.145,46040.72,46058.004,46067.473,46064.91,46055.492,46049.227,46049.766,46050.387,46031.715,46021.035,46017.273,46030.996,46057.684,46095.805,46141.906,46172.11,46199.65,46205.75,46209.43,46201.094,46201.598,46212.332,46219.055,46225.74,46217.895,46215.258,46213.56,46212.875,46213.02,46215.953,46219.484,46227.316,46235.91,46258.316,46299.234,46331.785,46366.453,46396.605,46425.19,46440.94,46425.24,46414.465,46407.355,46406.56,46410.68,46413.273,46426.496,46443.477,46456.895,46463.29,46461.47,46462.48,46463.277,46463.855,46462.773,46461.977,46462.656,46462.742,46463.562,46468.242,46471.797,46467.74,46463.453,46455.832,46439.465,46411.676,46379.934,46355.086,46320.074,46289.94,46267.676,46261.82,46260.93,46259.664,46267.65,46271.27,46272.164,46262.51,46254.137,46244.465,46236.547,46234.74,46253.918,46262.547,46266.19,46286.207,46313.21,46332.25,46337.766,46340.9,46351.04,46349.855,46343.2,46337.035,46328.676,46333.71,46337.9,46338.73,46337.09,46338.566,46340.85,46341.164,46341.027,46340.082,46340.406,46340.92,46368.793,46413.21,46468.57,46526.895,46580.656,46643.418,46683.273,46711.99,46728.168,46750.62,46784.74,46804.184,46814.0,46823.055,46834.85,46837.457,46831.605,46830.305,46835.332,46834.992,46835.61,46836.656,46839.41,46840.88,46839.67,46836.656,46817.07,46795.52,46768.062,46741.62,46708.285,46681.066,46666.633,46654.695,46644.06,46633.434,46631.312,46626.41,46624.75,46621.49,46622.613,46624.23,46626.895,46628.816,46629.938,46629.773,46629.332,46627.02,46628.883,46632.27,46632.32,46626.64,46619.215,46601.133,46580.977,46554.523,46530.67,46520.547,46510.312,46510.83,46509.324,46511.652,46514.402,46511.508,46510.715,46511.945,46513.15,46514.004,46515.23,46515.645,46515.184,46512.64,46509.246,46492.395,46469.15,46448.047,46431.28,46415.23,46394.668,46389.016,46391.477,46392.83,46395.21,46397.816,46404.28,46407.035,46409.047,46409.227,46405.84,46403.656,46401.18,46400.676,46401.14,46400.61,46400.59,46401.75,46412.71,46430.273,46452.957,46471.297,46491.082,46511.324,46521.973,46519.906,46508.63,46501.66,46491.28,46479.71,46468.207,46462.67,46461.543,46460.957,46461.227,46460.746,46461.473,46462.117,46461.63,46461.133,46462.05,46461.938,46461.438,46461.613,46462.734,46462.492,46460.176,46454.812,46446.406,46430.812,46416.105,46391.39,46366.844,46344.535,46325.707,46325.105,46329.01,46342.934,46355.77,46369.5,46383.81,46383.684,46377.645,46372.53,46364.63,46355.906,46338.45,46344.926,46332.285,46313.316,46295.52,46270.168,46249.66,46210.0,46180.707,46154.777,46139.266,46128.812,46122.223,46115.133,46128.062,46142.83,46148.742,46156.83,46165.418,46169.246,46164.914,46159.668,46155.617,46163.207,46177.4,46184.05,46182.027,46182.855,46186.293,46185.453,46174.434,46169.426,46175.105,46183.47,46191.766,46195.844,46202.918,46210.305,46215.8,46216.004,46215.535,46212.812,46210.477,46210.38,46209.59,46209.74,46208.426,46206.617,46206.508,46198.652,46207.645,46239.047,46271.6,46308.336,46344.02,46379.676,46396.336,46392.617,46385.34,46370.344,46351.594,46347.54,46345.61,46342.42,46340.926,46344.594,46351.016,46348.766,46346.438,46343.96,46339.46,46336.848,46344.05,46349.64,46347.055,46367.23,46395.94,46425.195,46443.93,46456.48,46487.4,46507.938,46523.895,46532.97,46541.69,46562.727,46574.85,46572.8,46569.633,46572.266,46574.254,46573.47,46571.86,46571.93,46580.04,46594.754,46611.28,46633.164,46656.395,46680.555,46696.5,46695.13,46686.02,46672.93,46661.22,46653.383,46639.37,46632.547,46632.973,46635.555,46636.684,46630.15,46630.844,46636.137,46639.953,46640.16,46637.94,46636.895,46635.973,46633.074,46629.48,46610.957,46596.133,46584.73,46578.098,46574.367,46575.027,46596.105,46618.188,46639.62,46658.52,46672.156,46681.45,46684.023,46682.43,46679.11,46675.17,46675.13,46676.453,46681.082,46685.06,46688.65,46691.168,46694.684,46703.08,46716.938,46733.664,46741.34,46752.996,46766.4,46782.633,46790.457,46814.746,46858.44,46894.633,46949.113,47001.805,47060.93,47105.098,47132.984,47162.72,47168.746,47167.543,47160.25,47151.367,47146.684,47145.645,47150.05,47152.19,47152.746,47173.152,47200.066,47231.535,47253.652,47269.914,47287.656,47275.324,47250.316,47220.64,47193.152,47173.39,47153.945,47142.79,47139.203,47132.906,47130.887,47130.754,47130.027,47129.81,47128.41,47125.805,47123.527,47124.7,47107.688,47084.402,47053.934,47027.336,47005.516,46977.062,46971.977,46974.004,46987.37,46996.28,47000.28,47006.97,47009.047,47010.348,47010.785,47014.098,47017.75,47021.79,47025.91,47030.445,47033.184,47034.69,47033.766,47017.293,47009.207,47017.42,47029.117,47039.31,47049.207,47074.008,47091.086,47093.973,47106.51,47118.97,47135.285,47150.293,47167.137,47181.684,47182.33,47183.89,47185.65,47187.31,47186.54,47184.83,47183.71,47183.074,47183.09,47188.54,47202.766,47214.277,47236.75,47263.203,47281.914,47318.793,47363.74,47401.082,47420.945,47429.906,47449.074,47442.8,47421.652,47413.195,47411.11,47414.984,47416.184,47419.367,47419.066,47417.09,47415.523,47412.13,47415.68,47427.97,47437.824,47435.305,47437.492,47440.055,47434.6,47418.996,47412.637,47415.59,47413.258,47414.008],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\"},\"mode\":\"lines\",\"name\":\"Test Actual\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47354.723,47386.867,47480.477,47473.496,47487.812,47481.54,47479.62,47470.945,47472.492,47478.9,47458.195,47448.29,47460.25,47502.414,47531.24,47498.434,47501.688,47516.1,47485.664,47526.723,47534.07,47558.37,47580.25,47563.18,47553.977,47579.523,47578.06,47567.715,47581.76,47578.43,47569.055,47570.44,47555.64,47533.176,47540.406,47546.89,47586.316,47688.715,47680.273,47713.484,47752.676,47771.42,47776.61,47787.48,47793.324,47894.84,47900.336,47876.62,47871.887,47895.88,47923.523,47921.543,47915.22,47883.438,47889.65,47888.965,47884.215,47877.965,47851.34,47849.41,47919.895,48024.168,48001.297,47956.777,47951.668,47942.934,47936.01,47891.285,47869.402,47898.98,47915.797,47921.13,47899.66,47902.406],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Test Pred\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47420.9,47413.48,47406.254,47417.316,47427.707,47439.484,47444.152,47464.97,47478.984,47477.652,47478.55,47473.613,47468.07,47464.848,47470.09,47479.88,47483.137,47490.383,47501.688,47505.926,47509.977,47510.45,47520.438,47533.53,47541.375,47552.76,47561.56,47568.89,47570.453,47570.703,47573.246,47575.758,47574.242,47570.508,47564.75,47557.86,47552.6,47555.48,47575.19,47595.965,47626.01,47661.39,47698.816,47730.53,47746.992,47765.832,47796.06,47820.668,47838.203,47854.08,47872.15,47893.844,47898.3,47900.777,47901.914,47904.875,47903.723,47897.17,47889.91,47879.266,47873.59,47878.633,47901.164,47920.68,47933.812,47950.535,47966.12,47968.805,47946.66,47924.68,47915.047,47909.066,47905.438,47899.375],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"shapes\":[{\"line\":{\"color\":\"black\",\"dash\":\"dot\",\"width\":2},\"type\":\"line\",\"x0\":657.5,\"x1\":657.5,\"xref\":\"x\",\"y0\":0,\"y1\":1,\"yref\":\"y domain\"}],\"legend\":{\"x\":0,\"y\":1},\"title\":{\"text\":\"Gold Price Forecast\"},\"xaxis\":{\"title\":{\"text\":\"Sample Index (time ordered)\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('350a093f-0d65-44da-af20-a3fa1cbe3c5a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'MAE': np.float32(42.853497),\n",
              "  'RMSE': np.float32(65.06158),\n",
              "  'MAPE': np.float32(0.09216803),\n",
              "  'sMAPE': np.float32(0.092192486),\n",
              "  'R2': np.float32(0.9735772),\n",
              "  'MASE': np.float32(1.8819138)},\n",
              " 'test': {'MAE': np.float32(40.150337),\n",
              "  'RMSE': np.float32(53.662975),\n",
              "  'MAPE': np.float32(0.08411548),\n",
              "  'sMAPE': np.float32(0.08416293),\n",
              "  'R2': np.float32(0.921264),\n",
              "  'MASE': np.float32(1.7632043)}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "6DKAPUcsyJD7",
        "outputId": "ea3cf322-cc62-4a8f-bb16-beef957ad3d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"7548a096-64fe-45db-93a3-1e2ce51f3430\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7548a096-64fe-45db-93a3-1e2ce51f3430\")) {                    Plotly.newPlot(                        \"7548a096-64fe-45db-93a3-1e2ce51f3430\",                        [{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Train Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[45984.137,45961.324,46025.574,45971.684,45888.23,45866.02,45903.69,45971.902,45982.81,45971.23,45955.87,45930.855,45936.633,45935.723,45946.992,45932.92,45938.992,45934.465,45920.04,45908.23,45920.67,45931.133,45949.0,46003.637,45957.676,45824.516,45878.844,45895.496,45962.71,46003.68,46004.375,45965.0,45902.656,45953.258,45981.41,45981.05,45980.324,45978.09,45978.098,45979.695,45974.86,45979.363,45979.918,45967.15,45986.773,45989.47,45925.57,45798.31,45796.082,45867.445,45846.465,45814.504,45861.227,45879.633,45885.312,45875.94,45996.523,46075.54,46069.523,46103.168,46102.56,46100.324,46098.492,46099.777,46099.227,46103.25,46101.34,46101.78,46152.32,46192.27,46214.51,46240.688,46335.863,46376.2,46325.07,46358.96,46411.56,46438.145,46305.71,46301.54,46310.957,46251.113,46280.188,46300.43,46331.156,46330.74,46330.082,46330.2,46329.35,46329.656,46327.496,46331.42,46326.82,46326.547,46425.258,46461.492,46313.63,46259.223,46287.18,46278.184,46274.793,46212.64,46284.184,46353.47,46372.207,46362.91,46440.656,46368.37,46360.89,46360.02,46360.17,46359.582,46359.574,46358.75,46362.523,46359.66,46359.43,46236.1,46178.438,46150.207,46118.336,45972.992,45895.27,45875.016,45864.05,45834.223,45938.613,45938.523,45994.297,46058.375,46109.63,46082.938,46051.254,46051.227,46048.504,46051.6,46050.18,46047.562,46049.613,46052.934,45944.496,45981.277,46025.227,46132.133,46217.305,46277.69,46223.12,46163.5,46186.758,46164.273,46239.766,46229.465,46227.367,46226.316,46231.13,46202.59,46198.02,46213.01,46215.875,46218.78,46227.316,46227.164,46219.516,46256.055,46273.973,46351.953,46469.805,46422.742,46428.203,46435.32,46445.69,46443.938,46377.61,46357.543,46387.797,46427.027,46470.688,46458.68,46458.96,46460.37,46466.215,46463.016,46461.53,46460.492,46466.906,46460.676,46456.59,46461.246,46464.586,46464.914,46472.86,46489.527,46482.64,46434.215,46438.715,46416.855,46371.582,46323.953,46293.883,46285.242,46230.383,46234.39,46237.887,46290.637,46285.32,46280.004,46277.914,46259.15,46237.34,46235.65,46233.09,46225.57,46228.465,46252.86,46352.977,46289.426,46254.62,46342.19,46391.01,46369.625,46380.684,46309.227,46315.3,46337.93,46347.43,46330.87,46329.793,46337.895,46339.125,46341.215,46343.3,46340.496,46339.234,46339.07,46340.05,46340.625,46340.555,46343.11,46507.996,46606.855,46671.09,46691.184,46663.67,46720.277,46747.15,46778.37,46769.6,46825.39,46868.215,46837.797,46805.66,46832.07,46839.74,46840.96,46833.246,46829.918,46835.832,46830.895,46843.113,46847.46,46850.04,46838.645,46828.684,46812.367,46725.887,46718.324,46685.15,46679.46,46628.703,46648.01,46641.02,46645.383,46619.543,46618.26,46614.95,46620.266,46629.703,46625.645,46626.324,46628.984,46632.94,46629.91,46636.574,46626.47,46621.543,46613.88,46643.68,46651.72,46636.527,46590.69,46576.805,46508.33,46522.047,46492.926,46496.72,46526.35,46516.586,46510.062,46512.85,46508.996,46509.414,46510.297,46511.465,46518.38,46516.08,46515.066,46518.242,46511.984,46509.277,46503.016,46497.504,46415.953,46377.887,46390.434,46409.35,46402.168,46376.965,46381.41,46389.668,46400.52,46421.51,46416.07,46414.734,46400.293,46400.098,46398.703,46400.652,46404.297,46401.277,46399.883,46398.42,46396.562,46400.97,46413.91,46463.734,46503.945,46536.97,46506.668,46520.527,46534.19,46528.875,46491.97,46470.47,46465.156,46459.383,46464.254,46459.07,46460.723,46463.258,46463.254,46461.0,46460.164,46464.06,46462.668,46460.586,46461.75,46462.52,46460.84,46461.47,46462.8,46466.77,46460.633,46453.34,46429.676,46407.4,46367.836,46377.906,46308.406,46300.414,46299.32,46293.984,46367.73,46400.79,46391.836,46380.86,46379.676,46377.895,46366.914,46369.586,46362.9,46333.65,46328.727,46272.508,46409.87,46290.992,46245.848,46226.062,46177.043,46146.64,46173.562,46114.008,46096.418,46127.363,46113.996,46105.223,46131.785,46186.883,46183.3,46169.258,46159.793,46160.137,46157.016,46161.21,46153.66,46146.1,46197.04,46250.203,46196.766,46148.01,46156.625,46164.14,46197.07,46181.332,46167.12,46181.56,46209.7,46210.402,46224.242,46223.383,46217.09,46212.12,46210.66,46210.734,46208.543,46209.48,46209.766,46207.742,46209.277,46202.887,46199.8,46206.145,46162.207,46265.95,46397.332,46399.656,46419.215,46422.316,46380.35,46363.156,46375.16,46355.03,46326.902,46308.008,46355.766,46352.145,46358.098,46347.867,46349.094,46346.2,46341.957,46339.36,46342.297,46320.863,46330.03,46389.594,46375.0,46322.58,46461.723,46494.887,46504.0,46506.883,46449.848,46506.758,46588.055,46589.68,46559.875,46558.453,46573.207,46579.277,46575.906,46573.29,46572.363,46571.613,46568.7,46570.08,46576.57,46620.71,46660.426,46669.453,46699.207,46710.695,46721.008,46718.215,46652.062,46614.242,46622.484,46638.34,46674.043,46634.08,46611.277,46615.734,46637.965,46646.4,46636.13,46639.453,46640.39,46639.83,46639.99,46633.297,46629.074,46633.094,46621.918,46617.16,46530.23,46543.23,46560.945,46592.688,46599.004,46623.273,46656.996,46675.59,46690.812,46705.445,46682.957,46679.074,46670.3,46667.766,46670.242,46681.836,46683.99,46685.18,46697.9,46690.684,46693.76,46697.7,46703.793,46733.63,46782.406,46791.55,46738.41,46768.08,46784.703,46830.766,46828.605,46937.742,47000.82,46985.117,47111.062,47146.18,47184.547,47201.83,47170.703,47161.77,47147.164,47138.887,47140.305,47150.14,47141.652,47155.734,47173.586,47151.38,47142.63,47272.79,47299.777,47344.66,47306.047,47249.27,47250.742,47199.47,47151.07,47168.633,47141.7,47130.246,47134.137,47131.344,47129.223,47130.586,47130.465,47128.492,47128.684,47129.703,47121.676,47114.668,47115.516,47136.145,47027.305,46990.285,46938.414,46955.8,46984.785,46965.81,46996.688,47001.84,47018.68,47009.54,47008.68,47006.312,47009.113,47010.324,47021.164,47029.24,47029.49,47030.55,47034.496,47036.79,47038.1,47038.11,47023.96,46931.684,46986.117,47086.465,47107.203,47100.773,47081.965,47081.34,47088.11,47103.117,47182.97,47177.367,47178.65,47172.836,47187.953,47191.26,47188.887,47185.24,47188.324,47184.215,47181.96,47180.63,47182.36,47183.31,47186.152,47216.46,47267.95,47248.465,47317.74,47339.68,47299.93,47440.21,47540.754,47471.6,47434.16,47396.6,47411.836,47401.457,47416.07,47422.438,47420.184,47419.74,47419.387,47419.13,47413.246,47409.992,47409.25,47401.496,47438.04,47491.504,47468.676,47397.133,47421.816,47414.67,47410.195,47401.418,47432.246,47412.895,47412.895,47418.137,47455.023],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Train Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657],\"y\":[46072.445,46015.977,45995.08,46051.625,46004.832,45934.348,45918.457,45947.188,46009.89,46016.42,46004.83,45990.086,45970.062,45972.098,45973.0,45985.05,45973.105,45976.8,45969.52,45954.844,45946.99,45961.008,45967.78,45988.39,46038.145,45992.344,45883.875,45929.652,45937.78,45999.637,46036.582,46035.992,46001.125,45944.65,45986.445,46014.785,46014.758,46011.336,46010.47,46011.21,46012.266,46011.043,46015.066,46009.594,46001.293,46018.582,46022.945,45968.73,45864.99,45861.56,45918.582,45895.895,45875.105,45909.477,45925.566,45927.836,45927.543,46020.78,46103.76,46093.1,46127.418,46127.64,46124.29,46123.516,46124.133,46127.902,46126.562,46122.29,46126.973,46177.312,46213.3,46231.887,46260.793,46351.098,46386.49,46341.336,46375.055,46420.906,46442.89,46324.918,46321.363,46325.36,46267.8,46296.645,46317.59,46346.395,46345.914,46346.293,46345.457,46347.918,46348.91,46345.355,46345.09,46345.027,46342.035,46431.543,46465.332,46332.098,46278.56,46305.535,46296.93,46293.016,46234.324,46302.406,46366.316,46386.4,46374.04,46444.14,46381.45,46371.195,46372.473,46372.387,46373.44,46372.258,46371.227,46372.375,46373.61,46372.11,46255.477,46198.51,46173.086,46138.574,46007.965,45938.875,45924.87,45916.06,45886.746,45975.273,45977.18,46021.895,46085.07,46134.18,46113.12,46079.953,46080.652,46079.71,46075.902,46078.7,46079.777,46076.37,46075.336,45979.56,46017.05,46055.395,46153.332,46235.36,46297.918,46241.09,46184.496,46207.37,46183.137,46262.383,46249.93,46245.24,46243.51,46248.965,46222.72,46217.297,46231.918,46234.402,46237.797,46246.137,46244.76,46240.24,46273.664,46291.47,46366.145,46473.566,46428.316,46431.21,46439.93,46450.777,46448.86,46389.883,46369.625,46393.64,46433.824,46473.793,46463.387,46461.44,46464.25,46469.77,46467.55,46466.266,46464.17,46469.684,46464.93,46464.49,46462.863,46468.137,46469.66,46474.543,46491.336,46483.566,46439.938,46446.113,46423.297,46384.562,46340.24,46314.457,46303.418,46251.367,46254.6,46256.836,46308.605,46306.133,46298.74,46297.18,46279.617,46260.12,46256.14,46248.305,46242.215,46250.062,46270.45,46367.746,46303.715,46273.965,46357.758,46399.688,46381.93,46391.016,46326.668,46332.996,46350.992,46364.96,46346.312,46346.96,46353.566,46355.844,46355.977,46357.98,46357.99,46356.1,46353.91,46354.023,46356.703,46355.594,46358.508,46509.777,46613.688,46671.72,46685.984,46666.207,46713.434,46739.57,46770.043,46761.027,46819.508,46866.023,46833.016,46798.22,46826.95,46835.223,46836.223,46828.176,46824.64,46831.156,46825.61,46838.73,46843.445,46846.055,46834.074,46822.848,46805.285,46719.02,46711.22,46681.773,46678.89,46633.89,46651.727,46643.707,46648.71,46627.34,46625.61,46622.906,46625.867,46635.22,46631.33,46630.727,46632.934,46636.727,46633.96,46640.77,46630.355,46629.207,46623.58,46646.055,46655.246,46640.418,46598.758,46584.668,46509.98,46525.355,46494.492,46498.777,46530.984,46519.613,46512.992,46514.19,46510.113,46511.58,46511.812,46514.51,46521.047,46521.86,46518.05,46522.402,46513.906,46510.688,46502.676,46500.3,46422.09,46388.03,46400.055,46415.746,46409.99,46388.76,46391.105,46396.996,46408.4,46428.887,46424.56,46422.906,46407.664,46408.33,46408.37,46409.53,46412.754,46408.043,46408.312,46406.918,46405.543,46409.746,46420.56,46468.863,46506.504,46541.2,46509.88,46523.457,46536.23,46532.02,46494.766,46473.562,46467.26,46461.707,46467.723,46461.133,46464.395,46465.586,46467.105,46462.617,46464.562,46466.95,46465.734,46464.473,46464.58,46465.01,46465.168,46465.04,46466.727,46468.33,46462.11,46456.92,46436.035,46415.76,46379.53,46389.664,46324.562,46321.176,46315.957,46311.523,46379.26,46410.176,46398.914,46390.918,46389.52,46389.42,46378.316,46378.04,46370.85,46349.43,46344.426,46290.465,46417.29,46308.688,46266.418,46244.152,46197.484,46170.703,46193.97,46135.715,46119.484,46152.406,46138.11,46129.695,46158.707,46209.7,46202.934,46188.07,46180.777,46181.773,46181.62,46183.05,46175.902,46167.016,46219.145,46271.805,46217.105,46169.234,46179.703,46184.062,46214.387,46203.457,46189.113,46202.97,46229.176,46229.156,46248.688,46240.684,46235.844,46231.33,46230.156,46230.09,46228.47,46226.38,46231.72,46231.664,46228.246,46221.312,46218.15,46227.676,46186.0,46284.062,46405.984,46409.65,46424.8,46427.406,46388.887,46373.938,46385.195,46369.082,46343.184,46324.867,46368.664,46365.473,46370.06,46363.465,46363.133,46358.7,46358.383,46353.023,46353.324,46339.64,46347.35,46398.52,46383.17,46342.56,46465.88,46496.895,46507.08,46508.895,46453.28,46508.52,46596.336,46597.285,46564.902,46564.52,46580.99,46587.914,46582.566,46579.79,46579.46,46579.953,46576.797,46578.03,46584.05,46627.797,46663.004,46669.438,46693.855,46703.586,46713.83,46710.65,46653.766,46622.594,46628.766,46643.277,46674.223,46638.78,46619.152,46622.523,46641.04,46648.113,46640.07,46643.13,46643.355,46643.39,46643.715,46637.64,46634.81,46636.258,46629.05,46625.98,46533.48,46547.246,46568.973,46601.01,46608.723,46629.13,46657.637,46675.664,46687.035,46699.254,46680.81,46677.676,46671.59,46669.1,46672.16,46678.605,46680.66,46682.3,46691.8,46687.56,46688.746,46691.17,46697.336,46726.85,46774.312,46783.773,46731.777,46760.023,46775.836,46825.375,46823.266,46941.27,47008.92,46992.094,47124.82,47161.03,47201.668,47220.97,47185.09,47177.266,47162.348,47153.293,47155.02,47164.195,47157.11,47171.344,47190.19,47166.438,47158.21,47301.445,47330.82,47378.516,47336.82,47274.023,47276.645,47217.832,47165.926,47184.453,47156.41,47144.832,47147.965,47146.016,47144.09,47144.65,47144.35,47143.57,47143.11,47144.26,47135.2,47129.535,47130.227,47151.05,47038.254,46997.773,46942.07,46960.715,46991.973,46971.594,47004.824,47010.266,47028.035,47018.543,47016.523,47014.94,47017.906,47018.99,47030.902,47039.973,47039.375,47040.785,47044.633,47047.51,47048.473,47049.156,47033.156,46934.824,46993.234,47098.785,47121.594,47114.047,47095.504,47095.176,47100.742,47116.18,47200.33,47193.875,47195.555,47189.207,47205.492,47208.77,47207.395,47203.312,47205.04,47201.547,47200.4,47198.11,47198.715,47201.62,47205.01,47238.055,47297.992,47275.06,47346.39,47372.855,47328.324,47472.63,47567.934,47505.33,47469.258,47431.023,47444.223,47437.023,47448.14,47453.64,47452.83,47454.543,47454.63,47455.625,47447.34,47443.457,47442.137,47434.56,47473.4,47526.457,47503.55,47429.84,47457.676,47449.38,47445.29,47431.67,47464.008,47447.582,47444.69,47451.95],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\"},\"mode\":\"lines\",\"name\":\"Test Actual\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47354.723,47386.867,47480.477,47473.496,47487.812,47481.54,47479.62,47470.945,47472.492,47478.9,47458.195,47448.29,47460.25,47502.414,47531.24,47498.434,47501.688,47516.1,47485.664,47526.723,47534.07,47558.37,47580.25,47563.18,47553.977,47579.523,47578.06,47567.715,47581.76,47578.43,47569.055,47570.44,47555.64,47533.176,47540.406,47546.89,47586.316,47688.715,47680.273,47713.484,47752.676,47771.42,47776.61,47787.48,47793.324,47894.84,47900.336,47876.62,47871.887,47895.88,47923.523,47921.543,47915.22,47883.438,47889.65,47888.965,47884.215,47877.965,47851.34,47849.41,47919.895,48024.168,48001.297,47956.777,47951.668,47942.934,47936.01,47891.285,47869.402,47898.98,47915.797,47921.13,47899.66,47902.406],\"type\":\"scatter\"},{\"line\":{\"color\":\"orange\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Test Pred\",\"x\":[658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731],\"y\":[47487.586,47387.688,47420.9,47513.63,47506.934,47520.645,47514.66,47512.83,47504.492,47505.984,47512.145,47492.14,47482.46,47494.145,47534.51,47561.383,47530.76,47533.832,47547.344,47518.64,47557.223,47564.004,47586.152,47605.734,47590.504,47582.2,47605.105,47603.81,47594.582,47607.1,47604.15,47595.793,47597.04,47583.73,47563.227,47569.863,47575.78,47611.13,47698.285,47691.336,47718.414,47749.336,47763.773,47767.76,47776.06,47780.484,47853.53,47857.297,47840.965,47837.668,47854.277,47872.934,47871.62,47867.39,47845.74,47850.027,47849.555,47846.28,47841.938,47823.16,47821.78,47870.527,47936.6,47922.71,47894.715,47891.426,47885.75,47881.22,47851.164,47835.96,47856.43,47867.797,47871.367,47856.895],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"shapes\":[{\"line\":{\"color\":\"black\",\"dash\":\"dot\",\"width\":2},\"type\":\"line\",\"x0\":657.5,\"x1\":657.5,\"xref\":\"x\",\"y0\":0,\"y1\":1,\"yref\":\"y domain\"}],\"legend\":{\"x\":0,\"y\":1},\"title\":{\"text\":\"Gold Price Forecast\"},\"xaxis\":{\"title\":{\"text\":\"Sample Index (time ordered)\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7548a096-64fe-45db-93a3-1e2ce51f3430');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'MAE': np.float32(28.6127),\n",
              "  'RMSE': np.float32(40.517532),\n",
              "  'MAPE': np.float32(0.061582796),\n",
              "  'sMAPE': np.float32(0.061566602),\n",
              "  'R2': np.float32(0.9897523),\n",
              "  'MASE': np.float32(1.2506454)},\n",
              " 'test': {'MAE': np.float32(37.992504),\n",
              "  'RMSE': np.float32(47.026203),\n",
              "  'MAPE': np.float32(0.07959964),\n",
              "  'sMAPE': np.float32(0.07961549),\n",
              "  'R2': np.float32(0.9395351),\n",
              "  'MASE': np.float32(1.6606315)}}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# you already have: trainset, testset, model\n",
        "# Create the same model architecture\n",
        "\n",
        "\n",
        "model = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size=output_size,\n",
        "    dropout=dropout)\n",
        "\n",
        "# model = LSTM(input_size, num_hidden_layers, num_stacked_layers, output_size)\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_state_dict(torch.load('/content/checkpoint_transformer.pt'))\n",
        "\n",
        "result = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "\n",
        "plot_predictions_plotly(result, horizon_step=0, title=\"Gold Price Forecast\")\n",
        "\n",
        "\n",
        "transformer_metrics = evaluate_forecast(result)\n",
        "transformer_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "tKLh7-2SFu0K",
        "outputId": "74091650-ba39-4b64-887c-b1bb2348e95b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"b609cd1e-076a-478a-bfb0-ece5b374857f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b609cd1e-076a-478a-bfb0-ece5b374857f\")) {                    Plotly.newPlot(                        \"b609cd1e-076a-478a-bfb0-ece5b374857f\",                        [{\"line\":{\"color\":\"black\"},\"mode\":\"lines\",\"name\":\"Test Actual\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47354.723,47386.867,47480.477,47473.496,47487.812,47481.54,47479.62,47470.945,47472.492,47478.9,47458.195,47448.29,47460.25,47502.414,47531.24,47498.434,47501.688,47516.1,47485.664,47526.723,47534.07,47558.37,47580.25,47563.18,47553.977,47579.523,47578.06,47567.715,47581.76,47578.43,47569.055,47570.44,47555.64,47533.176,47540.406,47546.89,47586.316,47688.715,47680.273,47713.484,47752.676,47771.42,47776.61,47787.48,47793.324,47894.84,47900.336,47876.62,47871.887,47895.88,47923.523,47921.543,47915.22,47883.438,47889.65,47888.965,47884.215,47877.965,47851.34,47849.41,47919.895,48024.168,48001.297,47956.777,47951.668,47942.934,47936.01,47891.285,47869.402,47898.98,47915.797,47921.13,47899.66,47902.406],\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"LSTM Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47459.29,47400.977,47405.35,47468.26,47481.734,47494.56,47494.906,47494.137,47488.715,47488.137,47492.01,47480.156,47470.164,47474.793,47502.47,47528.105,47516.254,47514.746,47523.305,47507.496,47527.535,47538.13,47555.586,47573.688,47569.938,47563.72,47576.547,47579.89,47575.14,47581.652,47581.938,47576.754,47575.914,47567.207,47551.285,47550.344,47553.727,47577.188,47639.15,47654.47,47676.39,47702.785,47720.617,47729.51,47737.832,47743.63,47786.9,47803.785,47801.33,47799.17,47807.684,47820.793,47824.86,47824.24,47812.74,47810.875,47809.953,47807.77,47804.574,47793.24,47788.285,47812.812,47855.74,47863.785,47852.965,47847.65,47843.04,47839.01,47821.81,47807.176,47812.582,47820.35,47824.844,47818.836],\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"GRU Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47449.598,47377.188,47389.05,47464.484,47473.58,47486.543,47482.844,47480.035,47472.0,47471.312,47475.8,47460.52,47450.055,47457.3,47491.457,47519.723,47499.863,47498.4,47508.254,47486.57,47514.086,47523.99,47544.44,47563.824,47554.176,47545.14,47562.0,47563.438,47555.996,47564.863,47563.617,47556.434,47556.055,47544.773,47525.95,47528.188,47533.6,47564.6,47644.76,47651.582,47676.62,47706.4,47723.38,47728.91,47736.414,47740.734,47808.54,47821.043,47808.375,47802.582,47816.594,47835.977,47837.484,47833.75,47812.336,47813.094,47812.227,47809.164,47804.67,47786.508,47782.613,47828.05,47899.758,47896.297,47869.445,47861.56,47853.992,47848.355,47818.594,47799.996,47816.3,47829.53,47835.477,47822.703],\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Transformer Pred\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73],\"y\":[47487.586,47387.688,47420.9,47513.63,47506.934,47520.645,47514.66,47512.83,47504.492,47505.984,47512.145,47492.14,47482.46,47494.145,47534.51,47561.383,47530.76,47533.832,47547.344,47518.64,47557.223,47564.004,47586.152,47605.734,47590.504,47582.2,47605.105,47603.81,47594.582,47607.1,47604.15,47595.793,47597.04,47583.73,47563.227,47569.863,47575.78,47611.13,47698.285,47691.336,47718.414,47749.336,47763.773,47767.76,47776.06,47780.484,47853.53,47857.297,47840.965,47837.668,47854.277,47872.934,47871.62,47867.39,47845.74,47850.027,47849.555,47846.28,47841.938,47823.16,47821.78,47870.527,47936.6,47922.71,47894.715,47891.426,47885.75,47881.22,47851.164,47835.96,47856.43,47867.797,47871.367,47856.895],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"legend\":{\"x\":0,\"y\":1},\"title\":{\"text\":\"Gold Price Forecasts (Test)\"},\"xaxis\":{\"title\":{\"text\":\"Test Sample Index (time ordered)\"}},\"yaxis\":{\"title\":{\"text\":\"Price\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b609cd1e-076a-478a-bfb0-ece5b374857f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot results of models on testset\n",
        "lstm_test_pred_inv = predict_train_test(lstm_model, trainset, testset, batch_size=128, scaler=scaler)['test_pred_inv']\n",
        "gru_test_pred_inv = predict_train_test(gru_model, trainset, testset, batch_size=128, scaler=scaler)['test_pred_inv']\n",
        "transformer_test_pred_inv = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)['test_pred_inv']\n",
        "test_true_inv = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)['test_true_inv']\n",
        "\n",
        "\n",
        "results = {\n",
        "    \"test_true_inv\": test_true_inv,           # shape [N] or [N, H]\n",
        "    \"preds\": {\n",
        "        \"LSTM\": lstm_test_pred_inv,           # shape [N] or [N, H]\n",
        "        \"GRU\": gru_test_pred_inv,\n",
        "        \"Transformer\": transformer_test_pred_inv\n",
        "    }\n",
        "}\n",
        "\n",
        "plot_test_predictions_plotly(results, horizon_step=0, title=\"Gold Price Forecasts (Test)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbgSeOvA8Knn"
      },
      "source": [
        "## Grid Search (Params Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s0nKb1M8PX2"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "param_grid = {\n",
        "    'd_model': [64, 256],\n",
        "    'lr': [0.001, 0.01],\n",
        "    'nhead': [8, 16],\n",
        "    'num_encoder_layers': [1, 6],\n",
        "    'dim_feedforward':[64, 256],\n",
        "    'dropout': [0.1, 0.5]\n",
        "\n",
        "}\n",
        "\n",
        "keys = param_grid.keys()\n",
        "\n",
        "combinations = [dict(zip(keys, values)) for values in product(*param_grid.values())]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT2tW6lh-N-q"
      },
      "outputs": [],
      "source": [
        "augmentations = Compose([AddGaussianNoise(), RandomScaling()])\n",
        "\n",
        "trainset, train_loader, testset, test_loader = get_loaders(df, time_bin=60, scaler=scaler, lookback=6,\n",
        "                                        lookforward=1, batch_size=16, train_size=0.9,\n",
        "                                        transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0LNDPU2X-iJJ",
        "outputId": "8ade23da-b59f-45ca-afff-66e07026ea1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 1] Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 2] Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 3] Early stopping at epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 4] Early stopping at epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 5] Early stopping at epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 6] Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 7] Early stopping at epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 8] Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 9] Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 10] Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 11] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 12] Early stopping at epoch 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 13] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 14] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 15] Early stopping at epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 16] Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 17] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 18] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 19] Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 20] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 21] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 22] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 23] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 24] Early stopping at epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 25] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 26] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 27] Early stopping at epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 28] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 29] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 30] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 31] Early stopping at epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 32] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 33] Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 34] Early stopping at epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 35] Early stopping at epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 36] Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 37] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 38] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 39] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 40] Early stopping at epoch 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 41] Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 42] Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 43] Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 44] Early stopping at epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 45] Early stopping at epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 46] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 47] Early stopping at epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 48] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 49] Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 50] Early stopping at epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 51] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 52] Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 53] Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 54] Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 55] Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 56] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 57] Early stopping at epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 58] Early stopping at epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 59] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 60] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 61] Early stopping at epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 62] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 63] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CFG 64] Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   d_model     lr  nhead  num_encoder_layers  dim_feedforward  dropout  \\\n",
              "0      256  0.001      8                   1              256      0.1   \n",
              "1      256  0.001     16                   1               64      0.1   \n",
              "2       64  0.010      8                   1               64      0.1   \n",
              "3      256  0.001      8                   1               64      0.1   \n",
              "4      256  0.001      8                   1              256      0.5   \n",
              "\n",
              "    train_MAE  train_RMSE  train_MAPE  train_sMAPE  train_R2  train_MASE  \\\n",
              "0   59.876881  311.896606    0.134548     0.131579  0.635762    1.277514   \n",
              "1  104.224777  318.076447    0.230257     0.227805  0.621185    2.223706   \n",
              "2   47.454025  307.333282    0.107617     0.104765  0.646342    1.012464   \n",
              "3  106.763321  324.294800    0.235487     0.232300  0.606228    2.277868   \n",
              "4  156.878799  349.178284    0.343728     0.340185  0.543481    3.347116   \n",
              "\n",
              "    test_MAE  test_RMSE  test_MAPE  test_sMAPE   test_R2  test_MASE  \n",
              "0  28.325327  38.131069   0.059378    0.059385  0.960992   0.604340  \n",
              "1  30.906672  38.924900   0.064777    0.064772  0.959350   0.659415  \n",
              "2  30.021906  41.927132   0.062885    0.062913  0.952838   0.640538  \n",
              "3  36.672825  45.775204   0.076973    0.076952  0.943784   0.782440  \n",
              "4  37.275181  46.248051   0.078194    0.078180  0.942616   0.795291  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec1bb963-416d-44e0-a227-27b0d5bcbf58\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>d_model</th>\n",
              "      <th>lr</th>\n",
              "      <th>nhead</th>\n",
              "      <th>num_encoder_layers</th>\n",
              "      <th>dim_feedforward</th>\n",
              "      <th>dropout</th>\n",
              "      <th>train_MAE</th>\n",
              "      <th>train_RMSE</th>\n",
              "      <th>train_MAPE</th>\n",
              "      <th>train_sMAPE</th>\n",
              "      <th>train_R2</th>\n",
              "      <th>train_MASE</th>\n",
              "      <th>test_MAE</th>\n",
              "      <th>test_RMSE</th>\n",
              "      <th>test_MAPE</th>\n",
              "      <th>test_sMAPE</th>\n",
              "      <th>test_R2</th>\n",
              "      <th>test_MASE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>256</td>\n",
              "      <td>0.1</td>\n",
              "      <td>59.876881</td>\n",
              "      <td>311.896606</td>\n",
              "      <td>0.134548</td>\n",
              "      <td>0.131579</td>\n",
              "      <td>0.635762</td>\n",
              "      <td>1.277514</td>\n",
              "      <td>28.325327</td>\n",
              "      <td>38.131069</td>\n",
              "      <td>0.059378</td>\n",
              "      <td>0.059385</td>\n",
              "      <td>0.960992</td>\n",
              "      <td>0.604340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>104.224777</td>\n",
              "      <td>318.076447</td>\n",
              "      <td>0.230257</td>\n",
              "      <td>0.227805</td>\n",
              "      <td>0.621185</td>\n",
              "      <td>2.223706</td>\n",
              "      <td>30.906672</td>\n",
              "      <td>38.924900</td>\n",
              "      <td>0.064777</td>\n",
              "      <td>0.064772</td>\n",
              "      <td>0.959350</td>\n",
              "      <td>0.659415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>0.010</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>47.454025</td>\n",
              "      <td>307.333282</td>\n",
              "      <td>0.107617</td>\n",
              "      <td>0.104765</td>\n",
              "      <td>0.646342</td>\n",
              "      <td>1.012464</td>\n",
              "      <td>30.021906</td>\n",
              "      <td>41.927132</td>\n",
              "      <td>0.062885</td>\n",
              "      <td>0.062913</td>\n",
              "      <td>0.952838</td>\n",
              "      <td>0.640538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>106.763321</td>\n",
              "      <td>324.294800</td>\n",
              "      <td>0.235487</td>\n",
              "      <td>0.232300</td>\n",
              "      <td>0.606228</td>\n",
              "      <td>2.277868</td>\n",
              "      <td>36.672825</td>\n",
              "      <td>45.775204</td>\n",
              "      <td>0.076973</td>\n",
              "      <td>0.076952</td>\n",
              "      <td>0.943784</td>\n",
              "      <td>0.782440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>256</td>\n",
              "      <td>0.001</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>256</td>\n",
              "      <td>0.5</td>\n",
              "      <td>156.878799</td>\n",
              "      <td>349.178284</td>\n",
              "      <td>0.343728</td>\n",
              "      <td>0.340185</td>\n",
              "      <td>0.543481</td>\n",
              "      <td>3.347116</td>\n",
              "      <td>37.275181</td>\n",
              "      <td>46.248051</td>\n",
              "      <td>0.078194</td>\n",
              "      <td>0.078180</td>\n",
              "      <td>0.942616</td>\n",
              "      <td>0.795291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec1bb963-416d-44e0-a227-27b0d5bcbf58')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec1bb963-416d-44e0-a227-27b0d5bcbf58 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec1bb963-416d-44e0-a227-27b0d5bcbf58');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-de38aaf9-2bb0-4547-9786-ed1dc501e5a1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de38aaf9-2bb0-4547-9786-ed1dc501e5a1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-de38aaf9-2bb0-4547-9786-ed1dc501e5a1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_grid",
              "summary": "{\n  \"name\": \"df_grid\",\n  \"rows\": 64,\n  \"fields\": [\n    {\n      \"column\": \"d_model\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96,\n        \"min\": 64,\n        \"max\": 256,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64,\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004535573676110723,\n        \"min\": 0.001,\n        \"max\": 0.01,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.01,\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nhead\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 8,\n        \"max\": 16,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          16,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_encoder_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dim_feedforward\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96,\n        \"min\": 64,\n        \"max\": 256,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64,\n          256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20158105227158793,\n        \"min\": 0.1,\n        \"max\": 0.5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_MAE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          614.7304077148438,\n          451.2472839355469\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_RMSE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          760.3920288085938,\n          598.1897583007812\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.3335003852844238,\n          0.9788501858711243\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_sMAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.3195170164108276,\n          0.9711848497390747\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_R2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          -1.1649088859558105,\n          -0.3398076295852661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_MASE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          13.115689277648926,\n          9.627666473388672\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_MAE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          632.9443969726562,\n          893.9075927734375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_RMSE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          661.4508056640625,\n          914.5366821289062\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_MAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.325530767440796,\n          1.8727104663848877\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_sMAPE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          1.3351795673370361,\n          1.8912229537963867\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_R2\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          -10.738036155700684,\n          -21.43895149230957\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_MASE\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          13.504297256469727,\n          19.072124481201172\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "num_epochs = 200\n",
        "results = []\n",
        "\n",
        "for cfg_idx, config in enumerate(combinations, start=1):\n",
        "\n",
        "  model = TransformerTS(\n",
        "      input_size=1,\n",
        "      d_model=config['d_model'],\n",
        "      nhead=config['nhead'],\n",
        "      num_encoder_layers=config['num_encoder_layers'],\n",
        "      dim_feedforward=config['dim_feedforward'],\n",
        "      output_size=1,\n",
        "      dropout=config['dropout'])\n",
        "\n",
        "  model.to(device)\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
        "  early_stopping = EarlyStopping(patience=10, mode='min', verbose=False, save_path= f'/content/checkpoint_{cfg_idx}.pt')\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      train_one_epoch(model, show=False)\n",
        "      stop_criteria = validate_one_epoch(model, show=False, early_stopping=early_stopping)\n",
        "\n",
        "      if stop_criteria:\n",
        "          print(f\"[CFG {cfg_idx}] Early stopping at epoch {epoch}\")\n",
        "          break\n",
        "\n",
        "\n",
        "\n",
        "  model = TransformerTS(\n",
        "      input_size=1,\n",
        "      d_model=config['d_model'],\n",
        "      nhead=config['nhead'],\n",
        "      num_encoder_layers=config['num_encoder_layers'],\n",
        "      dim_feedforward=config['dim_feedforward'],\n",
        "      output_size=1,\n",
        "      dropout=config['dropout'])\n",
        "\n",
        "  model.load_state_dict(torch.load(f'/content/checkpoint_{cfg_idx}.pt'))\n",
        "\n",
        "  result = predict_train_test(model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "  transformer_metrics = evaluate_forecast(result)\n",
        "\n",
        "\n",
        "  # متریک‌های train\n",
        "  for k, v in transformer_metrics['train'].items():\n",
        "      config[f\"train_{k}\"] = v\n",
        "\n",
        "  # متریک‌های test\n",
        "  for k, v in transformer_metrics['test'].items():\n",
        "      config[f\"test_{k}\"] = v\n",
        "\n",
        "  results.append(config)\n",
        "\n",
        "df_grid = pd.DataFrame(results)\n",
        "\n",
        "if \"test_RMSE\" in df_grid.columns:\n",
        "    df_grid = df_grid.sort_values(by=\"test_RMSE\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "df_grid.to_csv(\"transformer_grid_results.csv\", index=False)\n",
        "df_grid.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GtgFQkzQd8G"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8g_ilnxFzZh"
      },
      "source": [
        "\n",
        "*   Multiple step prediction\n",
        "*   How much Lookback\n",
        "*   Effect of TimeBin\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple step prediction"
      ],
      "metadata": {
        "id": "pR3teEB87jr2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WzAgIG6F33q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855bb28b-7fad-4b56-bccf-82287be7521b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning:\n",
            "\n",
            "enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.05561\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.05150\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02522\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.02186\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00452\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00149\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00246\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00101\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00127\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00151\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00088\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00143\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00069\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00132\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00076\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00084\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00062\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00310\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00143\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00190\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00155\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00189\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00059\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00395\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00080\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00386\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00116\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00153\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00324\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00269\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00208\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00174\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00135\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00170\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00237\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00048\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00176\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00545\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00066\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00219\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00045\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00037\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00047\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00066\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00207\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00149\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00069\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00076\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00144\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00306\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00113\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00111\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00277\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00042\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00347\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00050\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00410\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00102\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00224\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00155\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00059\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00225\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00164\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00215\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00165\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00036\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00039\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00158\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00130\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00116\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00130\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00088\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00065\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00060\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00057\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00070\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00114\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00052\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00030\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00212\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00028\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00044\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00067\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00074\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00093\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00242\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00039\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00113\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00035\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00076\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00078\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00112\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00092\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00041\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00213\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00053\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00286\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00152\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00040\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00124\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00054\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00108\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00378\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00057\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00137\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00043\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00042\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00197\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00038\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00139\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00131\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00126\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00508\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00065\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00066\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00034\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00135\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00242\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00236\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00029\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00058\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00032\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00044\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00037\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00321\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00037\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00030\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00042\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00040\n",
            "***************************************************\n",
            "\n",
            "Val Loss: 0.00095\n",
            "***************************************************\n",
            "\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "early_stopping = EarlyStopping(patience=50, mode='min', verbose=False, save_path='/content/checkpoint_lookforward.pt')\n",
        "\n",
        "\n",
        "trainset, train_loader, testset, test_loader = get_loaders(df, time_bin=60, scaler=scaler, lookback=6,\n",
        "                                        lookforward=10, batch_size=16, train_size=0.9,\n",
        "                                        transform=augmentations)\n",
        "model_lookforward = TransformerTS(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    output_size= 10,\n",
        "    dropout=dropout)\n",
        "\n",
        "# model = GRUModel(1, 4, 1, 1)\n",
        "model_lookforward.to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model_lookforward.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model_lookforward, show=False)\n",
        "    stop_criteria = validate_one_epoch(model_lookforward, early_stopping)\n",
        "\n",
        "    if stop_criteria:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self, sample_mean=3, lookahead_steps=3):\n",
        "        \"\"\"\n",
        "        Baseline model to predict `lookahead_steps` using the mean of the previous `sample_mean` time steps.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sample_mean = sample_mean\n",
        "        self.lookahead_steps = lookahead_steps\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: input of shape [batch_size, time_steps, features]\n",
        "        :return: predictions of shape [batch_size, lookahead_steps, features]\n",
        "        \"\"\"\n",
        "        # Get the last `sample_mean` number of time steps\n",
        "        last_samples = x[:, -self.sample_mean:, :]  # shape: [batch, sample_mean, features]\n",
        "\n",
        "        # Calculate the mean over the last `sample_mean` steps\n",
        "        mean_value = last_samples.mean(dim=1)  # shape: [batch, features]\n",
        "\n",
        "        # Repeat the mean value to generate predictions for the next `lookahead_steps`\n",
        "        repeated_predictions = mean_value.unsqueeze(1).repeat(1, self.lookahead_steps, 1)\n",
        "\n",
        "        return repeated_predictions\n"
      ],
      "metadata": {
        "id": "opzt7LyEAqY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "baselineModel = BaselineModel(sample_mean=10, lookahead_steps=10)\n",
        "\n",
        "\n",
        "model_lookforward.eval()\n",
        "all_predictions = []\n",
        "all_predictions_baseline = []\n",
        "all_actual_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(test_loader):  # test_loader should provide the test data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        predicted_baseline = baselineModel(inputs)\n",
        "        predicted = model_lookforward(inputs)  # predicted shape (batch_size, 3)\n",
        "        all_predictions_baseline.append(predicted_baseline.cpu().numpy())\n",
        "        all_predictions.append(predicted.cpu().numpy())\n",
        "        all_actual_values.append(labels.cpu().numpy())\n",
        "\n",
        "# Convert the list of predictions and labels to numpy arrays for easier plotting\n",
        "all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "all_actual_values = np.concatenate(all_actual_values, axis=0)\n",
        "all_predictions_baseline = np.concatenate(all_predictions_baseline, axis=0)\n",
        "\n",
        "\n",
        "print(mean_absolute_error(all_actual_values, all_predictions),\n",
        "mean_absolute_error(all_actual_values, all_predictions_baseline.squeeze(-1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc57iHEF7zJq",
        "outputId": "89b1397c-1250-49fc-9a05-e715dc02e5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.024832408875226974 0.013545608147978783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the BaselineModel\n",
        "baseline_model = BaselineModel(sample_mean=3, lookahead_steps=3)\n",
        "\n",
        "result = predict_train_test(baseline_model, trainset, testset, batch_size=128, scaler=scaler)\n",
        "\n",
        "baseline_metrics = evaluate_forecast(result)\n",
        "baseline_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KumMSy7s9Ry-",
        "outputId": "473c6ee1-2f33-467d-f155-e041b8b0ac41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'MAE': np.float32(60.92469),\n",
              "  'RMSE': np.float32(357.41925),\n",
              "  'MAPE': np.float32(0.13697843),\n",
              "  'sMAPE': np.float32(0.13486174),\n",
              "  'R2': np.float32(0.50790465),\n",
              "  'MASE': np.float32(1.053577)},\n",
              " 'test': {'MAE': np.float32(29.475332),\n",
              "  'RMSE': np.float32(41.677708),\n",
              "  'MAPE': np.float32(0.06180005),\n",
              "  'sMAPE': np.float32(0.0618294),\n",
              "  'R2': np.float32(0.95263547),\n",
              "  'MASE': np.float32(0.50971997)}}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NO2h3ZL_r4U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}